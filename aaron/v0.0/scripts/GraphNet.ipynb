{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pdb\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from GCN.GCN import GCN\n",
    "from GAT.GAT import GAT\n",
    "from Dense.Dense import Dense\n",
    "from full_data_process import graphDataProcess\n",
    "from sub_data_process import subGraphProcess\n",
    "\n",
    "from adjacency_functions import *\n",
    "from feature_functions import *\n",
    "from label_functions import *\n",
    "from train_test_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_graph_process(param_dict, data_dir, full_processed_path, full_redo):\n",
    "    if (not os.path.exists(full_processed_path)) or full_redo:\n",
    "        raw_data_path = param_dict['gen_params']['raw_data_path']\n",
    "        full_names_dict = param_dict['full_names_dict']\n",
    "        full_redo_dict = param_dict['full_redo_dict']\n",
    "        graph_data_obj = graphDataProcess(raw_data_path, data_dir, full_names_dict, full_redo_dict)\n",
    "        graph_data_obj.run_all()\n",
    "        pickle.dump(graph_data_obj, open(full_processed_path,'wb'))\n",
    "    else:\n",
    "        graph_data_obj = pickle.load(open(full_processed_path,'rb'))\n",
    "    return graph_data_obj\n",
    "\n",
    "def sub_graph_process(param_dict, data_path, full_processed_path, sub_processed_path, sub_redo):\n",
    "    if (not os.path.exists(sub_processed_path)) or sub_redo:\n",
    "        sampling_params = param_dict['sampling_params']\n",
    "        sub_names_dict = param_dict['sub_names_dict']\n",
    "        sub_redo_dict = param_dict['sub_redo_dict']\n",
    "        sub_functions_dict = get_func_dict(param_dict['sub_functions_dict'])\n",
    "        subgraph_data_obj = subGraphProcess(full_processed_path, data_path, sub_names_dict, sub_redo_dict, sub_functions_dict, sampling_params)\n",
    "        subgraph_data_obj.run_all()\n",
    "        pickle.dump(subgraph_data_obj, open(sub_processed_path,'wb'))\n",
    "    else:\n",
    "        subgraph_data_obj = pickle.load(open(sub_processed_path,'rb'))\n",
    "    return subgraph_data_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = 'v0.1'\n",
    "param_path = '/home/ds-team/aaron/other/MoonBoard/data/train_test/pytorch/graphNet/GraphNet/' + ver + '/params.json'\n",
    "param_dict = json.load(open(param_path,'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unwrap and set general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = param_dict['gen_params']['model_type']\n",
    "ver = param_dict['gen_params']['ver']\n",
    "data_dir = param_dict['gen_params']['data_dir']\n",
    "result_dir = param_dict['gen_params']['result_dir']\n",
    "\n",
    "data_path, result_path = set_paths(model_type, ver, data_dir, result_dir)\n",
    "\n",
    "full_processed_name = param_dict['gen_params']['full_processed_name']\n",
    "sub_processed_name = param_dict['gen_params']['sub_processed_name']\n",
    "\n",
    "full_processed_path = data_dir+full_processed_name\n",
    "sub_processed_path = data_path+sub_processed_name\n",
    "\n",
    "full_redo = param_dict['gen_params']['full_redo']\n",
    "sub_redo = param_dict['gen_params']['sub_redo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_obj = full_graph_process(param_dict, data_dir, full_processed_path, full_redo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling core nodes\n",
      "Getting samples node features\n",
      "Getting samples node adjacency\n",
      "Getting samples node labels\n"
     ]
    }
   ],
   "source": [
    "subgraph_data_obj = sub_graph_process(param_dict, data_path, full_processed_path, sub_processed_path, sub_redo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio_dict = param_dict['split_ratio_dict']\n",
    "target_grade = -1\n",
    "features, adj, labels, idx_train, idx_dev, idx_test = sample_and_load_pytorch_data(subgraph_data_obj, split_ratio_dict, result_path, target_grade, sub_redo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(list(set(list(np.asarray(labels)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_params = param_dict['dense_params']\n",
    "if dense_params:\n",
    "    num_epochs = dense_params['num_epochs']\n",
    "    model = Dense(nfeat=features.shape[1],\n",
    "                nhid_list=dense_params['hidden'],\n",
    "                nclass=num_labels,\n",
    "                dropout=dense_params['dropout'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=dense_params['lr'], weight_decay=dense_params['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_params = param_dict['gcn_params']\n",
    "if gcn_params:\n",
    "    num_epochs = gcn_params['num_epochs']\n",
    "    model = GCN(nfeat=features.shape[1],\n",
    "                nhid_list=gcn_params['hidden'],\n",
    "                nclass=num_labels,\n",
    "                dropout=gcn_params['dropout'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=gcn_params['lr'], weight_decay=gcn_params['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_params = param_dict['gat_params']\n",
    "if gat_params['on']:\n",
    "    num_epochs = gat_params['num_epochs']\n",
    "    model = GAT(nfeat=features.shape[1],\n",
    "                nhid=gat_params['hidden'],\n",
    "                nclass=num_labels,\n",
    "                dropout=gat_params['dropout'],\n",
    "                alpha=gat_params['alpha'],\n",
    "                nheads=gat_params['nb_heads'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=gat_params['lr'], weight_decay=gat_params['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gc_list): ModuleList(\n",
       "    (0): GraphConvolution (11140 -> 128)\n",
       "    (1): GraphConvolution (128 -> 12)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.5236 acc_train: 0.0000 loss_val: 2.5045 acc_val: 0.0000 time: 0.4145s\n",
      "Epoch: 0002 loss_train: 2.5196 acc_train: 0.0000 loss_val: 2.5033 acc_val: 0.0000 time: 0.4038s\n",
      "Epoch: 0003 loss_train: 2.5157 acc_train: 0.0001 loss_val: 2.5022 acc_val: 0.0041 time: 0.3798s\n",
      "Epoch: 0004 loss_train: 2.5120 acc_train: 0.0035 loss_val: 2.5012 acc_val: 0.0671 time: 0.3978s\n",
      "Epoch: 0005 loss_train: 2.5082 acc_train: 0.0854 loss_val: 2.5001 acc_val: 0.0758 time: 0.3768s\n",
      "Epoch: 0006 loss_train: 2.5047 acc_train: 0.0961 loss_val: 2.4991 acc_val: 0.0758 time: 0.3999s\n",
      "Epoch: 0007 loss_train: 2.5006 acc_train: 0.0961 loss_val: 2.4981 acc_val: 0.0758 time: 0.3971s\n",
      "Epoch: 0008 loss_train: 2.4969 acc_train: 0.0961 loss_val: 2.4969 acc_val: 0.0758 time: 0.4060s\n",
      "Epoch: 0009 loss_train: 2.4933 acc_train: 0.0961 loss_val: 2.4958 acc_val: 0.0758 time: 0.4177s\n",
      "Epoch: 0010 loss_train: 2.4893 acc_train: 0.0961 loss_val: 2.4946 acc_val: 0.0758 time: 0.3719s\n",
      "Epoch: 0011 loss_train: 2.4850 acc_train: 0.0963 loss_val: 2.4933 acc_val: 0.0758 time: 0.3492s\n",
      "Epoch: 0012 loss_train: 2.4812 acc_train: 0.0966 loss_val: 2.4920 acc_val: 0.0758 time: 0.3831s\n",
      "Epoch: 0013 loss_train: 2.4766 acc_train: 0.0969 loss_val: 2.4908 acc_val: 0.0758 time: 0.3457s\n",
      "Epoch: 0014 loss_train: 2.4726 acc_train: 0.0976 loss_val: 2.4894 acc_val: 0.0758 time: 0.3716s\n",
      "Epoch: 0015 loss_train: 2.4682 acc_train: 0.0998 loss_val: 2.4880 acc_val: 0.0758 time: 0.3967s\n",
      "Epoch: 0016 loss_train: 2.4645 acc_train: 0.0996 loss_val: 2.4866 acc_val: 0.0748 time: 0.4119s\n",
      "Epoch: 0017 loss_train: 2.4602 acc_train: 0.1024 loss_val: 2.4851 acc_val: 0.0748 time: 0.3844s\n",
      "Epoch: 0018 loss_train: 2.4555 acc_train: 0.1017 loss_val: 2.4836 acc_val: 0.0794 time: 0.4002s\n",
      "Epoch: 0019 loss_train: 2.4513 acc_train: 0.1020 loss_val: 2.4820 acc_val: 0.0778 time: 0.3727s\n",
      "Epoch: 0020 loss_train: 2.4469 acc_train: 0.1041 loss_val: 2.4803 acc_val: 0.0768 time: 0.4033s\n",
      "Epoch: 0021 loss_train: 2.4432 acc_train: 0.1033 loss_val: 2.4786 acc_val: 0.0778 time: 0.4245s\n",
      "Epoch: 0022 loss_train: 2.4406 acc_train: 0.1017 loss_val: 2.4768 acc_val: 0.0891 time: 0.3956s\n",
      "Epoch: 0023 loss_train: 2.4365 acc_train: 0.1014 loss_val: 2.4749 acc_val: 0.0881 time: 0.3919s\n",
      "Epoch: 0024 loss_train: 2.4315 acc_train: 0.1054 loss_val: 2.4729 acc_val: 0.0840 time: 0.3512s\n",
      "Epoch: 0025 loss_train: 2.4286 acc_train: 0.1076 loss_val: 2.4707 acc_val: 0.0881 time: 0.3941s\n",
      "Epoch: 0026 loss_train: 2.4260 acc_train: 0.1076 loss_val: 2.4684 acc_val: 0.0860 time: 0.3915s\n",
      "Epoch: 0027 loss_train: 2.4232 acc_train: 0.1064 loss_val: 2.4660 acc_val: 0.0850 time: 0.3875s\n",
      "Epoch: 0028 loss_train: 2.4194 acc_train: 0.1089 loss_val: 2.4634 acc_val: 0.0835 time: 0.4033s\n",
      "Epoch: 0029 loss_train: 2.4163 acc_train: 0.1154 loss_val: 2.4606 acc_val: 0.0829 time: 0.3661s\n",
      "Epoch: 0030 loss_train: 2.4145 acc_train: 0.1124 loss_val: 2.4577 acc_val: 0.0824 time: 0.3511s\n",
      "Epoch: 0031 loss_train: 2.4121 acc_train: 0.1130 loss_val: 2.4545 acc_val: 0.0819 time: 0.3774s\n",
      "Epoch: 0032 loss_train: 2.4087 acc_train: 0.1130 loss_val: 2.4511 acc_val: 0.0819 time: 0.4087s\n",
      "Epoch: 0033 loss_train: 2.4063 acc_train: 0.1130 loss_val: 2.4475 acc_val: 0.0829 time: 0.3633s\n",
      "Epoch: 0034 loss_train: 2.4043 acc_train: 0.1198 loss_val: 2.4439 acc_val: 0.0845 time: 0.3948s\n",
      "Epoch: 0035 loss_train: 2.4015 acc_train: 0.1218 loss_val: 2.4402 acc_val: 0.0968 time: 0.3515s\n",
      "Epoch: 0036 loss_train: 2.3978 acc_train: 0.1262 loss_val: 2.4363 acc_val: 0.0922 time: 0.4144s\n",
      "Epoch: 0037 loss_train: 2.3967 acc_train: 0.1267 loss_val: 2.4325 acc_val: 0.0993 time: 0.3844s\n",
      "Epoch: 0038 loss_train: 2.3948 acc_train: 0.1180 loss_val: 2.4288 acc_val: 0.1009 time: 0.4029s\n",
      "Epoch: 0039 loss_train: 2.3912 acc_train: 0.1225 loss_val: 2.4253 acc_val: 0.0983 time: 0.4325s\n",
      "Epoch: 0040 loss_train: 2.3904 acc_train: 0.1212 loss_val: 2.4219 acc_val: 0.0973 time: 0.4316s\n",
      "Epoch: 0041 loss_train: 2.3888 acc_train: 0.1205 loss_val: 2.4189 acc_val: 0.0973 time: 0.4261s\n",
      "Epoch: 0042 loss_train: 2.3876 acc_train: 0.1202 loss_val: 2.4160 acc_val: 0.0983 time: 0.3953s\n",
      "Epoch: 0043 loss_train: 2.3859 acc_train: 0.1203 loss_val: 2.4135 acc_val: 0.1034 time: 0.3779s\n",
      "Epoch: 0044 loss_train: 2.3831 acc_train: 0.1220 loss_val: 2.4113 acc_val: 0.1024 time: 0.3835s\n",
      "Epoch: 0045 loss_train: 2.3837 acc_train: 0.1231 loss_val: 2.4094 acc_val: 0.1024 time: 0.3731s\n",
      "Epoch: 0046 loss_train: 2.3818 acc_train: 0.1206 loss_val: 2.4079 acc_val: 0.0968 time: 0.4267s\n",
      "Epoch: 0047 loss_train: 2.3806 acc_train: 0.1227 loss_val: 2.4065 acc_val: 0.0947 time: 0.3889s\n",
      "Epoch: 0048 loss_train: 2.3781 acc_train: 0.1286 loss_val: 2.4053 acc_val: 0.0911 time: 0.3963s\n",
      "Epoch: 0049 loss_train: 2.3779 acc_train: 0.1234 loss_val: 2.4043 acc_val: 0.0911 time: 0.4187s\n",
      "Epoch: 0050 loss_train: 2.3765 acc_train: 0.1283 loss_val: 2.4035 acc_val: 0.0922 time: 0.4123s\n",
      "Epoch: 0051 loss_train: 2.3751 acc_train: 0.1297 loss_val: 2.4026 acc_val: 0.0958 time: 0.3986s\n",
      "Epoch: 0052 loss_train: 2.3736 acc_train: 0.1275 loss_val: 2.4018 acc_val: 0.0963 time: 0.4007s\n",
      "Epoch: 0053 loss_train: 2.3736 acc_train: 0.1312 loss_val: 2.4010 acc_val: 0.1004 time: 0.4166s\n",
      "Epoch: 0054 loss_train: 2.3737 acc_train: 0.1294 loss_val: 2.4001 acc_val: 0.0993 time: 0.4259s\n",
      "Epoch: 0055 loss_train: 2.3708 acc_train: 0.1325 loss_val: 2.3992 acc_val: 0.0958 time: 0.4085s\n",
      "Epoch: 0056 loss_train: 2.3677 acc_train: 0.1353 loss_val: 2.3980 acc_val: 0.0922 time: 0.4283s\n",
      "Epoch: 0057 loss_train: 2.3684 acc_train: 0.1284 loss_val: 2.3966 acc_val: 0.0917 time: 0.4627s\n",
      "Epoch: 0058 loss_train: 2.3667 acc_train: 0.1333 loss_val: 2.3951 acc_val: 0.0906 time: 0.4704s\n",
      "Epoch: 0059 loss_train: 2.3681 acc_train: 0.1290 loss_val: 2.3935 acc_val: 0.0927 time: 0.4230s\n",
      "Epoch: 0060 loss_train: 2.3682 acc_train: 0.1227 loss_val: 2.3919 acc_val: 0.0937 time: 0.4037s\n",
      "Epoch: 0061 loss_train: 2.3657 acc_train: 0.1250 loss_val: 2.3904 acc_val: 0.0932 time: 0.3936s\n",
      "Epoch: 0062 loss_train: 2.3644 acc_train: 0.1269 loss_val: 2.3890 acc_val: 0.0927 time: 0.3895s\n",
      "Epoch: 0063 loss_train: 2.3625 acc_train: 0.1268 loss_val: 2.3877 acc_val: 0.0978 time: 0.4024s\n",
      "Epoch: 0064 loss_train: 2.3607 acc_train: 0.1287 loss_val: 2.3865 acc_val: 0.0973 time: 0.4092s\n",
      "Epoch: 0065 loss_train: 2.3646 acc_train: 0.1246 loss_val: 2.3854 acc_val: 0.0963 time: 0.3871s\n",
      "Epoch: 0066 loss_train: 2.3621 acc_train: 0.1293 loss_val: 2.3845 acc_val: 0.0952 time: 0.3514s\n",
      "Epoch: 0067 loss_train: 2.3622 acc_train: 0.1262 loss_val: 2.3837 acc_val: 0.0958 time: 0.4022s\n",
      "Epoch: 0068 loss_train: 2.3584 acc_train: 0.1275 loss_val: 2.3829 acc_val: 0.0906 time: 0.3533s\n",
      "Epoch: 0069 loss_train: 2.3582 acc_train: 0.1281 loss_val: 2.3822 acc_val: 0.0917 time: 0.3573s\n",
      "Epoch: 0070 loss_train: 2.3592 acc_train: 0.1299 loss_val: 2.3815 acc_val: 0.0917 time: 0.4044s\n",
      "Epoch: 0071 loss_train: 2.3568 acc_train: 0.1316 loss_val: 2.3809 acc_val: 0.0911 time: 0.3956s\n",
      "Epoch: 0072 loss_train: 2.3586 acc_train: 0.1303 loss_val: 2.3801 acc_val: 0.0906 time: 0.3719s\n",
      "Epoch: 0073 loss_train: 2.3568 acc_train: 0.1252 loss_val: 2.3795 acc_val: 0.0891 time: 0.3817s\n",
      "Epoch: 0074 loss_train: 2.3594 acc_train: 0.1244 loss_val: 2.3788 acc_val: 0.0901 time: 0.4020s\n",
      "Epoch: 0075 loss_train: 2.3578 acc_train: 0.1250 loss_val: 2.3780 acc_val: 0.0886 time: 0.3513s\n",
      "Epoch: 0076 loss_train: 2.3560 acc_train: 0.1220 loss_val: 2.3772 acc_val: 0.0891 time: 0.3872s\n",
      "Epoch: 0077 loss_train: 2.3555 acc_train: 0.1231 loss_val: 2.3764 acc_val: 0.0901 time: 0.3700s\n",
      "Epoch: 0078 loss_train: 2.3554 acc_train: 0.1225 loss_val: 2.3756 acc_val: 0.0911 time: 0.3858s\n",
      "Epoch: 0079 loss_train: 2.3538 acc_train: 0.1269 loss_val: 2.3747 acc_val: 0.0911 time: 0.4157s\n",
      "Epoch: 0080 loss_train: 2.3567 acc_train: 0.1180 loss_val: 2.3741 acc_val: 0.1106 time: 0.4032s\n",
      "Epoch: 0081 loss_train: 2.3543 acc_train: 0.1198 loss_val: 2.3736 acc_val: 0.1091 time: 0.3945s\n",
      "Epoch: 0082 loss_train: 2.3537 acc_train: 0.1192 loss_val: 2.3730 acc_val: 0.1086 time: 0.3620s\n",
      "Epoch: 0083 loss_train: 2.3529 acc_train: 0.1234 loss_val: 2.3723 acc_val: 0.1091 time: 0.3634s\n",
      "Epoch: 0084 loss_train: 2.3534 acc_train: 0.1187 loss_val: 2.3714 acc_val: 0.1075 time: 0.3669s\n",
      "Epoch: 0085 loss_train: 2.3521 acc_train: 0.1234 loss_val: 2.3705 acc_val: 0.1065 time: 0.3558s\n",
      "Epoch: 0086 loss_train: 2.3556 acc_train: 0.1225 loss_val: 2.3696 acc_val: 0.0993 time: 0.3950s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 2.3511 acc_train: 0.1212 loss_val: 2.3688 acc_val: 0.1009 time: 0.3799s\n",
      "Epoch: 0088 loss_train: 2.3505 acc_train: 0.1196 loss_val: 2.3681 acc_val: 0.1009 time: 0.3939s\n",
      "Epoch: 0089 loss_train: 2.3507 acc_train: 0.1177 loss_val: 2.3675 acc_val: 0.1019 time: 0.3945s\n",
      "Epoch: 0090 loss_train: 2.3486 acc_train: 0.1306 loss_val: 2.3670 acc_val: 0.1019 time: 0.3850s\n",
      "Epoch: 0091 loss_train: 2.3476 acc_train: 0.1374 loss_val: 2.3666 acc_val: 0.1019 time: 0.3756s\n",
      "Epoch: 0092 loss_train: 2.3483 acc_train: 0.1250 loss_val: 2.3662 acc_val: 0.1019 time: 0.3633s\n",
      "Epoch: 0093 loss_train: 2.3501 acc_train: 0.1180 loss_val: 2.3658 acc_val: 0.1029 time: 0.4034s\n",
      "Epoch: 0094 loss_train: 2.3489 acc_train: 0.1244 loss_val: 2.3653 acc_val: 0.1019 time: 0.4212s\n",
      "Epoch: 0095 loss_train: 2.3486 acc_train: 0.1165 loss_val: 2.3649 acc_val: 0.0993 time: 0.3662s\n",
      "Epoch: 0096 loss_train: 2.3492 acc_train: 0.1218 loss_val: 2.3644 acc_val: 0.1039 time: 0.3674s\n",
      "Epoch: 0097 loss_train: 2.3485 acc_train: 0.1193 loss_val: 2.3639 acc_val: 0.1039 time: 0.3748s\n",
      "Epoch: 0098 loss_train: 2.3504 acc_train: 0.1211 loss_val: 2.3633 acc_val: 0.1039 time: 0.3844s\n",
      "Epoch: 0099 loss_train: 2.3460 acc_train: 0.1253 loss_val: 2.3626 acc_val: 0.1039 time: 0.3804s\n",
      "Epoch: 0100 loss_train: 2.3481 acc_train: 0.1250 loss_val: 2.3617 acc_val: 0.1039 time: 0.3523s\n",
      "Epoch: 0101 loss_train: 2.3474 acc_train: 0.1268 loss_val: 2.3611 acc_val: 0.1065 time: 0.3683s\n",
      "Epoch: 0102 loss_train: 2.3465 acc_train: 0.1380 loss_val: 2.3604 acc_val: 0.1039 time: 0.3861s\n",
      "Epoch: 0103 loss_train: 2.3454 acc_train: 0.1384 loss_val: 2.3596 acc_val: 0.1045 time: 0.3793s\n",
      "Epoch: 0104 loss_train: 2.3481 acc_train: 0.1368 loss_val: 2.3590 acc_val: 0.1039 time: 0.3918s\n",
      "Epoch: 0105 loss_train: 2.3479 acc_train: 0.1380 loss_val: 2.3585 acc_val: 0.1029 time: 0.3796s\n",
      "Epoch: 0106 loss_train: 2.3455 acc_train: 0.1437 loss_val: 2.3579 acc_val: 0.1086 time: 0.3794s\n",
      "Epoch: 0107 loss_train: 2.3463 acc_train: 0.1390 loss_val: 2.3575 acc_val: 0.1065 time: 0.3699s\n",
      "Epoch: 0108 loss_train: 2.3447 acc_train: 0.1517 loss_val: 2.3571 acc_val: 0.1039 time: 0.3803s\n",
      "Epoch: 0109 loss_train: 2.3432 acc_train: 0.1426 loss_val: 2.3568 acc_val: 0.1039 time: 0.3662s\n",
      "Epoch: 0110 loss_train: 2.3429 acc_train: 0.1450 loss_val: 2.3565 acc_val: 0.1039 time: 0.4051s\n",
      "Epoch: 0111 loss_train: 2.3415 acc_train: 0.1403 loss_val: 2.3563 acc_val: 0.1045 time: 0.3691s\n",
      "Epoch: 0112 loss_train: 2.3484 acc_train: 0.1397 loss_val: 2.3563 acc_val: 0.1039 time: 0.3506s\n",
      "Epoch: 0113 loss_train: 2.3448 acc_train: 0.1422 loss_val: 2.3564 acc_val: 0.1045 time: 0.3800s\n",
      "Epoch: 0114 loss_train: 2.3454 acc_train: 0.1426 loss_val: 2.3564 acc_val: 0.1019 time: 0.4002s\n",
      "Epoch: 0115 loss_train: 2.3439 acc_train: 0.1393 loss_val: 2.3561 acc_val: 0.1004 time: 0.3762s\n",
      "Epoch: 0116 loss_train: 2.3461 acc_train: 0.1491 loss_val: 2.3556 acc_val: 0.1034 time: 0.3649s\n",
      "Epoch: 0117 loss_train: 2.3437 acc_train: 0.1487 loss_val: 2.3549 acc_val: 0.1080 time: 0.3504s\n",
      "Epoch: 0118 loss_train: 2.3440 acc_train: 0.1448 loss_val: 2.3541 acc_val: 0.1260 time: 0.3931s\n",
      "Epoch: 0119 loss_train: 2.3394 acc_train: 0.1525 loss_val: 2.3533 acc_val: 0.1224 time: 0.4093s\n",
      "Epoch: 0120 loss_train: 2.3415 acc_train: 0.1591 loss_val: 2.3526 acc_val: 0.1224 time: 0.3443s\n",
      "Epoch: 0121 loss_train: 2.3389 acc_train: 0.1531 loss_val: 2.3520 acc_val: 0.1234 time: 0.3900s\n",
      "Epoch: 0122 loss_train: 2.3406 acc_train: 0.1520 loss_val: 2.3515 acc_val: 0.1254 time: 0.3571s\n",
      "Epoch: 0123 loss_train: 2.3434 acc_train: 0.1459 loss_val: 2.3512 acc_val: 0.1260 time: 0.4197s\n",
      "Epoch: 0124 loss_train: 2.3426 acc_train: 0.1585 loss_val: 2.3510 acc_val: 0.1239 time: 0.4173s\n",
      "Epoch: 0125 loss_train: 2.3419 acc_train: 0.1487 loss_val: 2.3508 acc_val: 0.1203 time: 0.3578s\n",
      "Epoch: 0126 loss_train: 2.3392 acc_train: 0.1495 loss_val: 2.3507 acc_val: 0.1024 time: 0.4107s\n",
      "Epoch: 0127 loss_train: 2.3391 acc_train: 0.1525 loss_val: 2.3506 acc_val: 0.1014 time: 0.3699s\n",
      "Epoch: 0128 loss_train: 2.3400 acc_train: 0.1520 loss_val: 2.3504 acc_val: 0.1039 time: 0.4068s\n",
      "Epoch: 0129 loss_train: 2.3379 acc_train: 0.1503 loss_val: 2.3501 acc_val: 0.1019 time: 0.3770s\n",
      "Epoch: 0130 loss_train: 2.3391 acc_train: 0.1406 loss_val: 2.3498 acc_val: 0.1229 time: 0.3753s\n",
      "Epoch: 0131 loss_train: 2.3419 acc_train: 0.1479 loss_val: 2.3494 acc_val: 0.1254 time: 0.4171s\n",
      "Epoch: 0132 loss_train: 2.3411 acc_train: 0.1526 loss_val: 2.3489 acc_val: 0.1219 time: 0.3694s\n",
      "Epoch: 0133 loss_train: 2.3413 acc_train: 0.1592 loss_val: 2.3485 acc_val: 0.1203 time: 0.3269s\n",
      "Epoch: 0134 loss_train: 2.3381 acc_train: 0.1578 loss_val: 2.3480 acc_val: 0.1208 time: 0.4009s\n",
      "Epoch: 0135 loss_train: 2.3411 acc_train: 0.1498 loss_val: 2.3475 acc_val: 0.1203 time: 0.4068s\n",
      "Epoch: 0136 loss_train: 2.3405 acc_train: 0.1498 loss_val: 2.3473 acc_val: 0.1203 time: 0.3888s\n",
      "Epoch: 0137 loss_train: 2.3405 acc_train: 0.1494 loss_val: 2.3471 acc_val: 0.1214 time: 0.3474s\n",
      "Epoch: 0138 loss_train: 2.3356 acc_train: 0.1569 loss_val: 2.3469 acc_val: 0.1208 time: 0.3690s\n",
      "Epoch: 0139 loss_train: 2.3366 acc_train: 0.1526 loss_val: 2.3467 acc_val: 0.1173 time: 0.3729s\n",
      "Epoch: 0140 loss_train: 2.3338 acc_train: 0.1495 loss_val: 2.3465 acc_val: 0.1162 time: 0.3742s\n",
      "Epoch: 0141 loss_train: 2.3351 acc_train: 0.1520 loss_val: 2.3465 acc_val: 0.1162 time: 0.3947s\n",
      "Epoch: 0142 loss_train: 2.3375 acc_train: 0.1481 loss_val: 2.3464 acc_val: 0.1178 time: 0.4186s\n",
      "Epoch: 0143 loss_train: 2.3376 acc_train: 0.1513 loss_val: 2.3462 acc_val: 0.1198 time: 0.3648s\n",
      "Epoch: 0144 loss_train: 2.3378 acc_train: 0.1541 loss_val: 2.3457 acc_val: 0.1188 time: 0.3984s\n",
      "Epoch: 0145 loss_train: 2.3386 acc_train: 0.1575 loss_val: 2.3451 acc_val: 0.1198 time: 0.3697s\n",
      "Epoch: 0146 loss_train: 2.3376 acc_train: 0.1515 loss_val: 2.3447 acc_val: 0.1193 time: 0.3944s\n",
      "Epoch: 0147 loss_train: 2.3347 acc_train: 0.1585 loss_val: 2.3442 acc_val: 0.1188 time: 0.3825s\n",
      "Epoch: 0148 loss_train: 2.3371 acc_train: 0.1539 loss_val: 2.3436 acc_val: 0.1219 time: 0.4170s\n",
      "Epoch: 0149 loss_train: 2.3314 acc_train: 0.1528 loss_val: 2.3429 acc_val: 0.1423 time: 0.4170s\n",
      "Epoch: 0150 loss_train: 2.3352 acc_train: 0.1520 loss_val: 2.3424 acc_val: 0.1418 time: 0.3928s\n",
      "Epoch: 0151 loss_train: 2.3362 acc_train: 0.1447 loss_val: 2.3419 acc_val: 0.1377 time: 0.4198s\n",
      "Epoch: 0152 loss_train: 2.3372 acc_train: 0.1501 loss_val: 2.3415 acc_val: 0.1382 time: 0.3729s\n",
      "Epoch: 0153 loss_train: 2.3343 acc_train: 0.1484 loss_val: 2.3412 acc_val: 0.1382 time: 0.3948s\n",
      "Epoch: 0154 loss_train: 2.3333 acc_train: 0.1520 loss_val: 2.3409 acc_val: 0.1377 time: 0.3753s\n",
      "Epoch: 0155 loss_train: 2.3324 acc_train: 0.1484 loss_val: 2.3408 acc_val: 0.1372 time: 0.3817s\n",
      "Epoch: 0156 loss_train: 2.3369 acc_train: 0.1494 loss_val: 2.3407 acc_val: 0.1393 time: 0.3809s\n",
      "Epoch: 0157 loss_train: 2.3356 acc_train: 0.1498 loss_val: 2.3408 acc_val: 0.1439 time: 0.3940s\n",
      "Epoch: 0158 loss_train: 2.3314 acc_train: 0.1478 loss_val: 2.3408 acc_val: 0.1429 time: 0.3881s\n",
      "Epoch: 0159 loss_train: 2.3389 acc_train: 0.1490 loss_val: 2.3406 acc_val: 0.1429 time: 0.4052s\n",
      "Epoch: 0160 loss_train: 2.3331 acc_train: 0.1497 loss_val: 2.3402 acc_val: 0.1434 time: 0.4192s\n",
      "Epoch: 0161 loss_train: 2.3296 acc_train: 0.1569 loss_val: 2.3399 acc_val: 0.1367 time: 0.3851s\n",
      "Epoch: 0162 loss_train: 2.3314 acc_train: 0.1503 loss_val: 2.3393 acc_val: 0.1331 time: 0.4150s\n",
      "Epoch: 0163 loss_train: 2.3314 acc_train: 0.1469 loss_val: 2.3390 acc_val: 0.1342 time: 0.3853s\n",
      "Epoch: 0164 loss_train: 2.3341 acc_train: 0.1479 loss_val: 2.3386 acc_val: 0.1336 time: 0.4025s\n",
      "Epoch: 0165 loss_train: 2.3312 acc_train: 0.1454 loss_val: 2.3383 acc_val: 0.1331 time: 0.3930s\n",
      "Epoch: 0166 loss_train: 2.3313 acc_train: 0.1517 loss_val: 2.3379 acc_val: 0.1444 time: 0.4084s\n",
      "Epoch: 0167 loss_train: 2.3306 acc_train: 0.1498 loss_val: 2.3377 acc_val: 0.1444 time: 0.4068s\n",
      "Epoch: 0168 loss_train: 2.3359 acc_train: 0.1488 loss_val: 2.3374 acc_val: 0.1418 time: 0.4005s\n",
      "Epoch: 0169 loss_train: 2.3339 acc_train: 0.1512 loss_val: 2.3371 acc_val: 0.1418 time: 0.3247s\n",
      "Epoch: 0170 loss_train: 2.3310 acc_train: 0.1476 loss_val: 2.3368 acc_val: 0.1439 time: 0.4056s\n",
      "Epoch: 0171 loss_train: 2.3313 acc_train: 0.1500 loss_val: 2.3364 acc_val: 0.1444 time: 0.4166s\n",
      "Epoch: 0172 loss_train: 2.3302 acc_train: 0.1482 loss_val: 2.3363 acc_val: 0.1423 time: 0.3784s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0173 loss_train: 2.3295 acc_train: 0.1481 loss_val: 2.3362 acc_val: 0.1434 time: 0.4005s\n",
      "Epoch: 0174 loss_train: 2.3315 acc_train: 0.1523 loss_val: 2.3360 acc_val: 0.1434 time: 0.3524s\n",
      "Epoch: 0175 loss_train: 2.3339 acc_train: 0.1437 loss_val: 2.3358 acc_val: 0.1357 time: 0.3653s\n",
      "Epoch: 0176 loss_train: 2.3290 acc_train: 0.1476 loss_val: 2.3354 acc_val: 0.1342 time: 0.3807s\n",
      "Epoch: 0177 loss_train: 2.3296 acc_train: 0.1429 loss_val: 2.3351 acc_val: 0.1342 time: 0.4020s\n",
      "Epoch: 0178 loss_train: 2.3324 acc_train: 0.1428 loss_val: 2.3348 acc_val: 0.1357 time: 0.4037s\n",
      "Epoch: 0179 loss_train: 2.3299 acc_train: 0.1416 loss_val: 2.3345 acc_val: 0.1347 time: 0.3582s\n",
      "Epoch: 0180 loss_train: 2.3332 acc_train: 0.1451 loss_val: 2.3343 acc_val: 0.1367 time: 0.3867s\n",
      "Epoch: 0181 loss_train: 2.3310 acc_train: 0.1426 loss_val: 2.3342 acc_val: 0.1347 time: 0.4066s\n",
      "Epoch: 0182 loss_train: 2.3306 acc_train: 0.1378 loss_val: 2.3341 acc_val: 0.1367 time: 0.3933s\n",
      "Epoch: 0183 loss_train: 2.3325 acc_train: 0.1402 loss_val: 2.3339 acc_val: 0.1382 time: 0.3741s\n",
      "Epoch: 0184 loss_train: 2.3306 acc_train: 0.1443 loss_val: 2.3338 acc_val: 0.1352 time: 0.3941s\n",
      "Epoch: 0185 loss_train: 2.3335 acc_train: 0.1438 loss_val: 2.3337 acc_val: 0.1357 time: 0.3631s\n",
      "Epoch: 0186 loss_train: 2.3330 acc_train: 0.1447 loss_val: 2.3336 acc_val: 0.1372 time: 0.3963s\n",
      "Epoch: 0187 loss_train: 2.3299 acc_train: 0.1384 loss_val: 2.3335 acc_val: 0.1367 time: 0.3824s\n",
      "Epoch: 0188 loss_train: 2.3309 acc_train: 0.1422 loss_val: 2.3332 acc_val: 0.1362 time: 0.3655s\n",
      "Epoch: 0189 loss_train: 2.3318 acc_train: 0.1444 loss_val: 2.3330 acc_val: 0.1357 time: 0.3819s\n",
      "Epoch: 0190 loss_train: 2.3347 acc_train: 0.1318 loss_val: 2.3326 acc_val: 0.1377 time: 0.3907s\n",
      "Epoch: 0191 loss_train: 2.3332 acc_train: 0.1338 loss_val: 2.3323 acc_val: 0.1388 time: 0.3696s\n",
      "Epoch: 0192 loss_train: 2.3321 acc_train: 0.1466 loss_val: 2.3319 acc_val: 0.1382 time: 0.3934s\n",
      "Epoch: 0193 loss_train: 2.3320 acc_train: 0.1475 loss_val: 2.3314 acc_val: 0.1382 time: 0.3619s\n",
      "Epoch: 0194 loss_train: 2.3308 acc_train: 0.1407 loss_val: 2.3312 acc_val: 0.1388 time: 0.3739s\n",
      "Epoch: 0195 loss_train: 2.3318 acc_train: 0.1437 loss_val: 2.3311 acc_val: 0.1377 time: 0.3604s\n",
      "Epoch: 0196 loss_train: 2.3276 acc_train: 0.1494 loss_val: 2.3310 acc_val: 0.1372 time: 0.3631s\n",
      "Epoch: 0197 loss_train: 2.3272 acc_train: 0.1506 loss_val: 2.3310 acc_val: 0.1357 time: 0.3894s\n",
      "Epoch: 0198 loss_train: 2.3311 acc_train: 0.1377 loss_val: 2.3310 acc_val: 0.1357 time: 0.3683s\n",
      "Epoch: 0199 loss_train: 2.3294 acc_train: 0.1465 loss_val: 2.3311 acc_val: 0.1362 time: 0.3581s\n",
      "Epoch: 0200 loss_train: 2.3326 acc_train: 0.1394 loss_val: 2.3311 acc_val: 0.1388 time: 0.3749s\n",
      "Epoch: 0201 loss_train: 2.3319 acc_train: 0.1328 loss_val: 2.3310 acc_val: 0.1572 time: 0.3645s\n",
      "Epoch: 0202 loss_train: 2.3305 acc_train: 0.1380 loss_val: 2.3308 acc_val: 0.1582 time: 0.3759s\n",
      "Epoch: 0203 loss_train: 2.3283 acc_train: 0.1438 loss_val: 2.3306 acc_val: 0.1592 time: 0.4032s\n",
      "Epoch: 0204 loss_train: 2.3291 acc_train: 0.1407 loss_val: 2.3303 acc_val: 0.1587 time: 0.3815s\n",
      "Epoch: 0205 loss_train: 2.3286 acc_train: 0.1403 loss_val: 2.3300 acc_val: 0.1582 time: 0.3756s\n",
      "Epoch: 0206 loss_train: 2.3296 acc_train: 0.1426 loss_val: 2.3295 acc_val: 0.1582 time: 0.3872s\n",
      "Epoch: 0207 loss_train: 2.3306 acc_train: 0.1404 loss_val: 2.3291 acc_val: 0.1587 time: 0.4037s\n",
      "Epoch: 0208 loss_train: 2.3352 acc_train: 0.1334 loss_val: 2.3288 acc_val: 0.1587 time: 0.3809s\n",
      "Epoch: 0209 loss_train: 2.3303 acc_train: 0.1372 loss_val: 2.3287 acc_val: 0.1587 time: 0.3795s\n",
      "Epoch: 0210 loss_train: 2.3248 acc_train: 0.1356 loss_val: 2.3286 acc_val: 0.1562 time: 0.3597s\n",
      "Epoch: 0211 loss_train: 2.3230 acc_train: 0.1434 loss_val: 2.3287 acc_val: 0.1587 time: 0.3962s\n",
      "Epoch: 0212 loss_train: 2.3278 acc_train: 0.1504 loss_val: 2.3288 acc_val: 0.1562 time: 0.4015s\n",
      "Epoch: 0213 loss_train: 2.3274 acc_train: 0.1425 loss_val: 2.3289 acc_val: 0.1587 time: 0.3568s\n",
      "Epoch: 0214 loss_train: 2.3348 acc_train: 0.1426 loss_val: 2.3290 acc_val: 0.1572 time: 0.3946s\n",
      "Epoch: 0215 loss_train: 2.3278 acc_train: 0.1429 loss_val: 2.3291 acc_val: 0.1382 time: 0.3657s\n",
      "Epoch: 0216 loss_train: 2.3264 acc_train: 0.1469 loss_val: 2.3290 acc_val: 0.1377 time: 0.3518s\n",
      "Epoch: 0217 loss_train: 2.3283 acc_train: 0.1471 loss_val: 2.3288 acc_val: 0.1577 time: 0.3880s\n",
      "Epoch: 0218 loss_train: 2.3252 acc_train: 0.1412 loss_val: 2.3282 acc_val: 0.1567 time: 0.3799s\n",
      "Epoch: 0219 loss_train: 2.3297 acc_train: 0.1415 loss_val: 2.3276 acc_val: 0.1582 time: 0.3897s\n",
      "Epoch: 0220 loss_train: 2.3320 acc_train: 0.1425 loss_val: 2.3270 acc_val: 0.1577 time: 0.3715s\n",
      "Epoch: 0221 loss_train: 2.3307 acc_train: 0.1418 loss_val: 2.3265 acc_val: 0.1577 time: 0.3557s\n",
      "Epoch: 0222 loss_train: 2.3268 acc_train: 0.1454 loss_val: 2.3261 acc_val: 0.1510 time: 0.3664s\n",
      "Epoch: 0223 loss_train: 2.3267 acc_train: 0.1428 loss_val: 2.3257 acc_val: 0.1521 time: 0.3709s\n",
      "Epoch: 0224 loss_train: 2.3268 acc_train: 0.1456 loss_val: 2.3255 acc_val: 0.1521 time: 0.4047s\n",
      "Epoch: 0225 loss_train: 2.3272 acc_train: 0.1418 loss_val: 2.3256 acc_val: 0.1557 time: 0.3903s\n",
      "Epoch: 0226 loss_train: 2.3255 acc_train: 0.1416 loss_val: 2.3257 acc_val: 0.1572 time: 0.3800s\n",
      "Epoch: 0227 loss_train: 2.3282 acc_train: 0.1431 loss_val: 2.3260 acc_val: 0.1598 time: 0.3814s\n",
      "Epoch: 0228 loss_train: 2.3248 acc_train: 0.1460 loss_val: 2.3262 acc_val: 0.1592 time: 0.3496s\n",
      "Epoch: 0229 loss_train: 2.3295 acc_train: 0.1396 loss_val: 2.3263 acc_val: 0.1598 time: 0.3612s\n",
      "Epoch: 0230 loss_train: 2.3263 acc_train: 0.1387 loss_val: 2.3262 acc_val: 0.1572 time: 0.3606s\n",
      "Epoch: 0231 loss_train: 2.3292 acc_train: 0.1394 loss_val: 2.3262 acc_val: 0.1587 time: 0.3584s\n",
      "Epoch: 0232 loss_train: 2.3267 acc_train: 0.1359 loss_val: 2.3261 acc_val: 0.1587 time: 0.3891s\n",
      "Epoch: 0233 loss_train: 2.3269 acc_train: 0.1510 loss_val: 2.3258 acc_val: 0.1562 time: 0.3475s\n",
      "Epoch: 0234 loss_train: 2.3282 acc_train: 0.1381 loss_val: 2.3254 acc_val: 0.1582 time: 0.3653s\n",
      "Epoch: 0235 loss_train: 2.3233 acc_train: 0.1438 loss_val: 2.3251 acc_val: 0.1582 time: 0.3661s\n",
      "Epoch: 0236 loss_train: 2.3248 acc_train: 0.1371 loss_val: 2.3245 acc_val: 0.1587 time: 0.3834s\n",
      "Epoch: 0237 loss_train: 2.3262 acc_train: 0.1459 loss_val: 2.3240 acc_val: 0.1582 time: 0.3831s\n",
      "Epoch: 0238 loss_train: 2.3282 acc_train: 0.1374 loss_val: 2.3237 acc_val: 0.1582 time: 0.3803s\n",
      "Epoch: 0239 loss_train: 2.3250 acc_train: 0.1469 loss_val: 2.3234 acc_val: 0.1536 time: 0.3974s\n",
      "Epoch: 0240 loss_train: 2.3272 acc_train: 0.1487 loss_val: 2.3232 acc_val: 0.1536 time: 0.3818s\n",
      "Epoch: 0241 loss_train: 2.3287 acc_train: 0.1415 loss_val: 2.3231 acc_val: 0.1536 time: 0.3908s\n",
      "Epoch: 0242 loss_train: 2.3268 acc_train: 0.1415 loss_val: 2.3231 acc_val: 0.1536 time: 0.3779s\n",
      "Epoch: 0243 loss_train: 2.3225 acc_train: 0.1443 loss_val: 2.3233 acc_val: 0.1536 time: 0.3698s\n",
      "Epoch: 0244 loss_train: 2.3233 acc_train: 0.1415 loss_val: 2.3235 acc_val: 0.1531 time: 0.4283s\n",
      "Epoch: 0245 loss_train: 2.3265 acc_train: 0.1353 loss_val: 2.3234 acc_val: 0.1526 time: 0.3692s\n",
      "Epoch: 0246 loss_train: 2.3262 acc_train: 0.1425 loss_val: 2.3233 acc_val: 0.1531 time: 0.3874s\n",
      "Epoch: 0247 loss_train: 2.3261 acc_train: 0.1394 loss_val: 2.3234 acc_val: 0.1526 time: 0.4063s\n",
      "Epoch: 0248 loss_train: 2.3237 acc_train: 0.1429 loss_val: 2.3235 acc_val: 0.1582 time: 0.3623s\n",
      "Epoch: 0249 loss_train: 2.3308 acc_train: 0.1363 loss_val: 2.3238 acc_val: 0.1582 time: 0.4005s\n",
      "Epoch: 0250 loss_train: 2.3289 acc_train: 0.1393 loss_val: 2.3241 acc_val: 0.1582 time: 0.3974s\n",
      "Epoch: 0251 loss_train: 2.3215 acc_train: 0.1435 loss_val: 2.3239 acc_val: 0.1577 time: 0.3723s\n",
      "Epoch: 0252 loss_train: 2.3291 acc_train: 0.1406 loss_val: 2.3236 acc_val: 0.1516 time: 0.3668s\n",
      "Epoch: 0253 loss_train: 2.3260 acc_train: 0.1517 loss_val: 2.3232 acc_val: 0.1521 time: 0.3578s\n",
      "Epoch: 0254 loss_train: 2.3262 acc_train: 0.1432 loss_val: 2.3227 acc_val: 0.1531 time: 0.3647s\n",
      "Epoch: 0255 loss_train: 2.3215 acc_train: 0.1454 loss_val: 2.3222 acc_val: 0.1531 time: 0.3580s\n",
      "Epoch: 0256 loss_train: 2.3263 acc_train: 0.1391 loss_val: 2.3218 acc_val: 0.1531 time: 0.3676s\n",
      "Epoch: 0257 loss_train: 2.3259 acc_train: 0.1510 loss_val: 2.3216 acc_val: 0.1526 time: 0.3747s\n",
      "Epoch: 0258 loss_train: 2.3268 acc_train: 0.1444 loss_val: 2.3217 acc_val: 0.1526 time: 0.4282s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0259 loss_train: 2.3243 acc_train: 0.1413 loss_val: 2.3217 acc_val: 0.1500 time: 0.3677s\n",
      "Epoch: 0260 loss_train: 2.3272 acc_train: 0.1363 loss_val: 2.3217 acc_val: 0.1536 time: 0.3826s\n",
      "Epoch: 0261 loss_train: 2.3252 acc_train: 0.1532 loss_val: 2.3215 acc_val: 0.1536 time: 0.3592s\n",
      "Epoch: 0262 loss_train: 2.3224 acc_train: 0.1473 loss_val: 2.3214 acc_val: 0.1521 time: 0.3461s\n",
      "Epoch: 0263 loss_train: 2.3259 acc_train: 0.1340 loss_val: 2.3213 acc_val: 0.1526 time: 0.3572s\n",
      "Epoch: 0264 loss_train: 2.3280 acc_train: 0.1387 loss_val: 2.3212 acc_val: 0.1572 time: 0.3883s\n",
      "Epoch: 0265 loss_train: 2.3275 acc_train: 0.1431 loss_val: 2.3211 acc_val: 0.1572 time: 0.3663s\n",
      "Epoch: 0266 loss_train: 2.3253 acc_train: 0.1425 loss_val: 2.3208 acc_val: 0.1567 time: 0.3621s\n",
      "Epoch: 0267 loss_train: 2.3294 acc_train: 0.1410 loss_val: 2.3205 acc_val: 0.1572 time: 0.3649s\n",
      "Epoch: 0268 loss_train: 2.3218 acc_train: 0.1450 loss_val: 2.3204 acc_val: 0.1582 time: 0.3784s\n",
      "Epoch: 0269 loss_train: 2.3223 acc_train: 0.1363 loss_val: 2.3202 acc_val: 0.1536 time: 0.3535s\n",
      "Epoch: 0270 loss_train: 2.3269 acc_train: 0.1491 loss_val: 2.3200 acc_val: 0.1531 time: 0.3387s\n",
      "Epoch: 0271 loss_train: 2.3262 acc_train: 0.1399 loss_val: 2.3198 acc_val: 0.1505 time: 0.3521s\n",
      "Epoch: 0272 loss_train: 2.3235 acc_train: 0.1473 loss_val: 2.3199 acc_val: 0.1536 time: 0.3981s\n",
      "Epoch: 0273 loss_train: 2.3250 acc_train: 0.1412 loss_val: 2.3202 acc_val: 0.1521 time: 0.3405s\n",
      "Epoch: 0274 loss_train: 2.3239 acc_train: 0.1378 loss_val: 2.3206 acc_val: 0.1531 time: 0.3848s\n",
      "Epoch: 0275 loss_train: 2.3241 acc_train: 0.1335 loss_val: 2.3209 acc_val: 0.1510 time: 0.3749s\n",
      "Epoch: 0276 loss_train: 2.3234 acc_train: 0.1488 loss_val: 2.3212 acc_val: 0.1510 time: 0.3461s\n",
      "Epoch: 0277 loss_train: 2.3231 acc_train: 0.1426 loss_val: 2.3213 acc_val: 0.1505 time: 0.3772s\n",
      "Epoch: 0278 loss_train: 2.3267 acc_train: 0.1447 loss_val: 2.3210 acc_val: 0.1510 time: 0.4068s\n",
      "Epoch: 0279 loss_train: 2.3263 acc_train: 0.1507 loss_val: 2.3206 acc_val: 0.1521 time: 0.3921s\n",
      "Epoch: 0280 loss_train: 2.3260 acc_train: 0.1450 loss_val: 2.3202 acc_val: 0.1510 time: 0.3763s\n",
      "Epoch: 0281 loss_train: 2.3262 acc_train: 0.1426 loss_val: 2.3198 acc_val: 0.1526 time: 0.3615s\n",
      "Epoch: 0282 loss_train: 2.3224 acc_train: 0.1404 loss_val: 2.3195 acc_val: 0.1495 time: 0.4254s\n",
      "Epoch: 0283 loss_train: 2.3271 acc_train: 0.1385 loss_val: 2.3194 acc_val: 0.1510 time: 0.3814s\n",
      "Epoch: 0284 loss_train: 2.3256 acc_train: 0.1312 loss_val: 2.3194 acc_val: 0.1521 time: 0.3477s\n",
      "Epoch: 0285 loss_train: 2.3238 acc_train: 0.1531 loss_val: 2.3193 acc_val: 0.1536 time: 0.4021s\n",
      "Epoch: 0286 loss_train: 2.3236 acc_train: 0.1448 loss_val: 2.3193 acc_val: 0.1516 time: 0.3941s\n",
      "Epoch: 0287 loss_train: 2.3277 acc_train: 0.1431 loss_val: 2.3193 acc_val: 0.1505 time: 0.3885s\n",
      "Epoch: 0288 loss_train: 2.3231 acc_train: 0.1610 loss_val: 2.3192 acc_val: 0.1505 time: 0.3883s\n",
      "Epoch: 0289 loss_train: 2.3283 acc_train: 0.1444 loss_val: 2.3192 acc_val: 0.1516 time: 0.3938s\n",
      "Epoch: 0290 loss_train: 2.3242 acc_train: 0.1444 loss_val: 2.3192 acc_val: 0.1521 time: 0.3839s\n",
      "Epoch: 0291 loss_train: 2.3215 acc_train: 0.1394 loss_val: 2.3192 acc_val: 0.1536 time: 0.4044s\n",
      "Epoch: 0292 loss_train: 2.3217 acc_train: 0.1357 loss_val: 2.3192 acc_val: 0.1526 time: 0.3839s\n",
      "Epoch: 0293 loss_train: 2.3231 acc_train: 0.1355 loss_val: 2.3192 acc_val: 0.1526 time: 0.3561s\n",
      "Epoch: 0294 loss_train: 2.3185 acc_train: 0.1365 loss_val: 2.3191 acc_val: 0.1526 time: 0.3850s\n",
      "Epoch: 0295 loss_train: 2.3238 acc_train: 0.1501 loss_val: 2.3190 acc_val: 0.1531 time: 0.3906s\n",
      "Epoch: 0296 loss_train: 2.3248 acc_train: 0.1473 loss_val: 2.3189 acc_val: 0.1521 time: 0.4022s\n",
      "Epoch: 0297 loss_train: 2.3295 acc_train: 0.1396 loss_val: 2.3187 acc_val: 0.1521 time: 0.3600s\n",
      "Epoch: 0298 loss_train: 2.3239 acc_train: 0.1429 loss_val: 2.3185 acc_val: 0.1516 time: 0.3800s\n",
      "Epoch: 0299 loss_train: 2.3236 acc_train: 0.1428 loss_val: 2.3183 acc_val: 0.1521 time: 0.3696s\n",
      "Epoch: 0300 loss_train: 2.3233 acc_train: 0.1349 loss_val: 2.3182 acc_val: 0.1521 time: 0.3781s\n",
      "Epoch: 0301 loss_train: 2.3261 acc_train: 0.1457 loss_val: 2.3181 acc_val: 0.1531 time: 0.3874s\n",
      "Epoch: 0302 loss_train: 2.3207 acc_train: 0.1422 loss_val: 2.3183 acc_val: 0.1495 time: 0.3592s\n",
      "Epoch: 0303 loss_train: 2.3217 acc_train: 0.1419 loss_val: 2.3185 acc_val: 0.1510 time: 0.3885s\n",
      "Epoch: 0304 loss_train: 2.3286 acc_train: 0.1418 loss_val: 2.3186 acc_val: 0.1510 time: 0.3810s\n",
      "Epoch: 0305 loss_train: 2.3220 acc_train: 0.1457 loss_val: 2.3187 acc_val: 0.1526 time: 0.3644s\n",
      "Epoch: 0306 loss_train: 2.3245 acc_train: 0.1378 loss_val: 2.3188 acc_val: 0.1531 time: 0.4109s\n",
      "Epoch: 0307 loss_train: 2.3205 acc_train: 0.1551 loss_val: 2.3188 acc_val: 0.1516 time: 0.3502s\n",
      "Epoch: 0308 loss_train: 2.3268 acc_train: 0.1466 loss_val: 2.3188 acc_val: 0.1516 time: 0.3825s\n",
      "Epoch: 0309 loss_train: 2.3291 acc_train: 0.1388 loss_val: 2.3187 acc_val: 0.1521 time: 0.3935s\n",
      "Epoch: 0310 loss_train: 2.3177 acc_train: 0.1429 loss_val: 2.3185 acc_val: 0.1521 time: 0.3719s\n",
      "Epoch: 0311 loss_train: 2.3258 acc_train: 0.1421 loss_val: 2.3181 acc_val: 0.1531 time: 0.3751s\n",
      "Epoch: 0312 loss_train: 2.3239 acc_train: 0.1451 loss_val: 2.3179 acc_val: 0.1526 time: 0.3484s\n",
      "Epoch: 0313 loss_train: 2.3205 acc_train: 0.1396 loss_val: 2.3177 acc_val: 0.1505 time: 0.3758s\n",
      "Epoch: 0314 loss_train: 2.3211 acc_train: 0.1490 loss_val: 2.3175 acc_val: 0.1516 time: 0.3593s\n",
      "Epoch: 0315 loss_train: 2.3215 acc_train: 0.1485 loss_val: 2.3172 acc_val: 0.1536 time: 0.3998s\n",
      "Epoch: 0316 loss_train: 2.3187 acc_train: 0.1512 loss_val: 2.3170 acc_val: 0.1521 time: 0.4138s\n",
      "Epoch: 0317 loss_train: 2.3214 acc_train: 0.1462 loss_val: 2.3170 acc_val: 0.1531 time: 0.3734s\n",
      "Epoch: 0318 loss_train: 2.3251 acc_train: 0.1493 loss_val: 2.3169 acc_val: 0.1541 time: 0.3805s\n",
      "Epoch: 0319 loss_train: 2.3186 acc_train: 0.1446 loss_val: 2.3168 acc_val: 0.1526 time: 0.3685s\n",
      "Epoch: 0320 loss_train: 2.3213 acc_train: 0.1550 loss_val: 2.3168 acc_val: 0.1505 time: 0.3428s\n",
      "Epoch: 0321 loss_train: 2.3250 acc_train: 0.1407 loss_val: 2.3168 acc_val: 0.1521 time: 0.3514s\n",
      "Epoch: 0322 loss_train: 2.3249 acc_train: 0.1473 loss_val: 2.3167 acc_val: 0.1531 time: 0.3818s\n",
      "Epoch: 0323 loss_train: 2.3199 acc_train: 0.1396 loss_val: 2.3168 acc_val: 0.1526 time: 0.3597s\n",
      "Epoch: 0324 loss_train: 2.3244 acc_train: 0.1280 loss_val: 2.3170 acc_val: 0.1510 time: 0.3822s\n",
      "Epoch: 0325 loss_train: 2.3242 acc_train: 0.1503 loss_val: 2.3171 acc_val: 0.1505 time: 0.3631s\n",
      "Epoch: 0326 loss_train: 2.3235 acc_train: 0.1415 loss_val: 2.3173 acc_val: 0.1541 time: 0.3666s\n",
      "Epoch: 0327 loss_train: 2.3198 acc_train: 0.1473 loss_val: 2.3174 acc_val: 0.1521 time: 0.3907s\n",
      "Epoch: 0328 loss_train: 2.3261 acc_train: 0.1312 loss_val: 2.3174 acc_val: 0.1516 time: 0.3811s\n",
      "Epoch: 0329 loss_train: 2.3215 acc_train: 0.1498 loss_val: 2.3172 acc_val: 0.1505 time: 0.4005s\n",
      "Epoch: 0330 loss_train: 2.3275 acc_train: 0.1419 loss_val: 2.3170 acc_val: 0.1521 time: 0.3944s\n",
      "Epoch: 0331 loss_train: 2.3239 acc_train: 0.1378 loss_val: 2.3167 acc_val: 0.1516 time: 0.3723s\n",
      "Epoch: 0332 loss_train: 2.3264 acc_train: 0.1396 loss_val: 2.3163 acc_val: 0.1510 time: 0.3803s\n",
      "Epoch: 0333 loss_train: 2.3251 acc_train: 0.1469 loss_val: 2.3160 acc_val: 0.1536 time: 0.4189s\n",
      "Epoch: 0334 loss_train: 2.3250 acc_train: 0.1509 loss_val: 2.3155 acc_val: 0.1536 time: 0.3523s\n",
      "Epoch: 0335 loss_train: 2.3236 acc_train: 0.1437 loss_val: 2.3151 acc_val: 0.1536 time: 0.3718s\n",
      "Epoch: 0336 loss_train: 2.3199 acc_train: 0.1450 loss_val: 2.3149 acc_val: 0.1521 time: 0.3782s\n",
      "Epoch: 0337 loss_train: 2.3188 acc_train: 0.1478 loss_val: 2.3149 acc_val: 0.1521 time: 0.3946s\n",
      "Epoch: 0338 loss_train: 2.3182 acc_train: 0.1459 loss_val: 2.3153 acc_val: 0.1531 time: 0.3976s\n",
      "Epoch: 0339 loss_train: 2.3270 acc_train: 0.1403 loss_val: 2.3157 acc_val: 0.1531 time: 0.3964s\n",
      "Epoch: 0340 loss_train: 2.3159 acc_train: 0.1542 loss_val: 2.3162 acc_val: 0.1521 time: 0.3633s\n",
      "Epoch: 0341 loss_train: 2.3186 acc_train: 0.1466 loss_val: 2.3166 acc_val: 0.1516 time: 0.3900s\n",
      "Epoch: 0342 loss_train: 2.3194 acc_train: 0.1462 loss_val: 2.3170 acc_val: 0.1510 time: 0.3498s\n",
      "Epoch: 0343 loss_train: 2.3196 acc_train: 0.1385 loss_val: 2.3173 acc_val: 0.1526 time: 0.3998s\n",
      "Epoch: 0344 loss_train: 2.3207 acc_train: 0.1412 loss_val: 2.3175 acc_val: 0.1541 time: 0.3922s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0345 loss_train: 2.3165 acc_train: 0.1534 loss_val: 2.3175 acc_val: 0.1521 time: 0.3786s\n",
      "Epoch: 0346 loss_train: 2.3241 acc_train: 0.1381 loss_val: 2.3174 acc_val: 0.1531 time: 0.3886s\n",
      "Epoch: 0347 loss_train: 2.3231 acc_train: 0.1451 loss_val: 2.3171 acc_val: 0.1531 time: 0.4262s\n",
      "Epoch: 0348 loss_train: 2.3172 acc_train: 0.1431 loss_val: 2.3167 acc_val: 0.1516 time: 0.4967s\n",
      "Epoch: 0349 loss_train: 2.3200 acc_train: 0.1506 loss_val: 2.3162 acc_val: 0.1516 time: 0.5334s\n",
      "Epoch: 0350 loss_train: 2.3186 acc_train: 0.1421 loss_val: 2.3157 acc_val: 0.1521 time: 0.5080s\n",
      "Epoch: 0351 loss_train: 2.3189 acc_train: 0.1501 loss_val: 2.3152 acc_val: 0.1521 time: 0.5197s\n",
      "Epoch: 0352 loss_train: 2.3246 acc_train: 0.1459 loss_val: 2.3150 acc_val: 0.1531 time: 0.4405s\n",
      "Epoch: 0353 loss_train: 2.3186 acc_train: 0.1494 loss_val: 2.3147 acc_val: 0.1536 time: 0.4109s\n",
      "Epoch: 0354 loss_train: 2.3228 acc_train: 0.1385 loss_val: 2.3146 acc_val: 0.1536 time: 0.3841s\n",
      "Epoch: 0355 loss_train: 2.3212 acc_train: 0.1448 loss_val: 2.3146 acc_val: 0.1541 time: 0.3926s\n",
      "Epoch: 0356 loss_train: 2.3213 acc_train: 0.1419 loss_val: 2.3146 acc_val: 0.1536 time: 0.3723s\n",
      "Epoch: 0357 loss_train: 2.3223 acc_train: 0.1450 loss_val: 2.3146 acc_val: 0.1531 time: 0.3716s\n",
      "Epoch: 0358 loss_train: 2.3264 acc_train: 0.1400 loss_val: 2.3147 acc_val: 0.1531 time: 0.3524s\n",
      "Epoch: 0359 loss_train: 2.3224 acc_train: 0.1443 loss_val: 2.3148 acc_val: 0.1531 time: 0.4097s\n",
      "Epoch: 0360 loss_train: 2.3239 acc_train: 0.1465 loss_val: 2.3149 acc_val: 0.1521 time: 0.3966s\n",
      "Epoch: 0361 loss_train: 2.3230 acc_train: 0.1416 loss_val: 2.3149 acc_val: 0.1521 time: 0.3882s\n",
      "Epoch: 0362 loss_train: 2.3183 acc_train: 0.1394 loss_val: 2.3150 acc_val: 0.1526 time: 0.3717s\n",
      "Epoch: 0363 loss_train: 2.3180 acc_train: 0.1519 loss_val: 2.3148 acc_val: 0.1521 time: 0.3951s\n",
      "Epoch: 0364 loss_train: 2.3209 acc_train: 0.1418 loss_val: 2.3147 acc_val: 0.1510 time: 0.3811s\n",
      "Epoch: 0365 loss_train: 2.3243 acc_train: 0.1426 loss_val: 2.3144 acc_val: 0.1510 time: 0.3700s\n",
      "Epoch: 0366 loss_train: 2.3243 acc_train: 0.1532 loss_val: 2.3142 acc_val: 0.1510 time: 0.3776s\n",
      "Epoch: 0367 loss_train: 2.3228 acc_train: 0.1504 loss_val: 2.3140 acc_val: 0.1510 time: 0.3801s\n",
      "Epoch: 0368 loss_train: 2.3199 acc_train: 0.1495 loss_val: 2.3141 acc_val: 0.1531 time: 0.3679s\n",
      "Epoch: 0369 loss_train: 2.3187 acc_train: 0.1444 loss_val: 2.3141 acc_val: 0.1541 time: 0.4016s\n",
      "Epoch: 0370 loss_train: 2.3172 acc_train: 0.1372 loss_val: 2.3146 acc_val: 0.1505 time: 0.3742s\n",
      "Epoch: 0371 loss_train: 2.3171 acc_train: 0.1434 loss_val: 2.3152 acc_val: 0.1521 time: 0.4019s\n",
      "Epoch: 0372 loss_train: 2.3258 acc_train: 0.1454 loss_val: 2.3158 acc_val: 0.1531 time: 0.3843s\n",
      "Epoch: 0373 loss_train: 2.3277 acc_train: 0.1466 loss_val: 2.3161 acc_val: 0.1526 time: 0.3547s\n",
      "Epoch: 0374 loss_train: 2.3233 acc_train: 0.1457 loss_val: 2.3163 acc_val: 0.1521 time: 0.3616s\n",
      "Epoch: 0375 loss_train: 2.3183 acc_train: 0.1463 loss_val: 2.3162 acc_val: 0.1505 time: 0.3459s\n",
      "Epoch: 0376 loss_train: 2.3215 acc_train: 0.1428 loss_val: 2.3158 acc_val: 0.1505 time: 0.3855s\n",
      "Epoch: 0377 loss_train: 2.3187 acc_train: 0.1391 loss_val: 2.3152 acc_val: 0.1500 time: 0.3717s\n",
      "Epoch: 0378 loss_train: 2.3188 acc_train: 0.1451 loss_val: 2.3146 acc_val: 0.1510 time: 0.3684s\n",
      "Epoch: 0379 loss_train: 2.3221 acc_train: 0.1460 loss_val: 2.3138 acc_val: 0.1536 time: 0.3670s\n",
      "Epoch: 0380 loss_train: 2.3214 acc_train: 0.1460 loss_val: 2.3134 acc_val: 0.1521 time: 0.3679s\n",
      "Epoch: 0381 loss_train: 2.3197 acc_train: 0.1465 loss_val: 2.3131 acc_val: 0.1526 time: 0.3666s\n",
      "Epoch: 0382 loss_train: 2.3273 acc_train: 0.1481 loss_val: 2.3130 acc_val: 0.1516 time: 0.3555s\n",
      "Epoch: 0383 loss_train: 2.3251 acc_train: 0.1338 loss_val: 2.3131 acc_val: 0.1536 time: 0.3549s\n",
      "Epoch: 0384 loss_train: 2.3231 acc_train: 0.1421 loss_val: 2.3134 acc_val: 0.1516 time: 0.3728s\n",
      "Epoch: 0385 loss_train: 2.3233 acc_train: 0.1507 loss_val: 2.3138 acc_val: 0.1516 time: 0.3663s\n",
      "Epoch: 0386 loss_train: 2.3176 acc_train: 0.1441 loss_val: 2.3143 acc_val: 0.1516 time: 0.4117s\n",
      "Epoch: 0387 loss_train: 2.3146 acc_train: 0.1432 loss_val: 2.3146 acc_val: 0.1521 time: 0.3429s\n",
      "Epoch: 0388 loss_train: 2.3235 acc_train: 0.1432 loss_val: 2.3146 acc_val: 0.1521 time: 0.3442s\n",
      "Epoch: 0389 loss_train: 2.3181 acc_train: 0.1390 loss_val: 2.3144 acc_val: 0.1510 time: 0.3810s\n",
      "Epoch: 0390 loss_train: 2.3228 acc_train: 0.1512 loss_val: 2.3140 acc_val: 0.1531 time: 0.3851s\n",
      "Epoch: 0391 loss_train: 2.3218 acc_train: 0.1390 loss_val: 2.3136 acc_val: 0.1526 time: 0.3885s\n",
      "Epoch: 0392 loss_train: 2.3238 acc_train: 0.1421 loss_val: 2.3133 acc_val: 0.1526 time: 0.3837s\n",
      "Epoch: 0393 loss_train: 2.3244 acc_train: 0.1412 loss_val: 2.3133 acc_val: 0.1531 time: 0.3878s\n",
      "Epoch: 0394 loss_train: 2.3213 acc_train: 0.1550 loss_val: 2.3133 acc_val: 0.1521 time: 0.3755s\n",
      "Epoch: 0395 loss_train: 2.3251 acc_train: 0.1432 loss_val: 2.3134 acc_val: 0.1526 time: 0.3582s\n",
      "Epoch: 0396 loss_train: 2.3197 acc_train: 0.1476 loss_val: 2.3135 acc_val: 0.1521 time: 0.3741s\n",
      "Epoch: 0397 loss_train: 2.3255 acc_train: 0.1479 loss_val: 2.3137 acc_val: 0.1521 time: 0.3559s\n",
      "Epoch: 0398 loss_train: 2.3170 acc_train: 0.1462 loss_val: 2.3137 acc_val: 0.1521 time: 0.4048s\n",
      "Epoch: 0399 loss_train: 2.3216 acc_train: 0.1503 loss_val: 2.3136 acc_val: 0.1521 time: 0.3979s\n",
      "Epoch: 0400 loss_train: 2.3215 acc_train: 0.1402 loss_val: 2.3136 acc_val: 0.1521 time: 0.3615s\n",
      "Epoch: 0401 loss_train: 2.3238 acc_train: 0.1478 loss_val: 2.3135 acc_val: 0.1516 time: 0.3751s\n",
      "Epoch: 0402 loss_train: 2.3232 acc_train: 0.1372 loss_val: 2.3133 acc_val: 0.1510 time: 0.3746s\n",
      "Epoch: 0403 loss_train: 2.3236 acc_train: 0.1316 loss_val: 2.3133 acc_val: 0.1516 time: 0.3962s\n",
      "Epoch: 0404 loss_train: 2.3231 acc_train: 0.1415 loss_val: 2.3132 acc_val: 0.1531 time: 0.3725s\n",
      "Epoch: 0405 loss_train: 2.3218 acc_train: 0.1422 loss_val: 2.3131 acc_val: 0.1526 time: 0.4114s\n",
      "Epoch: 0406 loss_train: 2.3192 acc_train: 0.1451 loss_val: 2.3132 acc_val: 0.1521 time: 0.3791s\n",
      "Epoch: 0407 loss_train: 2.3227 acc_train: 0.1415 loss_val: 2.3134 acc_val: 0.1510 time: 0.3663s\n",
      "Epoch: 0408 loss_train: 2.3216 acc_train: 0.1388 loss_val: 2.3138 acc_val: 0.1510 time: 0.3504s\n",
      "Epoch: 0409 loss_train: 2.3219 acc_train: 0.1406 loss_val: 2.3143 acc_val: 0.1516 time: 0.3325s\n",
      "Epoch: 0410 loss_train: 2.3222 acc_train: 0.1494 loss_val: 2.3148 acc_val: 0.1521 time: 0.3917s\n",
      "Epoch: 0411 loss_train: 2.3174 acc_train: 0.1468 loss_val: 2.3151 acc_val: 0.1521 time: 0.3684s\n",
      "Epoch: 0412 loss_train: 2.3223 acc_train: 0.1421 loss_val: 2.3151 acc_val: 0.1521 time: 0.3457s\n",
      "Epoch: 0413 loss_train: 2.3222 acc_train: 0.1424 loss_val: 2.3151 acc_val: 0.1521 time: 0.3762s\n",
      "Epoch: 0414 loss_train: 2.3207 acc_train: 0.1374 loss_val: 2.3150 acc_val: 0.1526 time: 0.3573s\n",
      "Epoch: 0415 loss_train: 2.3217 acc_train: 0.1391 loss_val: 2.3149 acc_val: 0.1526 time: 0.3811s\n",
      "Epoch: 0416 loss_train: 2.3196 acc_train: 0.1432 loss_val: 2.3145 acc_val: 0.1521 time: 0.3714s\n",
      "Epoch: 0417 loss_train: 2.3215 acc_train: 0.1454 loss_val: 2.3140 acc_val: 0.1521 time: 0.3589s\n",
      "Epoch: 0418 loss_train: 2.3217 acc_train: 0.1393 loss_val: 2.3134 acc_val: 0.1521 time: 0.3617s\n",
      "Epoch: 0419 loss_train: 2.3223 acc_train: 0.1484 loss_val: 2.3129 acc_val: 0.1510 time: 0.3689s\n",
      "Epoch: 0420 loss_train: 2.3221 acc_train: 0.1360 loss_val: 2.3125 acc_val: 0.1510 time: 0.3706s\n",
      "Epoch: 0421 loss_train: 2.3203 acc_train: 0.1438 loss_val: 2.3122 acc_val: 0.1510 time: 0.3778s\n",
      "Epoch: 0422 loss_train: 2.3221 acc_train: 0.1338 loss_val: 2.3121 acc_val: 0.1516 time: 0.3898s\n",
      "Epoch: 0423 loss_train: 2.3185 acc_train: 0.1468 loss_val: 2.3124 acc_val: 0.1521 time: 0.3809s\n",
      "Epoch: 0424 loss_train: 2.3196 acc_train: 0.1425 loss_val: 2.3129 acc_val: 0.1521 time: 0.3629s\n",
      "Epoch: 0425 loss_train: 2.3183 acc_train: 0.1493 loss_val: 2.3134 acc_val: 0.1516 time: 0.3546s\n",
      "Epoch: 0426 loss_train: 2.3264 acc_train: 0.1382 loss_val: 2.3138 acc_val: 0.1521 time: 0.3483s\n",
      "Epoch: 0427 loss_train: 2.3234 acc_train: 0.1362 loss_val: 2.3142 acc_val: 0.1521 time: 0.3872s\n",
      "Epoch: 0428 loss_train: 2.3214 acc_train: 0.1471 loss_val: 2.3145 acc_val: 0.1521 time: 0.3795s\n",
      "Epoch: 0429 loss_train: 2.3193 acc_train: 0.1482 loss_val: 2.3147 acc_val: 0.1510 time: 0.3944s\n",
      "Epoch: 0430 loss_train: 2.3226 acc_train: 0.1412 loss_val: 2.3146 acc_val: 0.1510 time: 0.4126s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0431 loss_train: 2.3222 acc_train: 0.1451 loss_val: 2.3141 acc_val: 0.1521 time: 0.3651s\n",
      "Epoch: 0432 loss_train: 2.3205 acc_train: 0.1403 loss_val: 2.3136 acc_val: 0.1521 time: 0.4090s\n",
      "Epoch: 0433 loss_train: 2.3224 acc_train: 0.1507 loss_val: 2.3131 acc_val: 0.1510 time: 0.3750s\n",
      "Epoch: 0434 loss_train: 2.3254 acc_train: 0.1500 loss_val: 2.3125 acc_val: 0.1510 time: 0.3375s\n",
      "Epoch: 0435 loss_train: 2.3183 acc_train: 0.1441 loss_val: 2.3120 acc_val: 0.1516 time: 0.4221s\n",
      "Epoch: 0436 loss_train: 2.3221 acc_train: 0.1416 loss_val: 2.3116 acc_val: 0.1521 time: 0.3860s\n",
      "Epoch: 0437 loss_train: 2.3172 acc_train: 0.1443 loss_val: 2.3114 acc_val: 0.1521 time: 0.3553s\n",
      "Epoch: 0438 loss_train: 2.3259 acc_train: 0.1396 loss_val: 2.3117 acc_val: 0.1521 time: 0.3570s\n",
      "Epoch: 0439 loss_train: 2.3215 acc_train: 0.1490 loss_val: 2.3120 acc_val: 0.1521 time: 0.3737s\n",
      "Epoch: 0440 loss_train: 2.3255 acc_train: 0.1353 loss_val: 2.3126 acc_val: 0.1521 time: 0.3798s\n",
      "Epoch: 0441 loss_train: 2.3188 acc_train: 0.1479 loss_val: 2.3134 acc_val: 0.1531 time: 0.3531s\n",
      "Epoch: 0442 loss_train: 2.3161 acc_train: 0.1488 loss_val: 2.3143 acc_val: 0.1541 time: 0.3971s\n",
      "Epoch: 0443 loss_train: 2.3207 acc_train: 0.1372 loss_val: 2.3151 acc_val: 0.1536 time: 0.3583s\n",
      "Epoch: 0444 loss_train: 2.3173 acc_train: 0.1422 loss_val: 2.3155 acc_val: 0.1546 time: 0.3932s\n",
      "Epoch: 0445 loss_train: 2.3164 acc_train: 0.1448 loss_val: 2.3156 acc_val: 0.1531 time: 0.3661s\n",
      "Epoch: 0446 loss_train: 2.3191 acc_train: 0.1501 loss_val: 2.3155 acc_val: 0.1521 time: 0.3861s\n",
      "Epoch: 0447 loss_train: 2.3210 acc_train: 0.1450 loss_val: 2.3149 acc_val: 0.1505 time: 0.3880s\n",
      "Epoch: 0448 loss_train: 2.3228 acc_train: 0.1394 loss_val: 2.3142 acc_val: 0.1521 time: 0.3654s\n",
      "Epoch: 0449 loss_train: 2.3216 acc_train: 0.1456 loss_val: 2.3133 acc_val: 0.1541 time: 0.3666s\n",
      "Epoch: 0450 loss_train: 2.3238 acc_train: 0.1448 loss_val: 2.3123 acc_val: 0.1521 time: 0.3836s\n",
      "Epoch: 0451 loss_train: 2.3189 acc_train: 0.1413 loss_val: 2.3116 acc_val: 0.1526 time: 0.3928s\n",
      "Epoch: 0452 loss_train: 2.3188 acc_train: 0.1400 loss_val: 2.3111 acc_val: 0.1536 time: 0.3916s\n",
      "Epoch: 0453 loss_train: 2.3242 acc_train: 0.1454 loss_val: 2.3109 acc_val: 0.1510 time: 0.3911s\n",
      "Epoch: 0454 loss_train: 2.3191 acc_train: 0.1451 loss_val: 2.3111 acc_val: 0.1526 time: 0.3823s\n",
      "Epoch: 0455 loss_train: 2.3261 acc_train: 0.1444 loss_val: 2.3116 acc_val: 0.1521 time: 0.3896s\n",
      "Epoch: 0456 loss_train: 2.3259 acc_train: 0.1335 loss_val: 2.3125 acc_val: 0.1546 time: 0.3519s\n",
      "Epoch: 0457 loss_train: 2.3149 acc_train: 0.1507 loss_val: 2.3134 acc_val: 0.1505 time: 0.3920s\n",
      "Epoch: 0458 loss_train: 2.3195 acc_train: 0.1487 loss_val: 2.3139 acc_val: 0.1485 time: 0.3607s\n",
      "Epoch: 0459 loss_train: 2.3219 acc_train: 0.1437 loss_val: 2.3144 acc_val: 0.1459 time: 0.3762s\n",
      "Epoch: 0460 loss_train: 2.3195 acc_train: 0.1473 loss_val: 2.3145 acc_val: 0.1485 time: 0.3400s\n",
      "Epoch: 0461 loss_train: 2.3187 acc_train: 0.1409 loss_val: 2.3145 acc_val: 0.1557 time: 0.3925s\n",
      "Epoch: 0462 loss_train: 2.3189 acc_train: 0.1381 loss_val: 2.3144 acc_val: 0.1557 time: 0.3850s\n",
      "Epoch: 0463 loss_train: 2.3247 acc_train: 0.1424 loss_val: 2.3141 acc_val: 0.1541 time: 0.3811s\n",
      "Epoch: 0464 loss_train: 2.3210 acc_train: 0.1478 loss_val: 2.3137 acc_val: 0.1510 time: 0.3650s\n",
      "Epoch: 0465 loss_train: 2.3171 acc_train: 0.1466 loss_val: 2.3134 acc_val: 0.1510 time: 0.3825s\n",
      "Epoch: 0466 loss_train: 2.3157 acc_train: 0.1482 loss_val: 2.3131 acc_val: 0.1510 time: 0.3852s\n",
      "Epoch: 0467 loss_train: 2.3170 acc_train: 0.1500 loss_val: 2.3129 acc_val: 0.1526 time: 0.4004s\n",
      "Epoch: 0468 loss_train: 2.3192 acc_train: 0.1422 loss_val: 2.3129 acc_val: 0.1521 time: 0.3942s\n",
      "Epoch: 0469 loss_train: 2.3205 acc_train: 0.1501 loss_val: 2.3128 acc_val: 0.1505 time: 0.3992s\n",
      "Epoch: 0470 loss_train: 2.3225 acc_train: 0.1459 loss_val: 2.3128 acc_val: 0.1505 time: 0.4002s\n",
      "Epoch: 0471 loss_train: 2.3202 acc_train: 0.1447 loss_val: 2.3130 acc_val: 0.1500 time: 0.3990s\n",
      "Epoch: 0472 loss_train: 2.3179 acc_train: 0.1498 loss_val: 2.3131 acc_val: 0.1521 time: 0.4154s\n",
      "Epoch: 0473 loss_train: 2.3168 acc_train: 0.1494 loss_val: 2.3131 acc_val: 0.1526 time: 0.4100s\n",
      "Epoch: 0474 loss_train: 2.3178 acc_train: 0.1440 loss_val: 2.3131 acc_val: 0.1536 time: 0.3794s\n",
      "Epoch: 0475 loss_train: 2.3212 acc_train: 0.1416 loss_val: 2.3129 acc_val: 0.1500 time: 0.3920s\n",
      "Epoch: 0476 loss_train: 2.3166 acc_train: 0.1509 loss_val: 2.3126 acc_val: 0.1531 time: 0.3584s\n",
      "Epoch: 0477 loss_train: 2.3221 acc_train: 0.1479 loss_val: 2.3124 acc_val: 0.1531 time: 0.3801s\n",
      "Epoch: 0478 loss_train: 2.3155 acc_train: 0.1460 loss_val: 2.3122 acc_val: 0.1526 time: 0.3766s\n",
      "Epoch: 0479 loss_train: 2.3204 acc_train: 0.1460 loss_val: 2.3121 acc_val: 0.1516 time: 0.3920s\n",
      "Epoch: 0480 loss_train: 2.3192 acc_train: 0.1404 loss_val: 2.3122 acc_val: 0.1510 time: 0.3826s\n",
      "Epoch: 0481 loss_train: 2.3232 acc_train: 0.1443 loss_val: 2.3125 acc_val: 0.1510 time: 0.3881s\n",
      "Epoch: 0482 loss_train: 2.3192 acc_train: 0.1399 loss_val: 2.3130 acc_val: 0.1510 time: 0.3769s\n",
      "Epoch: 0483 loss_train: 2.3228 acc_train: 0.1388 loss_val: 2.3134 acc_val: 0.1510 time: 0.3791s\n",
      "Epoch: 0484 loss_train: 2.3151 acc_train: 0.1469 loss_val: 2.3137 acc_val: 0.1521 time: 0.3827s\n",
      "Epoch: 0485 loss_train: 2.3226 acc_train: 0.1397 loss_val: 2.3140 acc_val: 0.1521 time: 0.4090s\n",
      "Epoch: 0486 loss_train: 2.3217 acc_train: 0.1525 loss_val: 2.3139 acc_val: 0.1536 time: 0.3604s\n",
      "Epoch: 0487 loss_train: 2.3187 acc_train: 0.1475 loss_val: 2.3138 acc_val: 0.1541 time: 0.3785s\n",
      "Epoch: 0488 loss_train: 2.3220 acc_train: 0.1382 loss_val: 2.3136 acc_val: 0.1536 time: 0.3760s\n",
      "Epoch: 0489 loss_train: 2.3217 acc_train: 0.1526 loss_val: 2.3133 acc_val: 0.1536 time: 0.3579s\n",
      "Epoch: 0490 loss_train: 2.3204 acc_train: 0.1396 loss_val: 2.3129 acc_val: 0.1531 time: 0.3797s\n",
      "Epoch: 0491 loss_train: 2.3182 acc_train: 0.1454 loss_val: 2.3125 acc_val: 0.1526 time: 0.3639s\n",
      "Epoch: 0492 loss_train: 2.3173 acc_train: 0.1487 loss_val: 2.3121 acc_val: 0.1516 time: 0.3898s\n",
      "Epoch: 0493 loss_train: 2.3173 acc_train: 0.1485 loss_val: 2.3116 acc_val: 0.1521 time: 0.3436s\n",
      "Epoch: 0494 loss_train: 2.3223 acc_train: 0.1347 loss_val: 2.3113 acc_val: 0.1516 time: 0.3716s\n",
      "Epoch: 0495 loss_train: 2.3184 acc_train: 0.1434 loss_val: 2.3112 acc_val: 0.1510 time: 0.3805s\n",
      "Epoch: 0496 loss_train: 2.3208 acc_train: 0.1451 loss_val: 2.3112 acc_val: 0.1510 time: 0.3431s\n",
      "Epoch: 0497 loss_train: 2.3245 acc_train: 0.1484 loss_val: 2.3114 acc_val: 0.1521 time: 0.3963s\n",
      "Epoch: 0498 loss_train: 2.3140 acc_train: 0.1487 loss_val: 2.3116 acc_val: 0.1531 time: 0.3623s\n",
      "Epoch: 0499 loss_train: 2.3230 acc_train: 0.1447 loss_val: 2.3119 acc_val: 0.1536 time: 0.3715s\n",
      "Epoch: 0500 loss_train: 2.3242 acc_train: 0.1399 loss_val: 2.3121 acc_val: 0.1526 time: 0.3695s\n",
      "Epoch: 0501 loss_train: 2.3183 acc_train: 0.1435 loss_val: 2.3124 acc_val: 0.1521 time: 0.3663s\n",
      "Epoch: 0502 loss_train: 2.3219 acc_train: 0.1432 loss_val: 2.3127 acc_val: 0.1521 time: 0.3823s\n",
      "Epoch: 0503 loss_train: 2.3252 acc_train: 0.1462 loss_val: 2.3130 acc_val: 0.1521 time: 0.3915s\n",
      "Epoch: 0504 loss_train: 2.3218 acc_train: 0.1465 loss_val: 2.3134 acc_val: 0.1521 time: 0.3971s\n",
      "Epoch: 0505 loss_train: 2.3233 acc_train: 0.1472 loss_val: 2.3137 acc_val: 0.1521 time: 0.3942s\n",
      "Epoch: 0506 loss_train: 2.3190 acc_train: 0.1402 loss_val: 2.3137 acc_val: 0.1531 time: 0.4032s\n",
      "Epoch: 0507 loss_train: 2.3228 acc_train: 0.1471 loss_val: 2.3136 acc_val: 0.1531 time: 0.4292s\n",
      "Epoch: 0508 loss_train: 2.3232 acc_train: 0.1359 loss_val: 2.3133 acc_val: 0.1510 time: 0.3359s\n",
      "Epoch: 0509 loss_train: 2.3238 acc_train: 0.1369 loss_val: 2.3132 acc_val: 0.1510 time: 0.3488s\n",
      "Epoch: 0510 loss_train: 2.3174 acc_train: 0.1447 loss_val: 2.3129 acc_val: 0.1510 time: 0.3756s\n",
      "Epoch: 0511 loss_train: 2.3146 acc_train: 0.1475 loss_val: 2.3126 acc_val: 0.1510 time: 0.3783s\n",
      "Epoch: 0512 loss_train: 2.3182 acc_train: 0.1476 loss_val: 2.3123 acc_val: 0.1510 time: 0.3757s\n",
      "Epoch: 0513 loss_train: 2.3188 acc_train: 0.1493 loss_val: 2.3123 acc_val: 0.1510 time: 0.3995s\n",
      "Epoch: 0514 loss_train: 2.3209 acc_train: 0.1431 loss_val: 2.3123 acc_val: 0.1521 time: 0.4050s\n",
      "Epoch: 0515 loss_train: 2.3144 acc_train: 0.1589 loss_val: 2.3124 acc_val: 0.1521 time: 0.3810s\n",
      "Epoch: 0516 loss_train: 2.3223 acc_train: 0.1441 loss_val: 2.3126 acc_val: 0.1526 time: 0.3986s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0517 loss_train: 2.3160 acc_train: 0.1434 loss_val: 2.3128 acc_val: 0.1531 time: 0.3759s\n",
      "Epoch: 0518 loss_train: 2.3225 acc_train: 0.1377 loss_val: 2.3129 acc_val: 0.1521 time: 0.3828s\n",
      "Epoch: 0519 loss_train: 2.3244 acc_train: 0.1472 loss_val: 2.3129 acc_val: 0.1521 time: 0.3480s\n",
      "Epoch: 0520 loss_train: 2.3237 acc_train: 0.1382 loss_val: 2.3129 acc_val: 0.1521 time: 0.3773s\n",
      "Epoch: 0521 loss_train: 2.3221 acc_train: 0.1429 loss_val: 2.3128 acc_val: 0.1521 time: 0.3763s\n",
      "Epoch: 0522 loss_train: 2.3214 acc_train: 0.1462 loss_val: 2.3127 acc_val: 0.1510 time: 0.3962s\n",
      "Epoch: 0523 loss_train: 2.3226 acc_train: 0.1421 loss_val: 2.3126 acc_val: 0.1510 time: 0.3710s\n",
      "Epoch: 0524 loss_train: 2.3217 acc_train: 0.1424 loss_val: 2.3126 acc_val: 0.1510 time: 0.3941s\n",
      "Epoch: 0525 loss_train: 2.3198 acc_train: 0.1425 loss_val: 2.3128 acc_val: 0.1521 time: 0.3752s\n",
      "Epoch: 0526 loss_train: 2.3197 acc_train: 0.1466 loss_val: 2.3128 acc_val: 0.1521 time: 0.4000s\n",
      "Epoch: 0527 loss_train: 2.3190 acc_train: 0.1484 loss_val: 2.3127 acc_val: 0.1526 time: 0.4148s\n",
      "Epoch: 0528 loss_train: 2.3167 acc_train: 0.1421 loss_val: 2.3126 acc_val: 0.1516 time: 0.3794s\n",
      "Epoch: 0529 loss_train: 2.3205 acc_train: 0.1372 loss_val: 2.3126 acc_val: 0.1516 time: 0.3887s\n",
      "Epoch: 0530 loss_train: 2.3189 acc_train: 0.1422 loss_val: 2.3127 acc_val: 0.1516 time: 0.3917s\n",
      "Epoch: 0531 loss_train: 2.3226 acc_train: 0.1388 loss_val: 2.3127 acc_val: 0.1516 time: 0.3880s\n",
      "Epoch: 0532 loss_train: 2.3169 acc_train: 0.1487 loss_val: 2.3127 acc_val: 0.1521 time: 0.3750s\n",
      "Epoch: 0533 loss_train: 2.3204 acc_train: 0.1391 loss_val: 2.3127 acc_val: 0.1521 time: 0.4063s\n",
      "Epoch: 0534 loss_train: 2.3206 acc_train: 0.1424 loss_val: 2.3127 acc_val: 0.1516 time: 0.3696s\n",
      "Epoch: 0535 loss_train: 2.3146 acc_train: 0.1413 loss_val: 2.3126 acc_val: 0.1516 time: 0.4073s\n",
      "Epoch: 0536 loss_train: 2.3235 acc_train: 0.1460 loss_val: 2.3126 acc_val: 0.1516 time: 0.3684s\n",
      "Epoch: 0537 loss_train: 2.3224 acc_train: 0.1365 loss_val: 2.3126 acc_val: 0.1521 time: 0.3499s\n",
      "Epoch: 0538 loss_train: 2.3185 acc_train: 0.1454 loss_val: 2.3127 acc_val: 0.1526 time: 0.3881s\n",
      "Epoch: 0539 loss_train: 2.3182 acc_train: 0.1476 loss_val: 2.3126 acc_val: 0.1546 time: 0.3497s\n",
      "Epoch: 0540 loss_train: 2.3203 acc_train: 0.1481 loss_val: 2.3124 acc_val: 0.1531 time: 0.3859s\n",
      "Epoch: 0541 loss_train: 2.3189 acc_train: 0.1459 loss_val: 2.3126 acc_val: 0.1536 time: 0.4004s\n",
      "Epoch: 0542 loss_train: 2.3192 acc_train: 0.1416 loss_val: 2.3126 acc_val: 0.1536 time: 0.3976s\n",
      "Epoch: 0543 loss_train: 2.3204 acc_train: 0.1393 loss_val: 2.3126 acc_val: 0.1526 time: 0.3923s\n",
      "Epoch: 0544 loss_train: 2.3154 acc_train: 0.1372 loss_val: 2.3128 acc_val: 0.1510 time: 0.4207s\n",
      "Epoch: 0545 loss_train: 2.3180 acc_train: 0.1410 loss_val: 2.3130 acc_val: 0.1510 time: 0.3492s\n",
      "Epoch: 0546 loss_train: 2.3182 acc_train: 0.1462 loss_val: 2.3131 acc_val: 0.1505 time: 0.3547s\n",
      "Epoch: 0547 loss_train: 2.3190 acc_train: 0.1428 loss_val: 2.3130 acc_val: 0.1521 time: 0.3898s\n",
      "Epoch: 0548 loss_train: 2.3211 acc_train: 0.1443 loss_val: 2.3126 acc_val: 0.1521 time: 0.3913s\n",
      "Epoch: 0549 loss_train: 2.3198 acc_train: 0.1453 loss_val: 2.3123 acc_val: 0.1510 time: 0.3855s\n",
      "Epoch: 0550 loss_train: 2.3236 acc_train: 0.1381 loss_val: 2.3123 acc_val: 0.1521 time: 0.4162s\n",
      "Epoch: 0551 loss_train: 2.3222 acc_train: 0.1509 loss_val: 2.3121 acc_val: 0.1531 time: 0.3931s\n",
      "Epoch: 0552 loss_train: 2.3172 acc_train: 0.1500 loss_val: 2.3119 acc_val: 0.1531 time: 0.3795s\n",
      "Epoch: 0553 loss_train: 2.3179 acc_train: 0.1469 loss_val: 2.3117 acc_val: 0.1521 time: 0.3756s\n",
      "Epoch: 0554 loss_train: 2.3215 acc_train: 0.1428 loss_val: 2.3114 acc_val: 0.1521 time: 0.3819s\n",
      "Epoch: 0555 loss_train: 2.3172 acc_train: 0.1512 loss_val: 2.3112 acc_val: 0.1521 time: 0.3789s\n",
      "Epoch: 0556 loss_train: 2.3193 acc_train: 0.1476 loss_val: 2.3113 acc_val: 0.1521 time: 0.3947s\n",
      "Epoch: 0557 loss_train: 2.3191 acc_train: 0.1484 loss_val: 2.3116 acc_val: 0.1521 time: 0.3888s\n",
      "Epoch: 0558 loss_train: 2.3153 acc_train: 0.1378 loss_val: 2.3122 acc_val: 0.1516 time: 0.3962s\n",
      "Epoch: 0559 loss_train: 2.3198 acc_train: 0.1476 loss_val: 2.3127 acc_val: 0.1505 time: 0.3958s\n",
      "Epoch: 0560 loss_train: 2.3185 acc_train: 0.1517 loss_val: 2.3134 acc_val: 0.1526 time: 0.4238s\n",
      "Epoch: 0561 loss_train: 2.3226 acc_train: 0.1349 loss_val: 2.3139 acc_val: 0.1516 time: 0.3971s\n",
      "Epoch: 0562 loss_train: 2.3163 acc_train: 0.1412 loss_val: 2.3142 acc_val: 0.1521 time: 0.3653s\n",
      "Epoch: 0563 loss_train: 2.3139 acc_train: 0.1447 loss_val: 2.3142 acc_val: 0.1521 time: 0.3809s\n",
      "Epoch: 0564 loss_train: 2.3224 acc_train: 0.1400 loss_val: 2.3138 acc_val: 0.1516 time: 0.3582s\n",
      "Epoch: 0565 loss_train: 2.3235 acc_train: 0.1446 loss_val: 2.3133 acc_val: 0.1562 time: 0.3828s\n",
      "Epoch: 0566 loss_train: 2.3189 acc_train: 0.1454 loss_val: 2.3128 acc_val: 0.1551 time: 0.3893s\n",
      "Epoch: 0567 loss_train: 2.3198 acc_train: 0.1515 loss_val: 2.3122 acc_val: 0.1521 time: 0.3480s\n",
      "Epoch: 0568 loss_train: 2.3199 acc_train: 0.1365 loss_val: 2.3118 acc_val: 0.1521 time: 0.3759s\n",
      "Epoch: 0569 loss_train: 2.3190 acc_train: 0.1463 loss_val: 2.3113 acc_val: 0.1521 time: 0.3894s\n",
      "Epoch: 0570 loss_train: 2.3212 acc_train: 0.1485 loss_val: 2.3111 acc_val: 0.1510 time: 0.3548s\n",
      "Epoch: 0571 loss_train: 2.3185 acc_train: 0.1409 loss_val: 2.3110 acc_val: 0.1510 time: 0.3858s\n",
      "Epoch: 0572 loss_train: 2.3214 acc_train: 0.1495 loss_val: 2.3111 acc_val: 0.1500 time: 0.3425s\n",
      "Epoch: 0573 loss_train: 2.3224 acc_train: 0.1466 loss_val: 2.3111 acc_val: 0.1510 time: 0.3809s\n",
      "Epoch: 0574 loss_train: 2.3235 acc_train: 0.1385 loss_val: 2.3113 acc_val: 0.1500 time: 0.3807s\n",
      "Epoch: 0575 loss_train: 2.3222 acc_train: 0.1441 loss_val: 2.3113 acc_val: 0.1521 time: 0.3778s\n",
      "Epoch: 0576 loss_train: 2.3190 acc_train: 0.1424 loss_val: 2.3114 acc_val: 0.1516 time: 0.4076s\n",
      "Epoch: 0577 loss_train: 2.3200 acc_train: 0.1462 loss_val: 2.3116 acc_val: 0.1521 time: 0.3745s\n",
      "Epoch: 0578 loss_train: 2.3203 acc_train: 0.1498 loss_val: 2.3119 acc_val: 0.1526 time: 0.3947s\n",
      "Epoch: 0579 loss_train: 2.3215 acc_train: 0.1416 loss_val: 2.3121 acc_val: 0.1505 time: 0.3783s\n",
      "Epoch: 0580 loss_train: 2.3187 acc_train: 0.1493 loss_val: 2.3123 acc_val: 0.1495 time: 0.3731s\n",
      "Epoch: 0581 loss_train: 2.3181 acc_train: 0.1412 loss_val: 2.3125 acc_val: 0.1526 time: 0.3725s\n",
      "Epoch: 0582 loss_train: 2.3224 acc_train: 0.1493 loss_val: 2.3125 acc_val: 0.1531 time: 0.4012s\n",
      "Epoch: 0583 loss_train: 2.3208 acc_train: 0.1482 loss_val: 2.3126 acc_val: 0.1516 time: 0.3498s\n",
      "Epoch: 0584 loss_train: 2.3195 acc_train: 0.1478 loss_val: 2.3126 acc_val: 0.1480 time: 0.3989s\n",
      "Epoch: 0585 loss_train: 2.3211 acc_train: 0.1453 loss_val: 2.3127 acc_val: 0.1505 time: 0.3999s\n",
      "Epoch: 0586 loss_train: 2.3167 acc_train: 0.1404 loss_val: 2.3127 acc_val: 0.1526 time: 0.4039s\n",
      "Epoch: 0587 loss_train: 2.3207 acc_train: 0.1363 loss_val: 2.3127 acc_val: 0.1505 time: 0.3425s\n",
      "Epoch: 0588 loss_train: 2.3158 acc_train: 0.1459 loss_val: 2.3127 acc_val: 0.1521 time: 0.3756s\n",
      "Epoch: 0589 loss_train: 2.3189 acc_train: 0.1334 loss_val: 2.3126 acc_val: 0.1510 time: 0.3884s\n",
      "Epoch: 0590 loss_train: 2.3197 acc_train: 0.1382 loss_val: 2.3125 acc_val: 0.1531 time: 0.3693s\n",
      "Epoch: 0591 loss_train: 2.3187 acc_train: 0.1403 loss_val: 2.3124 acc_val: 0.1531 time: 0.3886s\n",
      "Epoch: 0592 loss_train: 2.3234 acc_train: 0.1438 loss_val: 2.3124 acc_val: 0.1531 time: 0.3700s\n",
      "Epoch: 0593 loss_train: 2.3208 acc_train: 0.1469 loss_val: 2.3123 acc_val: 0.1531 time: 0.3819s\n",
      "Epoch: 0594 loss_train: 2.3195 acc_train: 0.1403 loss_val: 2.3123 acc_val: 0.1510 time: 0.3609s\n",
      "Epoch: 0595 loss_train: 2.3243 acc_train: 0.1434 loss_val: 2.3122 acc_val: 0.1510 time: 0.3502s\n",
      "Epoch: 0596 loss_train: 2.3205 acc_train: 0.1426 loss_val: 2.3123 acc_val: 0.1516 time: 0.3592s\n",
      "Epoch: 0597 loss_train: 2.3226 acc_train: 0.1438 loss_val: 2.3123 acc_val: 0.1521 time: 0.3413s\n",
      "Epoch: 0598 loss_train: 2.3173 acc_train: 0.1443 loss_val: 2.3123 acc_val: 0.1505 time: 0.3732s\n",
      "Epoch: 0599 loss_train: 2.3166 acc_train: 0.1409 loss_val: 2.3125 acc_val: 0.1505 time: 0.3800s\n",
      "Epoch: 0600 loss_train: 2.3218 acc_train: 0.1494 loss_val: 2.3128 acc_val: 0.1516 time: 0.3418s\n",
      "Epoch: 0601 loss_train: 2.3220 acc_train: 0.1473 loss_val: 2.3129 acc_val: 0.1531 time: 0.3729s\n",
      "Epoch: 0602 loss_train: 2.3195 acc_train: 0.1451 loss_val: 2.3132 acc_val: 0.1521 time: 0.3801s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0603 loss_train: 2.3213 acc_train: 0.1519 loss_val: 2.3133 acc_val: 0.1526 time: 0.3987s\n",
      "Epoch: 0604 loss_train: 2.3264 acc_train: 0.1397 loss_val: 2.3132 acc_val: 0.1536 time: 0.4022s\n",
      "Epoch: 0605 loss_train: 2.3233 acc_train: 0.1454 loss_val: 2.3129 acc_val: 0.1531 time: 0.3490s\n",
      "Epoch: 0606 loss_train: 2.3175 acc_train: 0.1441 loss_val: 2.3126 acc_val: 0.1510 time: 0.3591s\n",
      "Epoch: 0607 loss_train: 2.3224 acc_train: 0.1431 loss_val: 2.3122 acc_val: 0.1510 time: 0.3835s\n",
      "Epoch: 0608 loss_train: 2.3252 acc_train: 0.1448 loss_val: 2.3118 acc_val: 0.1516 time: 0.3898s\n",
      "Epoch: 0609 loss_train: 2.3153 acc_train: 0.1416 loss_val: 2.3115 acc_val: 0.1526 time: 0.3760s\n",
      "Epoch: 0610 loss_train: 2.3192 acc_train: 0.1488 loss_val: 2.3112 acc_val: 0.1521 time: 0.3500s\n",
      "Epoch: 0611 loss_train: 2.3208 acc_train: 0.1465 loss_val: 2.3112 acc_val: 0.1531 time: 0.3565s\n",
      "Epoch: 0612 loss_train: 2.3231 acc_train: 0.1468 loss_val: 2.3114 acc_val: 0.1526 time: 0.3560s\n",
      "Epoch: 0613 loss_train: 2.3163 acc_train: 0.1534 loss_val: 2.3116 acc_val: 0.1526 time: 0.3686s\n",
      "Epoch: 0614 loss_train: 2.3148 acc_train: 0.1554 loss_val: 2.3119 acc_val: 0.1526 time: 0.3588s\n",
      "Epoch: 0615 loss_train: 2.3203 acc_train: 0.1435 loss_val: 2.3123 acc_val: 0.1536 time: 0.3529s\n",
      "Epoch: 0616 loss_train: 2.3195 acc_train: 0.1459 loss_val: 2.3125 acc_val: 0.1531 time: 0.3990s\n",
      "Epoch: 0617 loss_train: 2.3203 acc_train: 0.1404 loss_val: 2.3126 acc_val: 0.1521 time: 0.3844s\n",
      "Epoch: 0618 loss_train: 2.3210 acc_train: 0.1446 loss_val: 2.3127 acc_val: 0.1521 time: 0.3667s\n",
      "Epoch: 0619 loss_train: 2.3217 acc_train: 0.1453 loss_val: 2.3128 acc_val: 0.1510 time: 0.3731s\n",
      "Epoch: 0620 loss_train: 2.3157 acc_train: 0.1444 loss_val: 2.3129 acc_val: 0.1521 time: 0.3988s\n",
      "Epoch: 0621 loss_train: 2.3176 acc_train: 0.1510 loss_val: 2.3127 acc_val: 0.1521 time: 0.3936s\n",
      "Epoch: 0622 loss_train: 2.3166 acc_train: 0.1644 loss_val: 2.3123 acc_val: 0.1521 time: 0.3863s\n",
      "Epoch: 0623 loss_train: 2.3270 acc_train: 0.1396 loss_val: 2.3122 acc_val: 0.1531 time: 0.3826s\n",
      "Epoch: 0624 loss_train: 2.3210 acc_train: 0.1284 loss_val: 2.3122 acc_val: 0.1531 time: 0.3827s\n",
      "Epoch: 0625 loss_train: 2.3190 acc_train: 0.1385 loss_val: 2.3122 acc_val: 0.1531 time: 0.3565s\n",
      "Epoch: 0626 loss_train: 2.3210 acc_train: 0.1500 loss_val: 2.3122 acc_val: 0.1531 time: 0.3441s\n",
      "Epoch: 0627 loss_train: 2.3176 acc_train: 0.1526 loss_val: 2.3121 acc_val: 0.1516 time: 0.3978s\n",
      "Epoch: 0628 loss_train: 2.3192 acc_train: 0.1482 loss_val: 2.3121 acc_val: 0.1516 time: 0.3832s\n",
      "Epoch: 0629 loss_train: 2.3222 acc_train: 0.1378 loss_val: 2.3121 acc_val: 0.1510 time: 0.3714s\n",
      "Epoch: 0630 loss_train: 2.3227 acc_train: 0.1359 loss_val: 2.3122 acc_val: 0.1510 time: 0.3557s\n",
      "Epoch: 0631 loss_train: 2.3188 acc_train: 0.1494 loss_val: 2.3122 acc_val: 0.1521 time: 0.3951s\n",
      "Epoch: 0632 loss_train: 2.3215 acc_train: 0.1434 loss_val: 2.3120 acc_val: 0.1521 time: 0.3934s\n",
      "Epoch: 0633 loss_train: 2.3220 acc_train: 0.1418 loss_val: 2.3120 acc_val: 0.1521 time: 0.3680s\n",
      "Epoch: 0634 loss_train: 2.3238 acc_train: 0.1371 loss_val: 2.3118 acc_val: 0.1521 time: 0.3760s\n",
      "Epoch: 0635 loss_train: 2.3223 acc_train: 0.1380 loss_val: 2.3116 acc_val: 0.1521 time: 0.3668s\n",
      "Epoch: 0636 loss_train: 2.3204 acc_train: 0.1465 loss_val: 2.3115 acc_val: 0.1510 time: 0.3684s\n",
      "Epoch: 0637 loss_train: 2.3187 acc_train: 0.1506 loss_val: 2.3114 acc_val: 0.1526 time: 0.3758s\n",
      "Epoch: 0638 loss_train: 2.3217 acc_train: 0.1384 loss_val: 2.3113 acc_val: 0.1510 time: 0.4032s\n",
      "Epoch: 0639 loss_train: 2.3199 acc_train: 0.1413 loss_val: 2.3113 acc_val: 0.1526 time: 0.3874s\n",
      "Epoch: 0640 loss_train: 2.3175 acc_train: 0.1434 loss_val: 2.3113 acc_val: 0.1536 time: 0.3706s\n",
      "Epoch: 0641 loss_train: 2.3227 acc_train: 0.1396 loss_val: 2.3112 acc_val: 0.1536 time: 0.3581s\n",
      "Epoch: 0642 loss_train: 2.3191 acc_train: 0.1416 loss_val: 2.3112 acc_val: 0.1541 time: 0.3878s\n",
      "Epoch: 0643 loss_train: 2.3195 acc_train: 0.1507 loss_val: 2.3113 acc_val: 0.1521 time: 0.3561s\n",
      "Epoch: 0644 loss_train: 2.3246 acc_train: 0.1440 loss_val: 2.3113 acc_val: 0.1510 time: 0.3815s\n",
      "Epoch: 0645 loss_train: 2.3185 acc_train: 0.1479 loss_val: 2.3114 acc_val: 0.1516 time: 0.3597s\n",
      "Epoch: 0646 loss_train: 2.3157 acc_train: 0.1443 loss_val: 2.3115 acc_val: 0.1521 time: 0.3534s\n",
      "Epoch: 0647 loss_train: 2.3194 acc_train: 0.1441 loss_val: 2.3116 acc_val: 0.1521 time: 0.3860s\n",
      "Epoch: 0648 loss_train: 2.3199 acc_train: 0.1434 loss_val: 2.3116 acc_val: 0.1521 time: 0.3458s\n",
      "Epoch: 0649 loss_train: 2.3214 acc_train: 0.1409 loss_val: 2.3116 acc_val: 0.1521 time: 0.3692s\n",
      "Epoch: 0650 loss_train: 2.3241 acc_train: 0.1426 loss_val: 2.3118 acc_val: 0.1521 time: 0.3695s\n",
      "Epoch: 0651 loss_train: 2.3230 acc_train: 0.1440 loss_val: 2.3119 acc_val: 0.1510 time: 0.3674s\n",
      "Epoch: 0652 loss_train: 2.3173 acc_train: 0.1371 loss_val: 2.3120 acc_val: 0.1510 time: 0.3827s\n",
      "Epoch: 0653 loss_train: 2.3200 acc_train: 0.1382 loss_val: 2.3119 acc_val: 0.1510 time: 0.4026s\n",
      "Epoch: 0654 loss_train: 2.3269 acc_train: 0.1382 loss_val: 2.3118 acc_val: 0.1510 time: 0.3933s\n",
      "Epoch: 0655 loss_train: 2.3194 acc_train: 0.1440 loss_val: 2.3116 acc_val: 0.1510 time: 0.3988s\n",
      "Epoch: 0656 loss_train: 2.3234 acc_train: 0.1369 loss_val: 2.3114 acc_val: 0.1510 time: 0.4033s\n",
      "Epoch: 0657 loss_train: 2.3208 acc_train: 0.1494 loss_val: 2.3113 acc_val: 0.1526 time: 0.4015s\n",
      "Epoch: 0658 loss_train: 2.3198 acc_train: 0.1406 loss_val: 2.3112 acc_val: 0.1526 time: 0.3710s\n",
      "Epoch: 0659 loss_train: 2.3156 acc_train: 0.1490 loss_val: 2.3113 acc_val: 0.1536 time: 0.3528s\n",
      "Epoch: 0660 loss_train: 2.3149 acc_train: 0.1529 loss_val: 2.3114 acc_val: 0.1526 time: 0.4079s\n",
      "Epoch: 0661 loss_train: 2.3191 acc_train: 0.1419 loss_val: 2.3116 acc_val: 0.1526 time: 0.3932s\n",
      "Epoch: 0662 loss_train: 2.3178 acc_train: 0.1432 loss_val: 2.3120 acc_val: 0.1526 time: 0.3751s\n",
      "Epoch: 0663 loss_train: 2.3192 acc_train: 0.1402 loss_val: 2.3125 acc_val: 0.1531 time: 0.3865s\n",
      "Epoch: 0664 loss_train: 2.3162 acc_train: 0.1507 loss_val: 2.3130 acc_val: 0.1505 time: 0.3789s\n",
      "Epoch: 0665 loss_train: 2.3201 acc_train: 0.1431 loss_val: 2.3136 acc_val: 0.1531 time: 0.3834s\n",
      "Epoch: 0666 loss_train: 2.3215 acc_train: 0.1385 loss_val: 2.3138 acc_val: 0.1531 time: 0.3685s\n",
      "Epoch: 0667 loss_train: 2.3235 acc_train: 0.1390 loss_val: 2.3139 acc_val: 0.1510 time: 0.3890s\n",
      "Epoch: 0668 loss_train: 2.3194 acc_train: 0.1418 loss_val: 2.3136 acc_val: 0.1510 time: 0.3827s\n",
      "Epoch: 0669 loss_train: 2.3192 acc_train: 0.1529 loss_val: 2.3134 acc_val: 0.1510 time: 0.3722s\n",
      "Epoch: 0670 loss_train: 2.3182 acc_train: 0.1528 loss_val: 2.3129 acc_val: 0.1510 time: 0.3701s\n",
      "Epoch: 0671 loss_train: 2.3201 acc_train: 0.1471 loss_val: 2.3124 acc_val: 0.1510 time: 0.3766s\n",
      "Epoch: 0672 loss_train: 2.3138 acc_train: 0.1453 loss_val: 2.3119 acc_val: 0.1510 time: 0.3752s\n",
      "Epoch: 0673 loss_train: 2.3220 acc_train: 0.1466 loss_val: 2.3114 acc_val: 0.1521 time: 0.3877s\n",
      "Epoch: 0674 loss_train: 2.3203 acc_train: 0.1509 loss_val: 2.3110 acc_val: 0.1531 time: 0.4084s\n",
      "Epoch: 0675 loss_train: 2.3234 acc_train: 0.1330 loss_val: 2.3107 acc_val: 0.1536 time: 0.4055s\n",
      "Epoch: 0676 loss_train: 2.3237 acc_train: 0.1404 loss_val: 2.3108 acc_val: 0.1531 time: 0.3733s\n",
      "Epoch: 0677 loss_train: 2.3265 acc_train: 0.1471 loss_val: 2.3109 acc_val: 0.1521 time: 0.3979s\n",
      "Epoch: 0678 loss_train: 2.3210 acc_train: 0.1491 loss_val: 2.3111 acc_val: 0.1505 time: 0.3759s\n",
      "Epoch: 0679 loss_train: 2.3190 acc_train: 0.1454 loss_val: 2.3113 acc_val: 0.1495 time: 0.3621s\n",
      "Epoch: 0680 loss_train: 2.3171 acc_train: 0.1410 loss_val: 2.3117 acc_val: 0.1510 time: 0.3628s\n",
      "Epoch: 0681 loss_train: 2.3191 acc_train: 0.1366 loss_val: 2.3118 acc_val: 0.1510 time: 0.3493s\n",
      "Epoch: 0682 loss_train: 2.3214 acc_train: 0.1478 loss_val: 2.3119 acc_val: 0.1521 time: 0.3850s\n",
      "Epoch: 0683 loss_train: 2.3197 acc_train: 0.1462 loss_val: 2.3119 acc_val: 0.1526 time: 0.3983s\n",
      "Epoch: 0684 loss_train: 2.3216 acc_train: 0.1362 loss_val: 2.3120 acc_val: 0.1536 time: 0.3945s\n",
      "Epoch: 0685 loss_train: 2.3208 acc_train: 0.1421 loss_val: 2.3122 acc_val: 0.1531 time: 0.3821s\n",
      "Epoch: 0686 loss_train: 2.3231 acc_train: 0.1406 loss_val: 2.3124 acc_val: 0.1505 time: 0.3751s\n",
      "Epoch: 0687 loss_train: 2.3271 acc_train: 0.1443 loss_val: 2.3126 acc_val: 0.1505 time: 0.3556s\n",
      "Epoch: 0688 loss_train: 2.3155 acc_train: 0.1377 loss_val: 2.3126 acc_val: 0.1505 time: 0.3901s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0689 loss_train: 2.3229 acc_train: 0.1487 loss_val: 2.3128 acc_val: 0.1510 time: 0.3673s\n",
      "Epoch: 0690 loss_train: 2.3257 acc_train: 0.1335 loss_val: 2.3129 acc_val: 0.1510 time: 0.3594s\n",
      "Epoch: 0691 loss_train: 2.3152 acc_train: 0.1484 loss_val: 2.3129 acc_val: 0.1510 time: 0.4092s\n",
      "Epoch: 0692 loss_train: 2.3228 acc_train: 0.1441 loss_val: 2.3127 acc_val: 0.1526 time: 0.3913s\n",
      "Epoch: 0693 loss_train: 2.3243 acc_train: 0.1347 loss_val: 2.3126 acc_val: 0.1521 time: 0.3725s\n",
      "Epoch: 0694 loss_train: 2.3154 acc_train: 0.1481 loss_val: 2.3124 acc_val: 0.1521 time: 0.3911s\n",
      "Epoch: 0695 loss_train: 2.3243 acc_train: 0.1440 loss_val: 2.3124 acc_val: 0.1516 time: 0.3910s\n",
      "Epoch: 0696 loss_train: 2.3236 acc_train: 0.1450 loss_val: 2.3122 acc_val: 0.1521 time: 0.3900s\n",
      "Epoch: 0697 loss_train: 2.3189 acc_train: 0.1385 loss_val: 2.3123 acc_val: 0.1521 time: 0.3713s\n",
      "Epoch: 0698 loss_train: 2.3209 acc_train: 0.1500 loss_val: 2.3124 acc_val: 0.1531 time: 0.3515s\n",
      "Epoch: 0699 loss_train: 2.3172 acc_train: 0.1390 loss_val: 2.3126 acc_val: 0.1536 time: 0.3961s\n",
      "Epoch: 0700 loss_train: 2.3191 acc_train: 0.1506 loss_val: 2.3126 acc_val: 0.1546 time: 0.3714s\n",
      "Epoch: 0701 loss_train: 2.3192 acc_train: 0.1431 loss_val: 2.3126 acc_val: 0.1536 time: 0.3865s\n",
      "Epoch: 0702 loss_train: 2.3202 acc_train: 0.1493 loss_val: 2.3126 acc_val: 0.1521 time: 0.3903s\n",
      "Epoch: 0703 loss_train: 2.3227 acc_train: 0.1468 loss_val: 2.3126 acc_val: 0.1516 time: 0.3948s\n",
      "Epoch: 0704 loss_train: 2.3184 acc_train: 0.1413 loss_val: 2.3125 acc_val: 0.1516 time: 0.3867s\n",
      "Epoch: 0705 loss_train: 2.3189 acc_train: 0.1515 loss_val: 2.3125 acc_val: 0.1516 time: 0.3944s\n",
      "Epoch: 0706 loss_train: 2.3202 acc_train: 0.1517 loss_val: 2.3125 acc_val: 0.1510 time: 0.3697s\n",
      "Epoch: 0707 loss_train: 2.3194 acc_train: 0.1475 loss_val: 2.3125 acc_val: 0.1521 time: 0.3577s\n",
      "Epoch: 0708 loss_train: 2.3171 acc_train: 0.1478 loss_val: 2.3126 acc_val: 0.1526 time: 0.3950s\n",
      "Epoch: 0709 loss_train: 2.3214 acc_train: 0.1400 loss_val: 2.3126 acc_val: 0.1531 time: 0.3767s\n",
      "Epoch: 0710 loss_train: 2.3245 acc_train: 0.1372 loss_val: 2.3128 acc_val: 0.1541 time: 0.4133s\n",
      "Epoch: 0711 loss_train: 2.3252 acc_train: 0.1369 loss_val: 2.3128 acc_val: 0.1531 time: 0.3725s\n",
      "Epoch: 0712 loss_train: 2.3172 acc_train: 0.1432 loss_val: 2.3128 acc_val: 0.1516 time: 0.3937s\n",
      "Epoch: 0713 loss_train: 2.3217 acc_train: 0.1418 loss_val: 2.3129 acc_val: 0.1505 time: 0.3776s\n",
      "Epoch: 0714 loss_train: 2.3194 acc_train: 0.1409 loss_val: 2.3129 acc_val: 0.1521 time: 0.3916s\n",
      "Epoch: 0715 loss_train: 2.3176 acc_train: 0.1447 loss_val: 2.3130 acc_val: 0.1521 time: 0.3740s\n",
      "Epoch: 0716 loss_train: 2.3152 acc_train: 0.1488 loss_val: 2.3129 acc_val: 0.1531 time: 0.3789s\n",
      "Epoch: 0717 loss_train: 2.3214 acc_train: 0.1380 loss_val: 2.3128 acc_val: 0.1521 time: 0.3772s\n",
      "Epoch: 0718 loss_train: 2.3223 acc_train: 0.1475 loss_val: 2.3127 acc_val: 0.1526 time: 0.3800s\n",
      "Epoch: 0719 loss_train: 2.3209 acc_train: 0.1381 loss_val: 2.3126 acc_val: 0.1526 time: 0.3906s\n",
      "Epoch: 0720 loss_train: 2.3169 acc_train: 0.1537 loss_val: 2.3123 acc_val: 0.1526 time: 0.3733s\n",
      "Epoch: 0721 loss_train: 2.3254 acc_train: 0.1366 loss_val: 2.3122 acc_val: 0.1531 time: 0.3591s\n",
      "Epoch: 0722 loss_train: 2.3196 acc_train: 0.1497 loss_val: 2.3121 acc_val: 0.1516 time: 0.3530s\n",
      "Epoch: 0723 loss_train: 2.3252 acc_train: 0.1426 loss_val: 2.3122 acc_val: 0.1521 time: 0.3864s\n",
      "Epoch: 0724 loss_train: 2.3193 acc_train: 0.1497 loss_val: 2.3124 acc_val: 0.1521 time: 0.3818s\n",
      "Epoch: 0725 loss_train: 2.3195 acc_train: 0.1385 loss_val: 2.3126 acc_val: 0.1505 time: 0.3572s\n",
      "Epoch: 0726 loss_train: 2.3160 acc_train: 0.1434 loss_val: 2.3129 acc_val: 0.1505 time: 0.3759s\n",
      "Epoch: 0727 loss_train: 2.3253 acc_train: 0.1390 loss_val: 2.3131 acc_val: 0.1505 time: 0.3475s\n",
      "Epoch: 0728 loss_train: 2.3159 acc_train: 0.1444 loss_val: 2.3133 acc_val: 0.1531 time: 0.3716s\n",
      "Epoch: 0729 loss_train: 2.3263 acc_train: 0.1390 loss_val: 2.3135 acc_val: 0.1536 time: 0.3731s\n",
      "Epoch: 0730 loss_train: 2.3174 acc_train: 0.1428 loss_val: 2.3135 acc_val: 0.1536 time: 0.3664s\n",
      "Epoch: 0731 loss_train: 2.3147 acc_train: 0.1413 loss_val: 2.3134 acc_val: 0.1526 time: 0.3525s\n",
      "Epoch: 0732 loss_train: 2.3213 acc_train: 0.1399 loss_val: 2.3130 acc_val: 0.1516 time: 0.3641s\n",
      "Epoch: 0733 loss_train: 2.3202 acc_train: 0.1443 loss_val: 2.3127 acc_val: 0.1510 time: 0.3558s\n",
      "Epoch: 0734 loss_train: 2.3186 acc_train: 0.1507 loss_val: 2.3122 acc_val: 0.1510 time: 0.3880s\n",
      "Epoch: 0735 loss_train: 2.3146 acc_train: 0.1510 loss_val: 2.3118 acc_val: 0.1521 time: 0.4099s\n",
      "Epoch: 0736 loss_train: 2.3217 acc_train: 0.1400 loss_val: 2.3117 acc_val: 0.1521 time: 0.4024s\n",
      "Epoch: 0737 loss_train: 2.3227 acc_train: 0.1446 loss_val: 2.3117 acc_val: 0.1516 time: 0.3890s\n",
      "Epoch: 0738 loss_train: 2.3261 acc_train: 0.1397 loss_val: 2.3118 acc_val: 0.1531 time: 0.3580s\n",
      "Epoch: 0739 loss_train: 2.3199 acc_train: 0.1399 loss_val: 2.3121 acc_val: 0.1531 time: 0.3820s\n",
      "Epoch: 0740 loss_train: 2.3222 acc_train: 0.1443 loss_val: 2.3124 acc_val: 0.1521 time: 0.3942s\n",
      "Epoch: 0741 loss_train: 2.3174 acc_train: 0.1547 loss_val: 2.3127 acc_val: 0.1521 time: 0.3766s\n",
      "Epoch: 0742 loss_train: 2.3221 acc_train: 0.1410 loss_val: 2.3128 acc_val: 0.1526 time: 0.3674s\n",
      "Epoch: 0743 loss_train: 2.3169 acc_train: 0.1479 loss_val: 2.3130 acc_val: 0.1526 time: 0.3828s\n",
      "Epoch: 0744 loss_train: 2.3214 acc_train: 0.1466 loss_val: 2.3132 acc_val: 0.1531 time: 0.3709s\n",
      "Epoch: 0745 loss_train: 2.3183 acc_train: 0.1391 loss_val: 2.3134 acc_val: 0.1526 time: 0.3776s\n",
      "Epoch: 0746 loss_train: 2.3200 acc_train: 0.1459 loss_val: 2.3136 acc_val: 0.1526 time: 0.3982s\n",
      "Epoch: 0747 loss_train: 2.3164 acc_train: 0.1397 loss_val: 2.3138 acc_val: 0.1526 time: 0.3825s\n",
      "Epoch: 0748 loss_train: 2.3204 acc_train: 0.1507 loss_val: 2.3138 acc_val: 0.1526 time: 0.3680s\n",
      "Epoch: 0749 loss_train: 2.3227 acc_train: 0.1397 loss_val: 2.3137 acc_val: 0.1510 time: 0.4097s\n",
      "Epoch: 0750 loss_train: 2.3152 acc_train: 0.1556 loss_val: 2.3133 acc_val: 0.1516 time: 0.3879s\n",
      "Epoch: 0751 loss_train: 2.3167 acc_train: 0.1475 loss_val: 2.3129 acc_val: 0.1510 time: 0.3848s\n",
      "Epoch: 0752 loss_train: 2.3209 acc_train: 0.1490 loss_val: 2.3124 acc_val: 0.1505 time: 0.3879s\n",
      "Epoch: 0753 loss_train: 2.3226 acc_train: 0.1382 loss_val: 2.3119 acc_val: 0.1510 time: 0.3845s\n",
      "Epoch: 0754 loss_train: 2.3183 acc_train: 0.1460 loss_val: 2.3115 acc_val: 0.1531 time: 0.3802s\n",
      "Epoch: 0755 loss_train: 2.3175 acc_train: 0.1500 loss_val: 2.3111 acc_val: 0.1510 time: 0.3665s\n",
      "Epoch: 0756 loss_train: 2.3169 acc_train: 0.1491 loss_val: 2.3109 acc_val: 0.1536 time: 0.3921s\n",
      "Epoch: 0757 loss_train: 2.3218 acc_train: 0.1409 loss_val: 2.3111 acc_val: 0.1521 time: 0.3656s\n",
      "Epoch: 0758 loss_train: 2.3214 acc_train: 0.1434 loss_val: 2.3117 acc_val: 0.1536 time: 0.3890s\n",
      "Epoch: 0759 loss_train: 2.3208 acc_train: 0.1472 loss_val: 2.3122 acc_val: 0.1526 time: 0.3561s\n",
      "Epoch: 0760 loss_train: 2.3202 acc_train: 0.1441 loss_val: 2.3126 acc_val: 0.1546 time: 0.3834s\n",
      "Epoch: 0761 loss_train: 2.3178 acc_train: 0.1456 loss_val: 2.3130 acc_val: 0.1516 time: 0.3745s\n",
      "Epoch: 0762 loss_train: 2.3180 acc_train: 0.1469 loss_val: 2.3133 acc_val: 0.1505 time: 0.3691s\n",
      "Epoch: 0763 loss_train: 2.3221 acc_train: 0.1460 loss_val: 2.3134 acc_val: 0.1495 time: 0.3625s\n",
      "Epoch: 0764 loss_train: 2.3240 acc_train: 0.1416 loss_val: 2.3133 acc_val: 0.1500 time: 0.3649s\n",
      "Epoch: 0765 loss_train: 2.3165 acc_train: 0.1377 loss_val: 2.3132 acc_val: 0.1505 time: 0.3503s\n",
      "Epoch: 0766 loss_train: 2.3115 acc_train: 0.1407 loss_val: 2.3131 acc_val: 0.1510 time: 0.3765s\n",
      "Epoch: 0767 loss_train: 2.3174 acc_train: 0.1471 loss_val: 2.3127 acc_val: 0.1531 time: 0.3957s\n",
      "Epoch: 0768 loss_train: 2.3140 acc_train: 0.1479 loss_val: 2.3122 acc_val: 0.1510 time: 0.3819s\n",
      "Epoch: 0769 loss_train: 2.3112 acc_train: 0.1451 loss_val: 2.3118 acc_val: 0.1516 time: 0.3705s\n",
      "Epoch: 0770 loss_train: 2.3204 acc_train: 0.1481 loss_val: 2.3114 acc_val: 0.1510 time: 0.3985s\n",
      "Epoch: 0771 loss_train: 2.3153 acc_train: 0.1510 loss_val: 2.3111 acc_val: 0.1536 time: 0.3538s\n",
      "Epoch: 0772 loss_train: 2.3185 acc_train: 0.1487 loss_val: 2.3110 acc_val: 0.1526 time: 0.3522s\n",
      "Epoch: 0773 loss_train: 2.3218 acc_train: 0.1468 loss_val: 2.3110 acc_val: 0.1531 time: 0.3593s\n",
      "Epoch: 0774 loss_train: 2.3202 acc_train: 0.1412 loss_val: 2.3112 acc_val: 0.1505 time: 0.4037s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0775 loss_train: 2.3159 acc_train: 0.1498 loss_val: 2.3117 acc_val: 0.1510 time: 0.3691s\n",
      "Epoch: 0776 loss_train: 2.3148 acc_train: 0.1453 loss_val: 2.3122 acc_val: 0.1521 time: 0.3474s\n",
      "Epoch: 0777 loss_train: 2.3142 acc_train: 0.1482 loss_val: 2.3126 acc_val: 0.1521 time: 0.3795s\n",
      "Epoch: 0778 loss_train: 2.3185 acc_train: 0.1517 loss_val: 2.3128 acc_val: 0.1526 time: 0.4097s\n",
      "Epoch: 0779 loss_train: 2.3189 acc_train: 0.1428 loss_val: 2.3131 acc_val: 0.1510 time: 0.3775s\n",
      "Epoch: 0780 loss_train: 2.3151 acc_train: 0.1472 loss_val: 2.3131 acc_val: 0.1510 time: 0.3736s\n",
      "Epoch: 0781 loss_train: 2.3219 acc_train: 0.1426 loss_val: 2.3129 acc_val: 0.1531 time: 0.3771s\n",
      "Epoch: 0782 loss_train: 2.3174 acc_train: 0.1428 loss_val: 2.3125 acc_val: 0.1521 time: 0.3677s\n",
      "Epoch: 0783 loss_train: 2.3160 acc_train: 0.1441 loss_val: 2.3120 acc_val: 0.1516 time: 0.3866s\n",
      "Epoch: 0784 loss_train: 2.3199 acc_train: 0.1416 loss_val: 2.3115 acc_val: 0.1490 time: 0.3630s\n",
      "Epoch: 0785 loss_train: 2.3189 acc_train: 0.1432 loss_val: 2.3113 acc_val: 0.1505 time: 0.3669s\n",
      "Epoch: 0786 loss_train: 2.3186 acc_train: 0.1463 loss_val: 2.3110 acc_val: 0.1541 time: 0.3766s\n",
      "Epoch: 0787 loss_train: 2.3225 acc_train: 0.1526 loss_val: 2.3109 acc_val: 0.1521 time: 0.3565s\n",
      "Epoch: 0788 loss_train: 2.3208 acc_train: 0.1444 loss_val: 2.3107 acc_val: 0.1526 time: 0.3494s\n",
      "Epoch: 0789 loss_train: 2.3186 acc_train: 0.1503 loss_val: 2.3106 acc_val: 0.1526 time: 0.3737s\n",
      "Epoch: 0790 loss_train: 2.3216 acc_train: 0.1426 loss_val: 2.3107 acc_val: 0.1510 time: 0.3582s\n",
      "Epoch: 0791 loss_train: 2.3208 acc_train: 0.1374 loss_val: 2.3109 acc_val: 0.1521 time: 0.3923s\n",
      "Epoch: 0792 loss_train: 2.3213 acc_train: 0.1498 loss_val: 2.3112 acc_val: 0.1526 time: 0.3872s\n",
      "Epoch: 0793 loss_train: 2.3184 acc_train: 0.1473 loss_val: 2.3114 acc_val: 0.1531 time: 0.3922s\n",
      "Epoch: 0794 loss_train: 2.3207 acc_train: 0.1416 loss_val: 2.3116 acc_val: 0.1526 time: 0.3843s\n",
      "Epoch: 0795 loss_train: 2.3259 acc_train: 0.1387 loss_val: 2.3118 acc_val: 0.1516 time: 0.3816s\n",
      "Epoch: 0796 loss_train: 2.3224 acc_train: 0.1456 loss_val: 2.3117 acc_val: 0.1505 time: 0.4004s\n",
      "Epoch: 0797 loss_train: 2.3181 acc_train: 0.1450 loss_val: 2.3116 acc_val: 0.1485 time: 0.3971s\n",
      "Epoch: 0798 loss_train: 2.3180 acc_train: 0.1525 loss_val: 2.3114 acc_val: 0.1510 time: 0.3722s\n",
      "Epoch: 0799 loss_train: 2.3188 acc_train: 0.1407 loss_val: 2.3113 acc_val: 0.1510 time: 0.3563s\n",
      "Epoch: 0800 loss_train: 2.3223 acc_train: 0.1447 loss_val: 2.3111 acc_val: 0.1536 time: 0.4001s\n",
      "Epoch: 0801 loss_train: 2.3175 acc_train: 0.1542 loss_val: 2.3111 acc_val: 0.1516 time: 0.3839s\n",
      "Epoch: 0802 loss_train: 2.3227 acc_train: 0.1365 loss_val: 2.3112 acc_val: 0.1531 time: 0.3913s\n",
      "Epoch: 0803 loss_train: 2.3155 acc_train: 0.1448 loss_val: 2.3114 acc_val: 0.1526 time: 0.3875s\n",
      "Epoch: 0804 loss_train: 2.3212 acc_train: 0.1544 loss_val: 2.3116 acc_val: 0.1536 time: 0.3897s\n",
      "Epoch: 0805 loss_train: 2.3230 acc_train: 0.1402 loss_val: 2.3118 acc_val: 0.1521 time: 0.3839s\n",
      "Epoch: 0806 loss_train: 2.3199 acc_train: 0.1406 loss_val: 2.3123 acc_val: 0.1531 time: 0.4003s\n",
      "Epoch: 0807 loss_train: 2.3197 acc_train: 0.1413 loss_val: 2.3126 acc_val: 0.1516 time: 0.3671s\n",
      "Epoch: 0808 loss_train: 2.3234 acc_train: 0.1478 loss_val: 2.3126 acc_val: 0.1510 time: 0.4095s\n",
      "Epoch: 0809 loss_train: 2.3192 acc_train: 0.1385 loss_val: 2.3125 acc_val: 0.1495 time: 0.3721s\n",
      "Epoch: 0810 loss_train: 2.3185 acc_train: 0.1454 loss_val: 2.3122 acc_val: 0.1521 time: 0.3946s\n",
      "Epoch: 0811 loss_train: 2.3155 acc_train: 0.1581 loss_val: 2.3119 acc_val: 0.1521 time: 0.3753s\n",
      "Epoch: 0812 loss_train: 2.3187 acc_train: 0.1457 loss_val: 2.3116 acc_val: 0.1526 time: 0.3776s\n",
      "Epoch: 0813 loss_train: 2.3169 acc_train: 0.1468 loss_val: 2.3116 acc_val: 0.1510 time: 0.3763s\n",
      "Epoch: 0814 loss_train: 2.3203 acc_train: 0.1425 loss_val: 2.3116 acc_val: 0.1510 time: 0.3734s\n",
      "Epoch: 0815 loss_train: 2.3171 acc_train: 0.1522 loss_val: 2.3116 acc_val: 0.1526 time: 0.3875s\n",
      "Epoch: 0816 loss_train: 2.3218 acc_train: 0.1388 loss_val: 2.3116 acc_val: 0.1521 time: 0.3850s\n",
      "Epoch: 0817 loss_train: 2.3220 acc_train: 0.1463 loss_val: 2.3114 acc_val: 0.1526 time: 0.4042s\n",
      "Epoch: 0818 loss_train: 2.3192 acc_train: 0.1412 loss_val: 2.3113 acc_val: 0.1505 time: 0.4045s\n",
      "Epoch: 0819 loss_train: 2.3205 acc_train: 0.1498 loss_val: 2.3111 acc_val: 0.1500 time: 0.3977s\n",
      "Epoch: 0820 loss_train: 2.3145 acc_train: 0.1481 loss_val: 2.3109 acc_val: 0.1505 time: 0.3666s\n",
      "Epoch: 0821 loss_train: 2.3193 acc_train: 0.1460 loss_val: 2.3108 acc_val: 0.1521 time: 0.3565s\n",
      "Epoch: 0822 loss_train: 2.3193 acc_train: 0.1421 loss_val: 2.3108 acc_val: 0.1510 time: 0.3815s\n",
      "Epoch: 0823 loss_train: 2.3204 acc_train: 0.1456 loss_val: 2.3109 acc_val: 0.1526 time: 0.3929s\n",
      "Epoch: 0824 loss_train: 2.3205 acc_train: 0.1429 loss_val: 2.3113 acc_val: 0.1505 time: 0.3904s\n",
      "Epoch: 0825 loss_train: 2.3161 acc_train: 0.1447 loss_val: 2.3117 acc_val: 0.1526 time: 0.4086s\n",
      "Epoch: 0826 loss_train: 2.3157 acc_train: 0.1520 loss_val: 2.3120 acc_val: 0.1526 time: 0.3883s\n",
      "Epoch: 0827 loss_train: 2.3133 acc_train: 0.1515 loss_val: 2.3121 acc_val: 0.1536 time: 0.3912s\n",
      "Epoch: 0828 loss_train: 2.3169 acc_train: 0.1456 loss_val: 2.3122 acc_val: 0.1505 time: 0.3854s\n",
      "Epoch: 0829 loss_train: 2.3249 acc_train: 0.1432 loss_val: 2.3124 acc_val: 0.1531 time: 0.3751s\n",
      "Epoch: 0830 loss_train: 2.3155 acc_train: 0.1493 loss_val: 2.3125 acc_val: 0.1531 time: 0.3904s\n",
      "Epoch: 0831 loss_train: 2.3217 acc_train: 0.1447 loss_val: 2.3123 acc_val: 0.1505 time: 0.3916s\n",
      "Epoch: 0832 loss_train: 2.3145 acc_train: 0.1503 loss_val: 2.3119 acc_val: 0.1505 time: 0.3831s\n",
      "Epoch: 0833 loss_train: 2.3183 acc_train: 0.1490 loss_val: 2.3116 acc_val: 0.1500 time: 0.3580s\n",
      "Epoch: 0834 loss_train: 2.3146 acc_train: 0.1479 loss_val: 2.3114 acc_val: 0.1510 time: 0.3800s\n",
      "Epoch: 0835 loss_train: 2.3231 acc_train: 0.1424 loss_val: 2.3111 acc_val: 0.1521 time: 0.4209s\n",
      "Epoch: 0836 loss_train: 2.3223 acc_train: 0.1415 loss_val: 2.3107 acc_val: 0.1541 time: 0.3922s\n",
      "Epoch: 0837 loss_train: 2.3193 acc_train: 0.1428 loss_val: 2.3103 acc_val: 0.1541 time: 0.3960s\n",
      "Epoch: 0838 loss_train: 2.3215 acc_train: 0.1381 loss_val: 2.3101 acc_val: 0.1536 time: 0.3664s\n",
      "Epoch: 0839 loss_train: 2.3170 acc_train: 0.1517 loss_val: 2.3101 acc_val: 0.1526 time: 0.3593s\n",
      "Epoch: 0840 loss_train: 2.3190 acc_train: 0.1495 loss_val: 2.3105 acc_val: 0.1531 time: 0.3933s\n",
      "Epoch: 0841 loss_train: 2.3161 acc_train: 0.1419 loss_val: 2.3113 acc_val: 0.1526 time: 0.3852s\n",
      "Epoch: 0842 loss_train: 2.3209 acc_train: 0.1403 loss_val: 2.3119 acc_val: 0.1526 time: 0.3702s\n",
      "Epoch: 0843 loss_train: 2.3180 acc_train: 0.1447 loss_val: 2.3125 acc_val: 0.1505 time: 0.3924s\n",
      "Epoch: 0844 loss_train: 2.3185 acc_train: 0.1393 loss_val: 2.3128 acc_val: 0.1526 time: 0.3914s\n",
      "Epoch: 0845 loss_train: 2.3142 acc_train: 0.1495 loss_val: 2.3130 acc_val: 0.1516 time: 0.3540s\n",
      "Epoch: 0846 loss_train: 2.3227 acc_train: 0.1504 loss_val: 2.3129 acc_val: 0.1521 time: 0.3903s\n",
      "Epoch: 0847 loss_train: 2.3178 acc_train: 0.1353 loss_val: 2.3126 acc_val: 0.1521 time: 0.4044s\n",
      "Epoch: 0848 loss_train: 2.3193 acc_train: 0.1468 loss_val: 2.3122 acc_val: 0.1521 time: 0.3881s\n",
      "Epoch: 0849 loss_train: 2.3171 acc_train: 0.1478 loss_val: 2.3118 acc_val: 0.1531 time: 0.3495s\n",
      "Epoch: 0850 loss_train: 2.3178 acc_train: 0.1429 loss_val: 2.3113 acc_val: 0.1526 time: 0.3850s\n",
      "Epoch: 0851 loss_train: 2.3191 acc_train: 0.1485 loss_val: 2.3109 acc_val: 0.1521 time: 0.3930s\n",
      "Epoch: 0852 loss_train: 2.3202 acc_train: 0.1457 loss_val: 2.3107 acc_val: 0.1521 time: 0.4171s\n",
      "Epoch: 0853 loss_train: 2.3176 acc_train: 0.1473 loss_val: 2.3107 acc_val: 0.1510 time: 0.3808s\n",
      "Epoch: 0854 loss_train: 2.3181 acc_train: 0.1448 loss_val: 2.3107 acc_val: 0.1521 time: 0.3943s\n",
      "Epoch: 0855 loss_train: 2.3257 acc_train: 0.1448 loss_val: 2.3109 acc_val: 0.1521 time: 0.3881s\n",
      "Epoch: 0856 loss_train: 2.3178 acc_train: 0.1424 loss_val: 2.3112 acc_val: 0.1521 time: 0.3777s\n",
      "Epoch: 0857 loss_train: 2.3161 acc_train: 0.1409 loss_val: 2.3116 acc_val: 0.1505 time: 0.3983s\n",
      "Epoch: 0858 loss_train: 2.3213 acc_train: 0.1363 loss_val: 2.3121 acc_val: 0.1510 time: 0.3965s\n",
      "Epoch: 0859 loss_train: 2.3169 acc_train: 0.1434 loss_val: 2.3127 acc_val: 0.1505 time: 0.3728s\n",
      "Epoch: 0860 loss_train: 2.3202 acc_train: 0.1418 loss_val: 2.3132 acc_val: 0.1531 time: 0.3919s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0861 loss_train: 2.3224 acc_train: 0.1381 loss_val: 2.3134 acc_val: 0.1531 time: 0.3878s\n",
      "Epoch: 0862 loss_train: 2.3194 acc_train: 0.1438 loss_val: 2.3132 acc_val: 0.1521 time: 0.4228s\n",
      "Epoch: 0863 loss_train: 2.3190 acc_train: 0.1334 loss_val: 2.3128 acc_val: 0.1521 time: 0.3961s\n",
      "Epoch: 0864 loss_train: 2.3207 acc_train: 0.1396 loss_val: 2.3125 acc_val: 0.1521 time: 0.4195s\n",
      "Epoch: 0865 loss_train: 2.3199 acc_train: 0.1460 loss_val: 2.3122 acc_val: 0.1526 time: 0.4178s\n",
      "Epoch: 0866 loss_train: 2.3255 acc_train: 0.1460 loss_val: 2.3118 acc_val: 0.1526 time: 0.4008s\n",
      "Epoch: 0867 loss_train: 2.3190 acc_train: 0.1537 loss_val: 2.3113 acc_val: 0.1531 time: 0.3802s\n",
      "Epoch: 0868 loss_train: 2.3176 acc_train: 0.1479 loss_val: 2.3108 acc_val: 0.1531 time: 0.3935s\n",
      "Epoch: 0869 loss_train: 2.3193 acc_train: 0.1500 loss_val: 2.3103 acc_val: 0.1505 time: 0.4071s\n",
      "Epoch: 0870 loss_train: 2.3180 acc_train: 0.1490 loss_val: 2.3101 acc_val: 0.1526 time: 0.3766s\n",
      "Epoch: 0871 loss_train: 2.3169 acc_train: 0.1422 loss_val: 2.3100 acc_val: 0.1526 time: 0.4070s\n",
      "Epoch: 0872 loss_train: 2.3183 acc_train: 0.1495 loss_val: 2.3101 acc_val: 0.1526 time: 0.4064s\n",
      "Epoch: 0873 loss_train: 2.3150 acc_train: 0.1485 loss_val: 2.3103 acc_val: 0.1521 time: 0.3705s\n",
      "Epoch: 0874 loss_train: 2.3209 acc_train: 0.1407 loss_val: 2.3106 acc_val: 0.1531 time: 0.3727s\n",
      "Epoch: 0875 loss_train: 2.3253 acc_train: 0.1410 loss_val: 2.3108 acc_val: 0.1521 time: 0.3853s\n",
      "Epoch: 0876 loss_train: 2.3234 acc_train: 0.1407 loss_val: 2.3111 acc_val: 0.1531 time: 0.3936s\n",
      "Epoch: 0877 loss_train: 2.3176 acc_train: 0.1491 loss_val: 2.3113 acc_val: 0.1521 time: 0.3595s\n",
      "Epoch: 0878 loss_train: 2.3161 acc_train: 0.1504 loss_val: 2.3116 acc_val: 0.1521 time: 0.3879s\n",
      "Epoch: 0879 loss_train: 2.3164 acc_train: 0.1525 loss_val: 2.3119 acc_val: 0.1521 time: 0.3656s\n",
      "Epoch: 0880 loss_train: 2.3193 acc_train: 0.1475 loss_val: 2.3119 acc_val: 0.1510 time: 0.4012s\n",
      "Epoch: 0881 loss_train: 2.3214 acc_train: 0.1509 loss_val: 2.3120 acc_val: 0.1510 time: 0.3587s\n",
      "Epoch: 0882 loss_train: 2.3184 acc_train: 0.1475 loss_val: 2.3122 acc_val: 0.1516 time: 0.3818s\n",
      "Epoch: 0883 loss_train: 2.3205 acc_train: 0.1504 loss_val: 2.3121 acc_val: 0.1521 time: 0.4211s\n",
      "Epoch: 0884 loss_train: 2.3178 acc_train: 0.1434 loss_val: 2.3120 acc_val: 0.1531 time: 0.3340s\n",
      "Epoch: 0885 loss_train: 2.3201 acc_train: 0.1419 loss_val: 2.3121 acc_val: 0.1531 time: 0.3624s\n",
      "Epoch: 0886 loss_train: 2.3222 acc_train: 0.1426 loss_val: 2.3122 acc_val: 0.1536 time: 0.3904s\n",
      "Epoch: 0887 loss_train: 2.3179 acc_train: 0.1397 loss_val: 2.3123 acc_val: 0.1536 time: 0.3695s\n",
      "Epoch: 0888 loss_train: 2.3189 acc_train: 0.1454 loss_val: 2.3124 acc_val: 0.1531 time: 0.3648s\n",
      "Epoch: 0889 loss_train: 2.3190 acc_train: 0.1494 loss_val: 2.3123 acc_val: 0.1521 time: 0.3802s\n",
      "Epoch: 0890 loss_train: 2.3156 acc_train: 0.1531 loss_val: 2.3121 acc_val: 0.1521 time: 0.3606s\n",
      "Epoch: 0891 loss_train: 2.3171 acc_train: 0.1352 loss_val: 2.3117 acc_val: 0.1521 time: 0.3613s\n",
      "Epoch: 0892 loss_train: 2.3215 acc_train: 0.1438 loss_val: 2.3113 acc_val: 0.1521 time: 0.3792s\n",
      "Epoch: 0893 loss_train: 2.3216 acc_train: 0.1419 loss_val: 2.3109 acc_val: 0.1526 time: 0.3741s\n",
      "Epoch: 0894 loss_train: 2.3109 acc_train: 0.1501 loss_val: 2.3105 acc_val: 0.1521 time: 0.3625s\n",
      "Epoch: 0895 loss_train: 2.3196 acc_train: 0.1446 loss_val: 2.3101 acc_val: 0.1526 time: 0.4045s\n",
      "Epoch: 0896 loss_train: 2.3214 acc_train: 0.1431 loss_val: 2.3100 acc_val: 0.1531 time: 0.3635s\n",
      "Epoch: 0897 loss_train: 2.3170 acc_train: 0.1448 loss_val: 2.3100 acc_val: 0.1531 time: 0.3860s\n",
      "Epoch: 0898 loss_train: 2.3199 acc_train: 0.1415 loss_val: 2.3102 acc_val: 0.1536 time: 0.3689s\n",
      "Epoch: 0899 loss_train: 2.3185 acc_train: 0.1482 loss_val: 2.3106 acc_val: 0.1536 time: 0.3873s\n",
      "Epoch: 0900 loss_train: 2.3216 acc_train: 0.1435 loss_val: 2.3110 acc_val: 0.1521 time: 0.3651s\n",
      "Epoch: 0901 loss_train: 2.3172 acc_train: 0.1347 loss_val: 2.3117 acc_val: 0.1521 time: 0.3595s\n",
      "Epoch: 0902 loss_train: 2.3180 acc_train: 0.1447 loss_val: 2.3122 acc_val: 0.1521 time: 0.3667s\n",
      "Epoch: 0903 loss_train: 2.3235 acc_train: 0.1473 loss_val: 2.3126 acc_val: 0.1521 time: 0.3797s\n",
      "Epoch: 0904 loss_train: 2.3222 acc_train: 0.1482 loss_val: 2.3130 acc_val: 0.1521 time: 0.3534s\n",
      "Epoch: 0905 loss_train: 2.3208 acc_train: 0.1472 loss_val: 2.3133 acc_val: 0.1510 time: 0.3829s\n",
      "Epoch: 0906 loss_train: 2.3217 acc_train: 0.1422 loss_val: 2.3134 acc_val: 0.1521 time: 0.3905s\n",
      "Epoch: 0907 loss_train: 2.3191 acc_train: 0.1517 loss_val: 2.3132 acc_val: 0.1521 time: 0.3832s\n",
      "Epoch: 0908 loss_train: 2.3152 acc_train: 0.1447 loss_val: 2.3129 acc_val: 0.1531 time: 0.3963s\n",
      "Epoch: 0909 loss_train: 2.3215 acc_train: 0.1490 loss_val: 2.3124 acc_val: 0.1521 time: 0.3705s\n",
      "Epoch: 0910 loss_train: 2.3132 acc_train: 0.1466 loss_val: 2.3121 acc_val: 0.1521 time: 0.3706s\n",
      "Epoch: 0911 loss_train: 2.3178 acc_train: 0.1490 loss_val: 2.3119 acc_val: 0.1516 time: 0.3756s\n",
      "Epoch: 0912 loss_train: 2.3231 acc_train: 0.1446 loss_val: 2.3117 acc_val: 0.1521 time: 0.3701s\n",
      "Epoch: 0913 loss_train: 2.3168 acc_train: 0.1487 loss_val: 2.3115 acc_val: 0.1516 time: 0.3818s\n",
      "Epoch: 0914 loss_train: 2.3247 acc_train: 0.1487 loss_val: 2.3115 acc_val: 0.1510 time: 0.3664s\n",
      "Epoch: 0915 loss_train: 2.3225 acc_train: 0.1438 loss_val: 2.3116 acc_val: 0.1510 time: 0.4085s\n",
      "Epoch: 0916 loss_train: 2.3200 acc_train: 0.1466 loss_val: 2.3117 acc_val: 0.1510 time: 0.3955s\n",
      "Epoch: 0917 loss_train: 2.3207 acc_train: 0.1363 loss_val: 2.3118 acc_val: 0.1510 time: 0.3892s\n",
      "Epoch: 0918 loss_train: 2.3209 acc_train: 0.1425 loss_val: 2.3119 acc_val: 0.1510 time: 0.4026s\n",
      "Epoch: 0919 loss_train: 2.3223 acc_train: 0.1469 loss_val: 2.3121 acc_val: 0.1510 time: 0.3954s\n",
      "Epoch: 0920 loss_train: 2.3141 acc_train: 0.1416 loss_val: 2.3122 acc_val: 0.1510 time: 0.3866s\n",
      "Epoch: 0921 loss_train: 2.3220 acc_train: 0.1448 loss_val: 2.3123 acc_val: 0.1516 time: 0.3937s\n",
      "Epoch: 0922 loss_train: 2.3216 acc_train: 0.1428 loss_val: 2.3125 acc_val: 0.1531 time: 0.3667s\n",
      "Epoch: 0923 loss_train: 2.3184 acc_train: 0.1399 loss_val: 2.3125 acc_val: 0.1531 time: 0.4114s\n",
      "Epoch: 0924 loss_train: 2.3145 acc_train: 0.1369 loss_val: 2.3126 acc_val: 0.1536 time: 0.3695s\n",
      "Epoch: 0925 loss_train: 2.3209 acc_train: 0.1453 loss_val: 2.3126 acc_val: 0.1536 time: 0.3835s\n",
      "Epoch: 0926 loss_train: 2.3208 acc_train: 0.1391 loss_val: 2.3126 acc_val: 0.1536 time: 0.3651s\n",
      "Epoch: 0927 loss_train: 2.3172 acc_train: 0.1469 loss_val: 2.3126 acc_val: 0.1526 time: 0.3666s\n",
      "Epoch: 0928 loss_train: 2.3152 acc_train: 0.1557 loss_val: 2.3126 acc_val: 0.1521 time: 0.3518s\n",
      "Epoch: 0929 loss_train: 2.3214 acc_train: 0.1468 loss_val: 2.3124 acc_val: 0.1521 time: 0.3491s\n",
      "Epoch: 0930 loss_train: 2.3258 acc_train: 0.1441 loss_val: 2.3124 acc_val: 0.1516 time: 0.3481s\n",
      "Epoch: 0931 loss_train: 2.3235 acc_train: 0.1340 loss_val: 2.3124 acc_val: 0.1510 time: 0.3676s\n",
      "Epoch: 0932 loss_train: 2.3198 acc_train: 0.1512 loss_val: 2.3124 acc_val: 0.1510 time: 0.3973s\n",
      "Epoch: 0933 loss_train: 2.3208 acc_train: 0.1419 loss_val: 2.3123 acc_val: 0.1521 time: 0.3636s\n",
      "Epoch: 0934 loss_train: 2.3173 acc_train: 0.1382 loss_val: 2.3122 acc_val: 0.1510 time: 0.3928s\n",
      "Epoch: 0935 loss_train: 2.3199 acc_train: 0.1453 loss_val: 2.3122 acc_val: 0.1510 time: 0.3824s\n",
      "Epoch: 0936 loss_train: 2.3177 acc_train: 0.1494 loss_val: 2.3122 acc_val: 0.1521 time: 0.3610s\n",
      "Epoch: 0937 loss_train: 2.3202 acc_train: 0.1440 loss_val: 2.3122 acc_val: 0.1531 time: 0.3716s\n",
      "Epoch: 0938 loss_train: 2.3130 acc_train: 0.1509 loss_val: 2.3121 acc_val: 0.1536 time: 0.3729s\n",
      "Epoch: 0939 loss_train: 2.3154 acc_train: 0.1495 loss_val: 2.3122 acc_val: 0.1521 time: 0.4001s\n",
      "Epoch: 0940 loss_train: 2.3152 acc_train: 0.1444 loss_val: 2.3123 acc_val: 0.1526 time: 0.3958s\n",
      "Epoch: 0941 loss_train: 2.3205 acc_train: 0.1422 loss_val: 2.3124 acc_val: 0.1536 time: 0.3917s\n",
      "Epoch: 0942 loss_train: 2.3183 acc_train: 0.1375 loss_val: 2.3126 acc_val: 0.1536 time: 0.3919s\n",
      "Epoch: 0943 loss_train: 2.3178 acc_train: 0.1385 loss_val: 2.3128 acc_val: 0.1531 time: 0.3854s\n",
      "Epoch: 0944 loss_train: 2.3190 acc_train: 0.1413 loss_val: 2.3129 acc_val: 0.1516 time: 0.3725s\n",
      "Epoch: 0945 loss_train: 2.3232 acc_train: 0.1466 loss_val: 2.3128 acc_val: 0.1510 time: 0.3411s\n",
      "Epoch: 0946 loss_train: 2.3177 acc_train: 0.1478 loss_val: 2.3125 acc_val: 0.1510 time: 0.3353s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0947 loss_train: 2.3189 acc_train: 0.1504 loss_val: 2.3123 acc_val: 0.1510 time: 0.3585s\n",
      "Epoch: 0948 loss_train: 2.3160 acc_train: 0.1487 loss_val: 2.3119 acc_val: 0.1521 time: 0.3453s\n",
      "Epoch: 0949 loss_train: 2.3261 acc_train: 0.1387 loss_val: 2.3118 acc_val: 0.1521 time: 0.3511s\n",
      "Epoch: 0950 loss_train: 2.3192 acc_train: 0.1434 loss_val: 2.3117 acc_val: 0.1531 time: 0.3768s\n",
      "Epoch: 0951 loss_train: 2.3184 acc_train: 0.1479 loss_val: 2.3116 acc_val: 0.1526 time: 0.3759s\n",
      "Epoch: 0952 loss_train: 2.3196 acc_train: 0.1385 loss_val: 2.3116 acc_val: 0.1526 time: 0.3923s\n",
      "Epoch: 0953 loss_train: 2.3182 acc_train: 0.1463 loss_val: 2.3115 acc_val: 0.1536 time: 0.3845s\n",
      "Epoch: 0954 loss_train: 2.3235 acc_train: 0.1440 loss_val: 2.3115 acc_val: 0.1521 time: 0.3679s\n",
      "Epoch: 0955 loss_train: 2.3160 acc_train: 0.1550 loss_val: 2.3115 acc_val: 0.1516 time: 0.3772s\n",
      "Epoch: 0956 loss_train: 2.3183 acc_train: 0.1528 loss_val: 2.3116 acc_val: 0.1516 time: 0.3760s\n",
      "Epoch: 0957 loss_train: 2.3184 acc_train: 0.1485 loss_val: 2.3115 acc_val: 0.1505 time: 0.3721s\n",
      "Epoch: 0958 loss_train: 2.3216 acc_train: 0.1421 loss_val: 2.3116 acc_val: 0.1521 time: 0.3418s\n",
      "Epoch: 0959 loss_train: 2.3270 acc_train: 0.1387 loss_val: 2.3117 acc_val: 0.1516 time: 0.3963s\n",
      "Epoch: 0960 loss_train: 2.3212 acc_train: 0.1415 loss_val: 2.3119 acc_val: 0.1510 time: 0.3690s\n",
      "Epoch: 0961 loss_train: 2.3185 acc_train: 0.1444 loss_val: 2.3122 acc_val: 0.1526 time: 0.3894s\n",
      "Epoch: 0962 loss_train: 2.3192 acc_train: 0.1512 loss_val: 2.3122 acc_val: 0.1531 time: 0.3842s\n",
      "Epoch: 0963 loss_train: 2.3169 acc_train: 0.1532 loss_val: 2.3122 acc_val: 0.1536 time: 0.3908s\n",
      "Epoch: 0964 loss_train: 2.3215 acc_train: 0.1457 loss_val: 2.3121 acc_val: 0.1536 time: 0.3825s\n",
      "Epoch: 0965 loss_train: 2.3177 acc_train: 0.1366 loss_val: 2.3120 acc_val: 0.1526 time: 0.3883s\n",
      "Epoch: 0966 loss_train: 2.3243 acc_train: 0.1448 loss_val: 2.3120 acc_val: 0.1531 time: 0.3696s\n",
      "Epoch: 0967 loss_train: 2.3209 acc_train: 0.1457 loss_val: 2.3121 acc_val: 0.1521 time: 0.3652s\n",
      "Epoch: 0968 loss_train: 2.3174 acc_train: 0.1450 loss_val: 2.3122 acc_val: 0.1510 time: 0.3719s\n",
      "Epoch: 0969 loss_train: 2.3196 acc_train: 0.1346 loss_val: 2.3124 acc_val: 0.1531 time: 0.3891s\n",
      "Epoch: 0970 loss_train: 2.3182 acc_train: 0.1377 loss_val: 2.3125 acc_val: 0.1505 time: 0.3784s\n",
      "Epoch: 0971 loss_train: 2.3177 acc_train: 0.1497 loss_val: 2.3126 acc_val: 0.1521 time: 0.3589s\n",
      "Epoch: 0972 loss_train: 2.3153 acc_train: 0.1523 loss_val: 2.3128 acc_val: 0.1516 time: 0.3901s\n",
      "Epoch: 0973 loss_train: 2.3190 acc_train: 0.1450 loss_val: 2.3128 acc_val: 0.1521 time: 0.3509s\n",
      "Epoch: 0974 loss_train: 2.3230 acc_train: 0.1402 loss_val: 2.3128 acc_val: 0.1526 time: 0.3893s\n",
      "Epoch: 0975 loss_train: 2.3231 acc_train: 0.1468 loss_val: 2.3126 acc_val: 0.1521 time: 0.3728s\n",
      "Epoch: 0976 loss_train: 2.3216 acc_train: 0.1393 loss_val: 2.3123 acc_val: 0.1521 time: 0.3455s\n",
      "Epoch: 0977 loss_train: 2.3174 acc_train: 0.1459 loss_val: 2.3118 acc_val: 0.1521 time: 0.3560s\n",
      "Epoch: 0978 loss_train: 2.3210 acc_train: 0.1375 loss_val: 2.3115 acc_val: 0.1510 time: 0.3653s\n",
      "Epoch: 0979 loss_train: 2.3232 acc_train: 0.1451 loss_val: 2.3113 acc_val: 0.1510 time: 0.3632s\n",
      "Epoch: 0980 loss_train: 2.3216 acc_train: 0.1457 loss_val: 2.3111 acc_val: 0.1510 time: 0.3568s\n",
      "Epoch: 0981 loss_train: 2.3188 acc_train: 0.1369 loss_val: 2.3109 acc_val: 0.1510 time: 0.3619s\n",
      "Epoch: 0982 loss_train: 2.3197 acc_train: 0.1516 loss_val: 2.3110 acc_val: 0.1521 time: 0.3641s\n",
      "Epoch: 0983 loss_train: 2.3206 acc_train: 0.1415 loss_val: 2.3113 acc_val: 0.1526 time: 0.3913s\n",
      "Epoch: 0984 loss_train: 2.3201 acc_train: 0.1454 loss_val: 2.3118 acc_val: 0.1526 time: 0.3652s\n",
      "Epoch: 0985 loss_train: 2.3189 acc_train: 0.1416 loss_val: 2.3123 acc_val: 0.1526 time: 0.3921s\n",
      "Epoch: 0986 loss_train: 2.3160 acc_train: 0.1534 loss_val: 2.3128 acc_val: 0.1526 time: 0.3604s\n",
      "Epoch: 0987 loss_train: 2.3145 acc_train: 0.1460 loss_val: 2.3132 acc_val: 0.1521 time: 0.3649s\n",
      "Epoch: 0988 loss_train: 2.3167 acc_train: 0.1491 loss_val: 2.3133 acc_val: 0.1516 time: 0.3823s\n",
      "Epoch: 0989 loss_train: 2.3164 acc_train: 0.1380 loss_val: 2.3134 acc_val: 0.1516 time: 0.3592s\n",
      "Epoch: 0990 loss_train: 2.3213 acc_train: 0.1391 loss_val: 2.3132 acc_val: 0.1521 time: 0.4078s\n",
      "Epoch: 0991 loss_train: 2.3198 acc_train: 0.1490 loss_val: 2.3129 acc_val: 0.1516 time: 0.3898s\n",
      "Epoch: 0992 loss_train: 2.3227 acc_train: 0.1431 loss_val: 2.3124 acc_val: 0.1510 time: 0.3922s\n",
      "Epoch: 0993 loss_train: 2.3176 acc_train: 0.1353 loss_val: 2.3120 acc_val: 0.1510 time: 0.4043s\n",
      "Epoch: 0994 loss_train: 2.3242 acc_train: 0.1466 loss_val: 2.3116 acc_val: 0.1531 time: 0.3512s\n",
      "Epoch: 0995 loss_train: 2.3176 acc_train: 0.1406 loss_val: 2.3113 acc_val: 0.1526 time: 0.4058s\n",
      "Epoch: 0996 loss_train: 2.3175 acc_train: 0.1413 loss_val: 2.3112 acc_val: 0.1526 time: 0.3188s\n",
      "Epoch: 0997 loss_train: 2.3192 acc_train: 0.1410 loss_val: 2.3111 acc_val: 0.1531 time: 0.3352s\n",
      "Epoch: 0998 loss_train: 2.3187 acc_train: 0.1413 loss_val: 2.3112 acc_val: 0.1536 time: 0.4214s\n",
      "Epoch: 0999 loss_train: 2.3176 acc_train: 0.1495 loss_val: 2.3114 acc_val: 0.1526 time: 0.3941s\n",
      "Epoch: 1000 loss_train: 2.3190 acc_train: 0.1468 loss_val: 2.3115 acc_val: 0.1536 time: 0.3806s\n",
      "Epoch: 1001 loss_train: 2.3178 acc_train: 0.1355 loss_val: 2.3115 acc_val: 0.1526 time: 0.3591s\n",
      "Epoch: 1002 loss_train: 2.3224 acc_train: 0.1437 loss_val: 2.3118 acc_val: 0.1516 time: 0.3587s\n",
      "Epoch: 1003 loss_train: 2.3192 acc_train: 0.1494 loss_val: 2.3121 acc_val: 0.1521 time: 0.3570s\n",
      "Epoch: 1004 loss_train: 2.3230 acc_train: 0.1341 loss_val: 2.3122 acc_val: 0.1521 time: 0.3758s\n",
      "Epoch: 1005 loss_train: 2.3160 acc_train: 0.1487 loss_val: 2.3122 acc_val: 0.1510 time: 0.3956s\n",
      "Epoch: 1006 loss_train: 2.3218 acc_train: 0.1377 loss_val: 2.3123 acc_val: 0.1531 time: 0.3951s\n",
      "Epoch: 1007 loss_train: 2.3186 acc_train: 0.1431 loss_val: 2.3123 acc_val: 0.1536 time: 0.3521s\n",
      "Epoch: 1008 loss_train: 2.3190 acc_train: 0.1432 loss_val: 2.3122 acc_val: 0.1536 time: 0.3467s\n",
      "Epoch: 1009 loss_train: 2.3210 acc_train: 0.1462 loss_val: 2.3120 acc_val: 0.1536 time: 0.3428s\n",
      "Epoch: 1010 loss_train: 2.3181 acc_train: 0.1453 loss_val: 2.3118 acc_val: 0.1526 time: 0.3561s\n",
      "Epoch: 1011 loss_train: 2.3171 acc_train: 0.1440 loss_val: 2.3115 acc_val: 0.1521 time: 0.3467s\n",
      "Epoch: 1012 loss_train: 2.3167 acc_train: 0.1519 loss_val: 2.3113 acc_val: 0.1516 time: 0.3653s\n",
      "Epoch: 1013 loss_train: 2.3215 acc_train: 0.1416 loss_val: 2.3112 acc_val: 0.1505 time: 0.3732s\n",
      "Epoch: 1014 loss_train: 2.3204 acc_train: 0.1482 loss_val: 2.3109 acc_val: 0.1516 time: 0.3891s\n",
      "Epoch: 1015 loss_train: 2.3169 acc_train: 0.1447 loss_val: 2.3108 acc_val: 0.1521 time: 0.3486s\n",
      "Epoch: 1016 loss_train: 2.3161 acc_train: 0.1531 loss_val: 2.3110 acc_val: 0.1521 time: 0.3830s\n",
      "Epoch: 1017 loss_train: 2.3170 acc_train: 0.1523 loss_val: 2.3111 acc_val: 0.1526 time: 0.3535s\n",
      "Epoch: 1018 loss_train: 2.3245 acc_train: 0.1393 loss_val: 2.3114 acc_val: 0.1526 time: 0.3785s\n",
      "Epoch: 1019 loss_train: 2.3200 acc_train: 0.1424 loss_val: 2.3117 acc_val: 0.1526 time: 0.3868s\n",
      "Epoch: 1020 loss_train: 2.3197 acc_train: 0.1412 loss_val: 2.3120 acc_val: 0.1526 time: 0.3765s\n",
      "Epoch: 1021 loss_train: 2.3191 acc_train: 0.1482 loss_val: 2.3122 acc_val: 0.1521 time: 0.4067s\n",
      "Epoch: 1022 loss_train: 2.3195 acc_train: 0.1425 loss_val: 2.3124 acc_val: 0.1521 time: 0.3851s\n",
      "Epoch: 1023 loss_train: 2.3176 acc_train: 0.1437 loss_val: 2.3124 acc_val: 0.1521 time: 0.3780s\n",
      "Epoch: 1024 loss_train: 2.3148 acc_train: 0.1534 loss_val: 2.3122 acc_val: 0.1516 time: 0.3976s\n",
      "Epoch: 1025 loss_train: 2.3213 acc_train: 0.1515 loss_val: 2.3120 acc_val: 0.1521 time: 0.4026s\n",
      "Epoch: 1026 loss_train: 2.3139 acc_train: 0.1503 loss_val: 2.3119 acc_val: 0.1521 time: 0.3794s\n",
      "Epoch: 1027 loss_train: 2.3177 acc_train: 0.1504 loss_val: 2.3118 acc_val: 0.1526 time: 0.3623s\n",
      "Epoch: 1028 loss_train: 2.3228 acc_train: 0.1469 loss_val: 2.3119 acc_val: 0.1531 time: 0.3857s\n",
      "Epoch: 1029 loss_train: 2.3171 acc_train: 0.1422 loss_val: 2.3119 acc_val: 0.1521 time: 0.3773s\n",
      "Epoch: 1030 loss_train: 2.3153 acc_train: 0.1400 loss_val: 2.3119 acc_val: 0.1521 time: 0.4001s\n",
      "Epoch: 1031 loss_train: 2.3146 acc_train: 0.1493 loss_val: 2.3118 acc_val: 0.1531 time: 0.3727s\n",
      "Epoch: 1032 loss_train: 2.3187 acc_train: 0.1432 loss_val: 2.3117 acc_val: 0.1521 time: 0.3889s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1033 loss_train: 2.3188 acc_train: 0.1513 loss_val: 2.3117 acc_val: 0.1521 time: 0.3872s\n",
      "Epoch: 1034 loss_train: 2.3184 acc_train: 0.1509 loss_val: 2.3117 acc_val: 0.1521 time: 0.3403s\n",
      "Epoch: 1035 loss_train: 2.3233 acc_train: 0.1484 loss_val: 2.3118 acc_val: 0.1521 time: 0.3785s\n",
      "Epoch: 1036 loss_train: 2.3154 acc_train: 0.1410 loss_val: 2.3120 acc_val: 0.1521 time: 0.3653s\n",
      "Epoch: 1037 loss_train: 2.3206 acc_train: 0.1372 loss_val: 2.3121 acc_val: 0.1521 time: 0.3924s\n",
      "Epoch: 1038 loss_train: 2.3213 acc_train: 0.1363 loss_val: 2.3122 acc_val: 0.1531 time: 0.3631s\n",
      "Epoch: 1039 loss_train: 2.3232 acc_train: 0.1349 loss_val: 2.3123 acc_val: 0.1526 time: 0.3824s\n",
      "Epoch: 1040 loss_train: 2.3239 acc_train: 0.1374 loss_val: 2.3123 acc_val: 0.1521 time: 0.3942s\n",
      "Epoch: 1041 loss_train: 2.3181 acc_train: 0.1468 loss_val: 2.3122 acc_val: 0.1521 time: 0.3777s\n",
      "Epoch: 1042 loss_train: 2.3148 acc_train: 0.1544 loss_val: 2.3120 acc_val: 0.1526 time: 0.3842s\n",
      "Epoch: 1043 loss_train: 2.3187 acc_train: 0.1471 loss_val: 2.3117 acc_val: 0.1516 time: 0.3648s\n",
      "Epoch: 1044 loss_train: 2.3197 acc_train: 0.1454 loss_val: 2.3113 acc_val: 0.1516 time: 0.3667s\n",
      "Epoch: 1045 loss_train: 2.3196 acc_train: 0.1537 loss_val: 2.3111 acc_val: 0.1521 time: 0.3781s\n",
      "Epoch: 1046 loss_train: 2.3189 acc_train: 0.1503 loss_val: 2.3111 acc_val: 0.1521 time: 0.3722s\n",
      "Epoch: 1047 loss_train: 2.3147 acc_train: 0.1403 loss_val: 2.3113 acc_val: 0.1516 time: 0.3785s\n",
      "Epoch: 1048 loss_train: 2.3190 acc_train: 0.1488 loss_val: 2.3114 acc_val: 0.1510 time: 0.3892s\n",
      "Epoch: 1049 loss_train: 2.3192 acc_train: 0.1397 loss_val: 2.3116 acc_val: 0.1521 time: 0.3699s\n",
      "Epoch: 1050 loss_train: 2.3202 acc_train: 0.1419 loss_val: 2.3116 acc_val: 0.1526 time: 0.3510s\n",
      "Epoch: 1051 loss_train: 2.3210 acc_train: 0.1441 loss_val: 2.3116 acc_val: 0.1531 time: 0.3786s\n",
      "Epoch: 1052 loss_train: 2.3185 acc_train: 0.1355 loss_val: 2.3118 acc_val: 0.1531 time: 0.3660s\n",
      "Epoch: 1053 loss_train: 2.3209 acc_train: 0.1335 loss_val: 2.3120 acc_val: 0.1521 time: 0.3844s\n",
      "Epoch: 1054 loss_train: 2.3171 acc_train: 0.1378 loss_val: 2.3123 acc_val: 0.1516 time: 0.3635s\n",
      "Epoch: 1055 loss_train: 2.3210 acc_train: 0.1453 loss_val: 2.3125 acc_val: 0.1516 time: 0.3728s\n",
      "Epoch: 1056 loss_train: 2.3195 acc_train: 0.1469 loss_val: 2.3126 acc_val: 0.1521 time: 0.3912s\n",
      "Epoch: 1057 loss_train: 2.3147 acc_train: 0.1434 loss_val: 2.3125 acc_val: 0.1510 time: 0.3489s\n",
      "Epoch: 1058 loss_train: 2.3217 acc_train: 0.1453 loss_val: 2.3122 acc_val: 0.1510 time: 0.3861s\n",
      "Epoch: 1059 loss_train: 2.3164 acc_train: 0.1462 loss_val: 2.3117 acc_val: 0.1510 time: 0.3693s\n",
      "Epoch: 1060 loss_train: 2.3172 acc_train: 0.1402 loss_val: 2.3112 acc_val: 0.1510 time: 0.3805s\n",
      "Epoch: 1061 loss_train: 2.3182 acc_train: 0.1426 loss_val: 2.3107 acc_val: 0.1510 time: 0.3693s\n",
      "Epoch: 1062 loss_train: 2.3214 acc_train: 0.1460 loss_val: 2.3104 acc_val: 0.1521 time: 0.3924s\n",
      "Epoch: 1063 loss_train: 2.3191 acc_train: 0.1468 loss_val: 2.3104 acc_val: 0.1536 time: 0.3933s\n",
      "Epoch: 1064 loss_train: 2.3157 acc_train: 0.1435 loss_val: 2.3105 acc_val: 0.1531 time: 0.3546s\n",
      "Epoch: 1065 loss_train: 2.3204 acc_train: 0.1365 loss_val: 2.3106 acc_val: 0.1536 time: 0.3982s\n",
      "Epoch: 1066 loss_train: 2.3220 acc_train: 0.1425 loss_val: 2.3108 acc_val: 0.1536 time: 0.4010s\n",
      "Epoch: 1067 loss_train: 2.3243 acc_train: 0.1465 loss_val: 2.3110 acc_val: 0.1526 time: 0.3777s\n",
      "Epoch: 1068 loss_train: 2.3164 acc_train: 0.1412 loss_val: 2.3112 acc_val: 0.1505 time: 0.3799s\n",
      "Epoch: 1069 loss_train: 2.3172 acc_train: 0.1431 loss_val: 2.3115 acc_val: 0.1516 time: 0.3773s\n",
      "Epoch: 1070 loss_train: 2.3145 acc_train: 0.1429 loss_val: 2.3118 acc_val: 0.1521 time: 0.3689s\n",
      "Epoch: 1071 loss_train: 2.3195 acc_train: 0.1409 loss_val: 2.3120 acc_val: 0.1521 time: 0.3662s\n",
      "Epoch: 1072 loss_train: 2.3196 acc_train: 0.1457 loss_val: 2.3120 acc_val: 0.1521 time: 0.2931s\n",
      "Epoch: 1073 loss_train: 2.3215 acc_train: 0.1485 loss_val: 2.3120 acc_val: 0.1510 time: 0.3798s\n",
      "Epoch: 1074 loss_train: 2.3197 acc_train: 0.1440 loss_val: 2.3119 acc_val: 0.1521 time: 0.3815s\n",
      "Epoch: 1075 loss_train: 2.3192 acc_train: 0.1460 loss_val: 2.3118 acc_val: 0.1526 time: 0.3776s\n",
      "Epoch: 1076 loss_train: 2.3243 acc_train: 0.1356 loss_val: 2.3117 acc_val: 0.1526 time: 0.3618s\n",
      "Epoch: 1077 loss_train: 2.3193 acc_train: 0.1362 loss_val: 2.3117 acc_val: 0.1531 time: 0.3520s\n",
      "Epoch: 1078 loss_train: 2.3148 acc_train: 0.1412 loss_val: 2.3116 acc_val: 0.1531 time: 0.3667s\n",
      "Epoch: 1079 loss_train: 2.3179 acc_train: 0.1434 loss_val: 2.3115 acc_val: 0.1526 time: 0.3792s\n",
      "Epoch: 1080 loss_train: 2.3212 acc_train: 0.1421 loss_val: 2.3114 acc_val: 0.1526 time: 0.3446s\n",
      "Epoch: 1081 loss_train: 2.3207 acc_train: 0.1494 loss_val: 2.3113 acc_val: 0.1531 time: 0.3627s\n",
      "Epoch: 1082 loss_train: 2.3193 acc_train: 0.1468 loss_val: 2.3113 acc_val: 0.1516 time: 0.3632s\n",
      "Epoch: 1083 loss_train: 2.3202 acc_train: 0.1478 loss_val: 2.3113 acc_val: 0.1521 time: 0.3383s\n",
      "Epoch: 1084 loss_train: 2.3214 acc_train: 0.1488 loss_val: 2.3113 acc_val: 0.1510 time: 0.3794s\n",
      "Epoch: 1085 loss_train: 2.3178 acc_train: 0.1327 loss_val: 2.3114 acc_val: 0.1510 time: 0.3850s\n",
      "Epoch: 1086 loss_train: 2.3176 acc_train: 0.1440 loss_val: 2.3114 acc_val: 0.1521 time: 0.3749s\n",
      "Epoch: 1087 loss_train: 2.3190 acc_train: 0.1453 loss_val: 2.3113 acc_val: 0.1521 time: 0.3639s\n",
      "Epoch: 1088 loss_train: 2.3154 acc_train: 0.1503 loss_val: 2.3112 acc_val: 0.1531 time: 0.3708s\n",
      "Epoch: 1089 loss_train: 2.3207 acc_train: 0.1434 loss_val: 2.3112 acc_val: 0.1526 time: 0.3794s\n",
      "Epoch: 1090 loss_train: 2.3163 acc_train: 0.1509 loss_val: 2.3111 acc_val: 0.1526 time: 0.3623s\n",
      "Epoch: 1091 loss_train: 2.3189 acc_train: 0.1469 loss_val: 2.3113 acc_val: 0.1536 time: 0.3942s\n",
      "Epoch: 1092 loss_train: 2.3240 acc_train: 0.1375 loss_val: 2.3116 acc_val: 0.1521 time: 0.3860s\n",
      "Epoch: 1093 loss_train: 2.3136 acc_train: 0.1407 loss_val: 2.3120 acc_val: 0.1526 time: 0.3589s\n",
      "Epoch: 1094 loss_train: 2.3182 acc_train: 0.1447 loss_val: 2.3125 acc_val: 0.1526 time: 0.4047s\n",
      "Epoch: 1095 loss_train: 2.3194 acc_train: 0.1487 loss_val: 2.3131 acc_val: 0.1531 time: 0.3741s\n",
      "Epoch: 1096 loss_train: 2.3169 acc_train: 0.1413 loss_val: 2.3134 acc_val: 0.1505 time: 0.3581s\n",
      "Epoch: 1097 loss_train: 2.3185 acc_train: 0.1466 loss_val: 2.3134 acc_val: 0.1521 time: 0.3735s\n",
      "Epoch: 1098 loss_train: 2.3186 acc_train: 0.1473 loss_val: 2.3130 acc_val: 0.1521 time: 0.3692s\n",
      "Epoch: 1099 loss_train: 2.3151 acc_train: 0.1481 loss_val: 2.3124 acc_val: 0.1516 time: 0.3819s\n",
      "Epoch: 1100 loss_train: 2.3204 acc_train: 0.1357 loss_val: 2.3117 acc_val: 0.1521 time: 0.3681s\n",
      "Epoch: 1101 loss_train: 2.3166 acc_train: 0.1406 loss_val: 2.3110 acc_val: 0.1510 time: 0.4003s\n",
      "Epoch: 1102 loss_train: 2.3201 acc_train: 0.1419 loss_val: 2.3105 acc_val: 0.1510 time: 0.3677s\n",
      "Epoch: 1103 loss_train: 2.3180 acc_train: 0.1460 loss_val: 2.3103 acc_val: 0.1510 time: 0.3667s\n",
      "Epoch: 1104 loss_train: 2.3213 acc_train: 0.1435 loss_val: 2.3102 acc_val: 0.1510 time: 0.3854s\n",
      "Epoch: 1105 loss_train: 2.3168 acc_train: 0.1434 loss_val: 2.3103 acc_val: 0.1521 time: 0.3929s\n",
      "Epoch: 1106 loss_train: 2.3157 acc_train: 0.1531 loss_val: 2.3107 acc_val: 0.1536 time: 0.3677s\n",
      "Epoch: 1107 loss_train: 2.3233 acc_train: 0.1311 loss_val: 2.3113 acc_val: 0.1536 time: 0.3751s\n",
      "Epoch: 1108 loss_train: 2.3168 acc_train: 0.1485 loss_val: 2.3117 acc_val: 0.1531 time: 0.3904s\n",
      "Epoch: 1109 loss_train: 2.3203 acc_train: 0.1441 loss_val: 2.3123 acc_val: 0.1516 time: 0.3902s\n",
      "Epoch: 1110 loss_train: 2.3213 acc_train: 0.1403 loss_val: 2.3127 acc_val: 0.1526 time: 0.4007s\n",
      "Epoch: 1111 loss_train: 2.3172 acc_train: 0.1475 loss_val: 2.3128 acc_val: 0.1516 time: 0.3972s\n",
      "Epoch: 1112 loss_train: 2.3192 acc_train: 0.1391 loss_val: 2.3128 acc_val: 0.1521 time: 0.3647s\n",
      "Epoch: 1113 loss_train: 2.3212 acc_train: 0.1460 loss_val: 2.3127 acc_val: 0.1516 time: 0.3969s\n",
      "Epoch: 1114 loss_train: 2.3140 acc_train: 0.1394 loss_val: 2.3125 acc_val: 0.1551 time: 0.3595s\n",
      "Epoch: 1115 loss_train: 2.3224 acc_train: 0.1484 loss_val: 2.3123 acc_val: 0.1531 time: 0.3715s\n",
      "Epoch: 1116 loss_train: 2.3207 acc_train: 0.1438 loss_val: 2.3119 acc_val: 0.1536 time: 0.3885s\n",
      "Epoch: 1117 loss_train: 2.3219 acc_train: 0.1409 loss_val: 2.3114 acc_val: 0.1536 time: 0.3715s\n",
      "Epoch: 1118 loss_train: 2.3145 acc_train: 0.1446 loss_val: 2.3109 acc_val: 0.1526 time: 0.3574s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1119 loss_train: 2.3190 acc_train: 0.1400 loss_val: 2.3105 acc_val: 0.1541 time: 0.3478s\n",
      "Epoch: 1120 loss_train: 2.3222 acc_train: 0.1393 loss_val: 2.3102 acc_val: 0.1505 time: 0.3587s\n",
      "Epoch: 1121 loss_train: 2.3187 acc_train: 0.1428 loss_val: 2.3101 acc_val: 0.1526 time: 0.4085s\n",
      "Epoch: 1122 loss_train: 2.3206 acc_train: 0.1360 loss_val: 2.3103 acc_val: 0.1526 time: 0.3884s\n",
      "Epoch: 1123 loss_train: 2.3127 acc_train: 0.1515 loss_val: 2.3105 acc_val: 0.1516 time: 0.3869s\n",
      "Epoch: 1124 loss_train: 2.3234 acc_train: 0.1473 loss_val: 2.3108 acc_val: 0.1521 time: 0.3481s\n",
      "Epoch: 1125 loss_train: 2.3221 acc_train: 0.1440 loss_val: 2.3111 acc_val: 0.1510 time: 0.3963s\n",
      "Epoch: 1126 loss_train: 2.3159 acc_train: 0.1407 loss_val: 2.3114 acc_val: 0.1521 time: 0.3692s\n",
      "Epoch: 1127 loss_train: 2.3226 acc_train: 0.1451 loss_val: 2.3114 acc_val: 0.1510 time: 0.3652s\n",
      "Epoch: 1128 loss_train: 2.3187 acc_train: 0.1418 loss_val: 2.3113 acc_val: 0.1521 time: 0.4193s\n",
      "Epoch: 1129 loss_train: 2.3166 acc_train: 0.1495 loss_val: 2.3111 acc_val: 0.1526 time: 0.3982s\n",
      "Epoch: 1130 loss_train: 2.3160 acc_train: 0.1529 loss_val: 2.3111 acc_val: 0.1516 time: 0.3749s\n",
      "Epoch: 1131 loss_train: 2.3170 acc_train: 0.1393 loss_val: 2.3112 acc_val: 0.1521 time: 0.3744s\n",
      "Epoch: 1132 loss_train: 2.3202 acc_train: 0.1485 loss_val: 2.3114 acc_val: 0.1526 time: 0.3677s\n",
      "Epoch: 1133 loss_train: 2.3217 acc_train: 0.1468 loss_val: 2.3117 acc_val: 0.1531 time: 0.3820s\n",
      "Epoch: 1134 loss_train: 2.3150 acc_train: 0.1535 loss_val: 2.3120 acc_val: 0.1536 time: 0.3923s\n",
      "Epoch: 1135 loss_train: 2.3209 acc_train: 0.1371 loss_val: 2.3124 acc_val: 0.1526 time: 0.3439s\n",
      "Epoch: 1136 loss_train: 2.3188 acc_train: 0.1465 loss_val: 2.3128 acc_val: 0.1531 time: 0.3830s\n",
      "Epoch: 1137 loss_train: 2.3167 acc_train: 0.1384 loss_val: 2.3130 acc_val: 0.1516 time: 0.3829s\n",
      "Epoch: 1138 loss_train: 2.3213 acc_train: 0.1369 loss_val: 2.3130 acc_val: 0.1521 time: 0.3599s\n",
      "Epoch: 1139 loss_train: 2.3245 acc_train: 0.1350 loss_val: 2.3130 acc_val: 0.1521 time: 0.3710s\n",
      "Epoch: 1140 loss_train: 2.3153 acc_train: 0.1432 loss_val: 2.3127 acc_val: 0.1531 time: 0.3337s\n",
      "Epoch: 1141 loss_train: 2.3204 acc_train: 0.1396 loss_val: 2.3122 acc_val: 0.1521 time: 0.4003s\n",
      "Epoch: 1142 loss_train: 2.3206 acc_train: 0.1471 loss_val: 2.3117 acc_val: 0.1526 time: 0.3698s\n",
      "Epoch: 1143 loss_train: 2.3153 acc_train: 0.1460 loss_val: 2.3115 acc_val: 0.1526 time: 0.3864s\n",
      "Epoch: 1144 loss_train: 2.3179 acc_train: 0.1513 loss_val: 2.3114 acc_val: 0.1526 time: 0.4023s\n",
      "Epoch: 1145 loss_train: 2.3190 acc_train: 0.1490 loss_val: 2.3113 acc_val: 0.1521 time: 0.3870s\n",
      "Epoch: 1146 loss_train: 2.3210 acc_train: 0.1425 loss_val: 2.3112 acc_val: 0.1521 time: 0.4122s\n",
      "Epoch: 1147 loss_train: 2.3166 acc_train: 0.1382 loss_val: 2.3112 acc_val: 0.1526 time: 0.3946s\n",
      "Epoch: 1148 loss_train: 2.3190 acc_train: 0.1444 loss_val: 2.3112 acc_val: 0.1521 time: 0.3823s\n",
      "Epoch: 1149 loss_train: 2.3209 acc_train: 0.1431 loss_val: 2.3113 acc_val: 0.1516 time: 0.3671s\n",
      "Epoch: 1150 loss_train: 2.3159 acc_train: 0.1406 loss_val: 2.3115 acc_val: 0.1521 time: 0.4131s\n",
      "Epoch: 1151 loss_train: 2.3211 acc_train: 0.1418 loss_val: 2.3117 acc_val: 0.1521 time: 0.3813s\n",
      "Epoch: 1152 loss_train: 2.3176 acc_train: 0.1456 loss_val: 2.3119 acc_val: 0.1536 time: 0.3962s\n",
      "Epoch: 1153 loss_train: 2.3187 acc_train: 0.1473 loss_val: 2.3121 acc_val: 0.1536 time: 0.3667s\n",
      "Epoch: 1154 loss_train: 2.3191 acc_train: 0.1504 loss_val: 2.3121 acc_val: 0.1526 time: 0.3741s\n",
      "Epoch: 1155 loss_train: 2.3224 acc_train: 0.1432 loss_val: 2.3121 acc_val: 0.1536 time: 0.3714s\n",
      "Epoch: 1156 loss_train: 2.3149 acc_train: 0.1457 loss_val: 2.3120 acc_val: 0.1531 time: 0.3949s\n",
      "Epoch: 1157 loss_train: 2.3172 acc_train: 0.1523 loss_val: 2.3119 acc_val: 0.1531 time: 0.3753s\n",
      "Epoch: 1158 loss_train: 2.3158 acc_train: 0.1500 loss_val: 2.3117 acc_val: 0.1521 time: 0.3724s\n",
      "Epoch: 1159 loss_train: 2.3175 acc_train: 0.1429 loss_val: 2.3115 acc_val: 0.1505 time: 0.3878s\n",
      "Epoch: 1160 loss_train: 2.3143 acc_train: 0.1466 loss_val: 2.3113 acc_val: 0.1505 time: 0.3661s\n",
      "Epoch: 1161 loss_train: 2.3147 acc_train: 0.1503 loss_val: 2.3112 acc_val: 0.1505 time: 0.3640s\n",
      "Epoch: 1162 loss_train: 2.3223 acc_train: 0.1517 loss_val: 2.3113 acc_val: 0.1521 time: 0.3712s\n",
      "Epoch: 1163 loss_train: 2.3190 acc_train: 0.1526 loss_val: 2.3113 acc_val: 0.1531 time: 0.3480s\n",
      "Epoch: 1164 loss_train: 2.3218 acc_train: 0.1493 loss_val: 2.3115 acc_val: 0.1531 time: 0.3702s\n",
      "Epoch: 1165 loss_train: 2.3156 acc_train: 0.1454 loss_val: 2.3115 acc_val: 0.1521 time: 0.3703s\n",
      "Epoch: 1166 loss_train: 2.3177 acc_train: 0.1488 loss_val: 2.3117 acc_val: 0.1526 time: 0.3796s\n",
      "Epoch: 1167 loss_train: 2.3249 acc_train: 0.1429 loss_val: 2.3119 acc_val: 0.1531 time: 0.3755s\n",
      "Epoch: 1168 loss_train: 2.3114 acc_train: 0.1468 loss_val: 2.3120 acc_val: 0.1521 time: 0.3619s\n",
      "Epoch: 1169 loss_train: 2.3243 acc_train: 0.1512 loss_val: 2.3118 acc_val: 0.1521 time: 0.3672s\n",
      "Epoch: 1170 loss_train: 2.3168 acc_train: 0.1440 loss_val: 2.3118 acc_val: 0.1521 time: 0.3764s\n",
      "Epoch: 1171 loss_train: 2.3216 acc_train: 0.1429 loss_val: 2.3117 acc_val: 0.1516 time: 0.3629s\n",
      "Epoch: 1172 loss_train: 2.3182 acc_train: 0.1475 loss_val: 2.3115 acc_val: 0.1521 time: 0.3865s\n",
      "Epoch: 1173 loss_train: 2.3227 acc_train: 0.1346 loss_val: 2.3112 acc_val: 0.1516 time: 0.3572s\n",
      "Epoch: 1174 loss_train: 2.3220 acc_train: 0.1482 loss_val: 2.3109 acc_val: 0.1510 time: 0.3579s\n",
      "Epoch: 1175 loss_train: 2.3179 acc_train: 0.1437 loss_val: 2.3106 acc_val: 0.1521 time: 0.3648s\n",
      "Epoch: 1176 loss_train: 2.3182 acc_train: 0.1468 loss_val: 2.3104 acc_val: 0.1521 time: 0.3885s\n",
      "Epoch: 1177 loss_train: 2.3192 acc_train: 0.1441 loss_val: 2.3104 acc_val: 0.1510 time: 0.3870s\n",
      "Epoch: 1178 loss_train: 2.3206 acc_train: 0.1446 loss_val: 2.3106 acc_val: 0.1510 time: 0.3418s\n",
      "Epoch: 1179 loss_train: 2.3188 acc_train: 0.1432 loss_val: 2.3108 acc_val: 0.1521 time: 0.3921s\n",
      "Epoch: 1180 loss_train: 2.3190 acc_train: 0.1506 loss_val: 2.3110 acc_val: 0.1531 time: 0.3851s\n",
      "Epoch: 1181 loss_train: 2.3165 acc_train: 0.1357 loss_val: 2.3111 acc_val: 0.1531 time: 0.4044s\n",
      "Epoch: 1182 loss_train: 2.3192 acc_train: 0.1375 loss_val: 2.3113 acc_val: 0.1531 time: 0.3983s\n",
      "Epoch: 1183 loss_train: 2.3122 acc_train: 0.1479 loss_val: 2.3115 acc_val: 0.1531 time: 0.3598s\n",
      "Epoch: 1184 loss_train: 2.3115 acc_train: 0.1447 loss_val: 2.3115 acc_val: 0.1526 time: 0.3844s\n",
      "Epoch: 1185 loss_train: 2.3210 acc_train: 0.1462 loss_val: 2.3115 acc_val: 0.1505 time: 0.4084s\n",
      "Epoch: 1186 loss_train: 2.3203 acc_train: 0.1523 loss_val: 2.3115 acc_val: 0.1516 time: 0.3450s\n",
      "Epoch: 1187 loss_train: 2.3200 acc_train: 0.1390 loss_val: 2.3116 acc_val: 0.1521 time: 0.3613s\n",
      "Epoch: 1188 loss_train: 2.3176 acc_train: 0.1403 loss_val: 2.3119 acc_val: 0.1521 time: 0.3918s\n",
      "Epoch: 1189 loss_train: 2.3194 acc_train: 0.1387 loss_val: 2.3119 acc_val: 0.1510 time: 0.3699s\n",
      "Epoch: 1190 loss_train: 2.3224 acc_train: 0.1472 loss_val: 2.3118 acc_val: 0.1521 time: 0.4033s\n",
      "Epoch: 1191 loss_train: 2.3149 acc_train: 0.1425 loss_val: 2.3115 acc_val: 0.1521 time: 0.3532s\n",
      "Epoch: 1192 loss_train: 2.3154 acc_train: 0.1412 loss_val: 2.3113 acc_val: 0.1521 time: 0.3433s\n",
      "Epoch: 1193 loss_train: 2.3217 acc_train: 0.1400 loss_val: 2.3111 acc_val: 0.1521 time: 0.3388s\n",
      "Epoch: 1194 loss_train: 2.3218 acc_train: 0.1428 loss_val: 2.3110 acc_val: 0.1521 time: 0.3885s\n",
      "Epoch: 1195 loss_train: 2.3201 acc_train: 0.1539 loss_val: 2.3109 acc_val: 0.1536 time: 0.3835s\n",
      "Epoch: 1196 loss_train: 2.3232 acc_train: 0.1475 loss_val: 2.3110 acc_val: 0.1531 time: 0.3607s\n",
      "Epoch: 1197 loss_train: 2.3217 acc_train: 0.1350 loss_val: 2.3111 acc_val: 0.1526 time: 0.3577s\n",
      "Epoch: 1198 loss_train: 2.3134 acc_train: 0.1446 loss_val: 2.3113 acc_val: 0.1521 time: 0.4056s\n",
      "Epoch: 1199 loss_train: 2.3198 acc_train: 0.1481 loss_val: 2.3115 acc_val: 0.1521 time: 0.3562s\n",
      "Epoch: 1200 loss_train: 2.3207 acc_train: 0.1404 loss_val: 2.3117 acc_val: 0.1521 time: 0.3705s\n",
      "Epoch: 1201 loss_train: 2.3218 acc_train: 0.1447 loss_val: 2.3117 acc_val: 0.1516 time: 0.3669s\n",
      "Epoch: 1202 loss_train: 2.3255 acc_train: 0.1440 loss_val: 2.3115 acc_val: 0.1516 time: 0.3845s\n",
      "Epoch: 1203 loss_train: 2.3167 acc_train: 0.1485 loss_val: 2.3111 acc_val: 0.1510 time: 0.3921s\n",
      "Epoch: 1204 loss_train: 2.3137 acc_train: 0.1575 loss_val: 2.3107 acc_val: 0.1510 time: 0.4079s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1205 loss_train: 2.3177 acc_train: 0.1431 loss_val: 2.3104 acc_val: 0.1526 time: 0.3940s\n",
      "Epoch: 1206 loss_train: 2.3186 acc_train: 0.1431 loss_val: 2.3103 acc_val: 0.1526 time: 0.3628s\n",
      "Epoch: 1207 loss_train: 2.3181 acc_train: 0.1479 loss_val: 2.3103 acc_val: 0.1526 time: 0.3677s\n",
      "Epoch: 1208 loss_train: 2.3179 acc_train: 0.1488 loss_val: 2.3104 acc_val: 0.1521 time: 0.3690s\n",
      "Epoch: 1209 loss_train: 2.3210 acc_train: 0.1475 loss_val: 2.3106 acc_val: 0.1521 time: 0.3469s\n",
      "Epoch: 1210 loss_train: 2.3187 acc_train: 0.1522 loss_val: 2.3109 acc_val: 0.1516 time: 0.3835s\n",
      "Epoch: 1211 loss_train: 2.3202 acc_train: 0.1412 loss_val: 2.3114 acc_val: 0.1521 time: 0.3643s\n",
      "Epoch: 1212 loss_train: 2.3197 acc_train: 0.1393 loss_val: 2.3117 acc_val: 0.1521 time: 0.3680s\n",
      "Epoch: 1213 loss_train: 2.3217 acc_train: 0.1381 loss_val: 2.3119 acc_val: 0.1526 time: 0.3778s\n",
      "Epoch: 1214 loss_train: 2.3222 acc_train: 0.1410 loss_val: 2.3118 acc_val: 0.1521 time: 0.3465s\n",
      "Epoch: 1215 loss_train: 2.3203 acc_train: 0.1406 loss_val: 2.3115 acc_val: 0.1516 time: 0.3943s\n",
      "Epoch: 1216 loss_train: 2.3209 acc_train: 0.1447 loss_val: 2.3112 acc_val: 0.1531 time: 0.3673s\n",
      "Epoch: 1217 loss_train: 2.3230 acc_train: 0.1457 loss_val: 2.3109 acc_val: 0.1536 time: 0.3971s\n",
      "Epoch: 1218 loss_train: 2.3177 acc_train: 0.1448 loss_val: 2.3107 acc_val: 0.1536 time: 0.3803s\n",
      "Epoch: 1219 loss_train: 2.3198 acc_train: 0.1447 loss_val: 2.3106 acc_val: 0.1531 time: 0.3848s\n",
      "Epoch: 1220 loss_train: 2.3172 acc_train: 0.1434 loss_val: 2.3108 acc_val: 0.1531 time: 0.3976s\n",
      "Epoch: 1221 loss_train: 2.3200 acc_train: 0.1476 loss_val: 2.3111 acc_val: 0.1521 time: 0.4009s\n",
      "Epoch: 1222 loss_train: 2.3174 acc_train: 0.1448 loss_val: 2.3114 acc_val: 0.1521 time: 0.4099s\n",
      "Epoch: 1223 loss_train: 2.3175 acc_train: 0.1482 loss_val: 2.3116 acc_val: 0.1521 time: 0.3542s\n",
      "Epoch: 1224 loss_train: 2.3199 acc_train: 0.1382 loss_val: 2.3118 acc_val: 0.1516 time: 0.3292s\n",
      "Epoch: 1225 loss_train: 2.3200 acc_train: 0.1472 loss_val: 2.3119 acc_val: 0.1521 time: 0.3634s\n",
      "Epoch: 1226 loss_train: 2.3247 acc_train: 0.1397 loss_val: 2.3119 acc_val: 0.1521 time: 0.3911s\n",
      "Epoch: 1227 loss_train: 2.3163 acc_train: 0.1520 loss_val: 2.3116 acc_val: 0.1510 time: 0.3901s\n",
      "Epoch: 1228 loss_train: 2.3222 acc_train: 0.1466 loss_val: 2.3115 acc_val: 0.1505 time: 0.3810s\n",
      "Epoch: 1229 loss_train: 2.3164 acc_train: 0.1447 loss_val: 2.3113 acc_val: 0.1516 time: 0.3843s\n",
      "Epoch: 1230 loss_train: 2.3206 acc_train: 0.1374 loss_val: 2.3111 acc_val: 0.1510 time: 0.3945s\n",
      "Epoch: 1231 loss_train: 2.3179 acc_train: 0.1391 loss_val: 2.3111 acc_val: 0.1526 time: 0.3584s\n",
      "Epoch: 1232 loss_train: 2.3197 acc_train: 0.1454 loss_val: 2.3113 acc_val: 0.1500 time: 0.4029s\n",
      "Epoch: 1233 loss_train: 2.3160 acc_train: 0.1485 loss_val: 2.3115 acc_val: 0.1500 time: 0.3501s\n",
      "Epoch: 1234 loss_train: 2.3181 acc_train: 0.1490 loss_val: 2.3118 acc_val: 0.1531 time: 0.3485s\n",
      "Epoch: 1235 loss_train: 2.3225 acc_train: 0.1471 loss_val: 2.3120 acc_val: 0.1531 time: 0.3518s\n",
      "Epoch: 1236 loss_train: 2.3159 acc_train: 0.1476 loss_val: 2.3123 acc_val: 0.1521 time: 0.3597s\n",
      "Epoch: 1237 loss_train: 2.3212 acc_train: 0.1491 loss_val: 2.3126 acc_val: 0.1505 time: 0.3950s\n",
      "Epoch: 1238 loss_train: 2.3191 acc_train: 0.1485 loss_val: 2.3128 acc_val: 0.1495 time: 0.3680s\n",
      "Epoch: 1239 loss_train: 2.3139 acc_train: 0.1516 loss_val: 2.3129 acc_val: 0.1505 time: 0.3545s\n",
      "Epoch: 1240 loss_train: 2.3181 acc_train: 0.1516 loss_val: 2.3126 acc_val: 0.1510 time: 0.3533s\n",
      "Epoch: 1241 loss_train: 2.3138 acc_train: 0.1391 loss_val: 2.3122 acc_val: 0.1516 time: 0.3697s\n",
      "Epoch: 1242 loss_train: 2.3211 acc_train: 0.1431 loss_val: 2.3117 acc_val: 0.1521 time: 0.3955s\n",
      "Epoch: 1243 loss_train: 2.3193 acc_train: 0.1443 loss_val: 2.3110 acc_val: 0.1521 time: 0.3739s\n",
      "Epoch: 1244 loss_train: 2.3169 acc_train: 0.1406 loss_val: 2.3107 acc_val: 0.1531 time: 0.3922s\n",
      "Epoch: 1245 loss_train: 2.3205 acc_train: 0.1503 loss_val: 2.3104 acc_val: 0.1526 time: 0.3902s\n",
      "Epoch: 1246 loss_train: 2.3219 acc_train: 0.1360 loss_val: 2.3103 acc_val: 0.1531 time: 0.3573s\n",
      "Epoch: 1247 loss_train: 2.3253 acc_train: 0.1497 loss_val: 2.3105 acc_val: 0.1526 time: 0.3683s\n",
      "Epoch: 1248 loss_train: 2.3192 acc_train: 0.1478 loss_val: 2.3108 acc_val: 0.1526 time: 0.3663s\n",
      "Epoch: 1249 loss_train: 2.3201 acc_train: 0.1454 loss_val: 2.3112 acc_val: 0.1521 time: 0.3608s\n",
      "Epoch: 1250 loss_train: 2.3198 acc_train: 0.1435 loss_val: 2.3118 acc_val: 0.1521 time: 0.3733s\n",
      "Epoch: 1251 loss_train: 2.3177 acc_train: 0.1390 loss_val: 2.3123 acc_val: 0.1521 time: 0.3943s\n",
      "Epoch: 1252 loss_train: 2.3210 acc_train: 0.1434 loss_val: 2.3124 acc_val: 0.1521 time: 0.3629s\n",
      "Epoch: 1253 loss_train: 2.3153 acc_train: 0.1469 loss_val: 2.3124 acc_val: 0.1516 time: 0.3732s\n",
      "Epoch: 1254 loss_train: 2.3161 acc_train: 0.1429 loss_val: 2.3123 acc_val: 0.1516 time: 0.3606s\n",
      "Epoch: 1255 loss_train: 2.3176 acc_train: 0.1438 loss_val: 2.3120 acc_val: 0.1516 time: 0.3724s\n",
      "Epoch: 1256 loss_train: 2.3168 acc_train: 0.1441 loss_val: 2.3118 acc_val: 0.1536 time: 0.3673s\n",
      "Epoch: 1257 loss_train: 2.3226 acc_train: 0.1296 loss_val: 2.3115 acc_val: 0.1536 time: 0.3918s\n",
      "Epoch: 1258 loss_train: 2.3193 acc_train: 0.1388 loss_val: 2.3114 acc_val: 0.1536 time: 0.3954s\n",
      "Epoch: 1259 loss_train: 2.3206 acc_train: 0.1404 loss_val: 2.3115 acc_val: 0.1536 time: 0.3704s\n",
      "Epoch: 1260 loss_train: 2.3183 acc_train: 0.1374 loss_val: 2.3114 acc_val: 0.1536 time: 0.3595s\n",
      "Epoch: 1261 loss_train: 2.3210 acc_train: 0.1450 loss_val: 2.3114 acc_val: 0.1536 time: 0.3953s\n",
      "Epoch: 1262 loss_train: 2.3196 acc_train: 0.1444 loss_val: 2.3114 acc_val: 0.1521 time: 0.3586s\n",
      "Epoch: 1263 loss_train: 2.3199 acc_train: 0.1456 loss_val: 2.3113 acc_val: 0.1521 time: 0.3530s\n",
      "Epoch: 1264 loss_train: 2.3149 acc_train: 0.1516 loss_val: 2.3111 acc_val: 0.1516 time: 0.4193s\n",
      "Epoch: 1265 loss_train: 2.3218 acc_train: 0.1460 loss_val: 2.3111 acc_val: 0.1510 time: 0.3658s\n",
      "Epoch: 1266 loss_train: 2.3220 acc_train: 0.1444 loss_val: 2.3113 acc_val: 0.1510 time: 0.3799s\n",
      "Epoch: 1267 loss_train: 2.3192 acc_train: 0.1487 loss_val: 2.3116 acc_val: 0.1526 time: 0.3583s\n",
      "Epoch: 1268 loss_train: 2.3195 acc_train: 0.1457 loss_val: 2.3119 acc_val: 0.1531 time: 0.3622s\n",
      "Epoch: 1269 loss_train: 2.3216 acc_train: 0.1456 loss_val: 2.3121 acc_val: 0.1536 time: 0.3665s\n",
      "Epoch: 1270 loss_train: 2.3199 acc_train: 0.1447 loss_val: 2.3122 acc_val: 0.1536 time: 0.3753s\n",
      "Epoch: 1271 loss_train: 2.3191 acc_train: 0.1500 loss_val: 2.3123 acc_val: 0.1526 time: 0.3855s\n",
      "Epoch: 1272 loss_train: 2.3224 acc_train: 0.1388 loss_val: 2.3122 acc_val: 0.1516 time: 0.3819s\n",
      "Epoch: 1273 loss_train: 2.3135 acc_train: 0.1446 loss_val: 2.3120 acc_val: 0.1521 time: 0.3934s\n",
      "Epoch: 1274 loss_train: 2.3215 acc_train: 0.1485 loss_val: 2.3117 acc_val: 0.1516 time: 0.3892s\n",
      "Epoch: 1275 loss_train: 2.3205 acc_train: 0.1501 loss_val: 2.3114 acc_val: 0.1510 time: 0.3551s\n",
      "Epoch: 1276 loss_train: 2.3217 acc_train: 0.1374 loss_val: 2.3114 acc_val: 0.1521 time: 0.3706s\n",
      "Epoch: 1277 loss_train: 2.3191 acc_train: 0.1409 loss_val: 2.3114 acc_val: 0.1521 time: 0.3605s\n",
      "Epoch: 1278 loss_train: 2.3217 acc_train: 0.1451 loss_val: 2.3112 acc_val: 0.1521 time: 0.3790s\n",
      "Epoch: 1279 loss_train: 2.3193 acc_train: 0.1484 loss_val: 2.3111 acc_val: 0.1510 time: 0.3592s\n",
      "Epoch: 1280 loss_train: 2.3203 acc_train: 0.1410 loss_val: 2.3110 acc_val: 0.1510 time: 0.3690s\n",
      "Epoch: 1281 loss_train: 2.3193 acc_train: 0.1497 loss_val: 2.3108 acc_val: 0.1516 time: 0.3949s\n",
      "Epoch: 1282 loss_train: 2.3162 acc_train: 0.1559 loss_val: 2.3107 acc_val: 0.1521 time: 0.3511s\n",
      "Epoch: 1283 loss_train: 2.3212 acc_train: 0.1412 loss_val: 2.3106 acc_val: 0.1521 time: 0.4311s\n",
      "Epoch: 1284 loss_train: 2.3189 acc_train: 0.1481 loss_val: 2.3106 acc_val: 0.1526 time: 0.3564s\n",
      "Epoch: 1285 loss_train: 2.3236 acc_train: 0.1450 loss_val: 2.3108 acc_val: 0.1516 time: 0.3832s\n",
      "Epoch: 1286 loss_train: 2.3206 acc_train: 0.1362 loss_val: 2.3112 acc_val: 0.1521 time: 0.3629s\n",
      "Epoch: 1287 loss_train: 2.3192 acc_train: 0.1424 loss_val: 2.3115 acc_val: 0.1531 time: 0.3780s\n",
      "Epoch: 1288 loss_train: 2.3196 acc_train: 0.1406 loss_val: 2.3118 acc_val: 0.1526 time: 0.3937s\n",
      "Epoch: 1289 loss_train: 2.3212 acc_train: 0.1432 loss_val: 2.3123 acc_val: 0.1526 time: 0.3988s\n",
      "Epoch: 1290 loss_train: 2.3194 acc_train: 0.1481 loss_val: 2.3126 acc_val: 0.1536 time: 0.3672s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1291 loss_train: 2.3229 acc_train: 0.1322 loss_val: 2.3127 acc_val: 0.1505 time: 0.3639s\n",
      "Epoch: 1292 loss_train: 2.3193 acc_train: 0.1422 loss_val: 2.3127 acc_val: 0.1526 time: 0.3651s\n",
      "Epoch: 1293 loss_train: 2.3202 acc_train: 0.1539 loss_val: 2.3124 acc_val: 0.1505 time: 0.3677s\n",
      "Epoch: 1294 loss_train: 2.3170 acc_train: 0.1381 loss_val: 2.3120 acc_val: 0.1495 time: 0.3536s\n",
      "Epoch: 1295 loss_train: 2.3158 acc_train: 0.1459 loss_val: 2.3116 acc_val: 0.1521 time: 0.3924s\n",
      "Epoch: 1296 loss_train: 2.3213 acc_train: 0.1410 loss_val: 2.3112 acc_val: 0.1521 time: 0.3716s\n",
      "Epoch: 1297 loss_train: 2.3210 acc_train: 0.1422 loss_val: 2.3112 acc_val: 0.1526 time: 0.3860s\n",
      "Epoch: 1298 loss_train: 2.3192 acc_train: 0.1509 loss_val: 2.3114 acc_val: 0.1510 time: 0.4161s\n",
      "Epoch: 1299 loss_train: 2.3198 acc_train: 0.1391 loss_val: 2.3118 acc_val: 0.1536 time: 0.3780s\n",
      "Epoch: 1300 loss_train: 2.3216 acc_train: 0.1506 loss_val: 2.3122 acc_val: 0.1526 time: 0.3411s\n",
      "Epoch: 1301 loss_train: 2.3152 acc_train: 0.1412 loss_val: 2.3127 acc_val: 0.1500 time: 0.3729s\n",
      "Epoch: 1302 loss_train: 2.3160 acc_train: 0.1478 loss_val: 2.3127 acc_val: 0.1531 time: 0.3432s\n",
      "Epoch: 1303 loss_train: 2.3191 acc_train: 0.1388 loss_val: 2.3126 acc_val: 0.1526 time: 0.3727s\n",
      "Epoch: 1304 loss_train: 2.3211 acc_train: 0.1441 loss_val: 2.3122 acc_val: 0.1490 time: 0.4005s\n",
      "Epoch: 1305 loss_train: 2.3160 acc_train: 0.1375 loss_val: 2.3117 acc_val: 0.1485 time: 0.3914s\n",
      "Epoch: 1306 loss_train: 2.3168 acc_train: 0.1402 loss_val: 2.3111 acc_val: 0.1510 time: 0.3827s\n",
      "Epoch: 1307 loss_train: 2.3161 acc_train: 0.1478 loss_val: 2.3106 acc_val: 0.1521 time: 0.3792s\n",
      "Epoch: 1308 loss_train: 2.3151 acc_train: 0.1447 loss_val: 2.3103 acc_val: 0.1531 time: 0.3828s\n",
      "Epoch: 1309 loss_train: 2.3172 acc_train: 0.1397 loss_val: 2.3100 acc_val: 0.1526 time: 0.3647s\n",
      "Epoch: 1310 loss_train: 2.3218 acc_train: 0.1388 loss_val: 2.3101 acc_val: 0.1510 time: 0.3749s\n",
      "Epoch: 1311 loss_train: 2.3153 acc_train: 0.1424 loss_val: 2.3104 acc_val: 0.1510 time: 0.3600s\n",
      "Epoch: 1312 loss_train: 2.3208 acc_train: 0.1424 loss_val: 2.3109 acc_val: 0.1526 time: 0.4004s\n",
      "Epoch: 1313 loss_train: 2.3177 acc_train: 0.1554 loss_val: 2.3115 acc_val: 0.1521 time: 0.3637s\n",
      "Epoch: 1314 loss_train: 2.3186 acc_train: 0.1469 loss_val: 2.3120 acc_val: 0.1526 time: 0.3786s\n",
      "Epoch: 1315 loss_train: 2.3173 acc_train: 0.1493 loss_val: 2.3125 acc_val: 0.1521 time: 0.3717s\n",
      "Epoch: 1316 loss_train: 2.3161 acc_train: 0.1454 loss_val: 2.3130 acc_val: 0.1505 time: 0.3900s\n",
      "Epoch: 1317 loss_train: 2.3173 acc_train: 0.1487 loss_val: 2.3131 acc_val: 0.1516 time: 0.3724s\n",
      "Epoch: 1318 loss_train: 2.3229 acc_train: 0.1421 loss_val: 2.3131 acc_val: 0.1521 time: 0.4085s\n",
      "Epoch: 1319 loss_train: 2.3215 acc_train: 0.1378 loss_val: 2.3129 acc_val: 0.1521 time: 0.3971s\n",
      "Epoch: 1320 loss_train: 2.3192 acc_train: 0.1406 loss_val: 2.3123 acc_val: 0.1521 time: 0.3657s\n",
      "Epoch: 1321 loss_train: 2.3186 acc_train: 0.1362 loss_val: 2.3120 acc_val: 0.1541 time: 0.3807s\n",
      "Epoch: 1322 loss_train: 2.3231 acc_train: 0.1440 loss_val: 2.3117 acc_val: 0.1536 time: 0.3517s\n",
      "Epoch: 1323 loss_train: 2.3202 acc_train: 0.1450 loss_val: 2.3114 acc_val: 0.1526 time: 0.3715s\n",
      "Epoch: 1324 loss_train: 2.3142 acc_train: 0.1384 loss_val: 2.3112 acc_val: 0.1526 time: 0.3984s\n",
      "Epoch: 1325 loss_train: 2.3141 acc_train: 0.1382 loss_val: 2.3111 acc_val: 0.1546 time: 0.3955s\n",
      "Epoch: 1326 loss_train: 2.3160 acc_train: 0.1415 loss_val: 2.3110 acc_val: 0.1531 time: 0.3629s\n",
      "Epoch: 1327 loss_train: 2.3195 acc_train: 0.1404 loss_val: 2.3110 acc_val: 0.1521 time: 0.3831s\n",
      "Epoch: 1328 loss_train: 2.3160 acc_train: 0.1391 loss_val: 2.3111 acc_val: 0.1521 time: 0.3401s\n",
      "Epoch: 1329 loss_train: 2.3247 acc_train: 0.1409 loss_val: 2.3111 acc_val: 0.1521 time: 0.3932s\n",
      "Epoch: 1330 loss_train: 2.3166 acc_train: 0.1460 loss_val: 2.3112 acc_val: 0.1516 time: 0.4057s\n",
      "Epoch: 1331 loss_train: 2.3195 acc_train: 0.1409 loss_val: 2.3114 acc_val: 0.1521 time: 0.3656s\n",
      "Epoch: 1332 loss_train: 2.3211 acc_train: 0.1476 loss_val: 2.3114 acc_val: 0.1510 time: 0.4080s\n",
      "Epoch: 1333 loss_train: 2.3210 acc_train: 0.1382 loss_val: 2.3115 acc_val: 0.1510 time: 0.4061s\n",
      "Epoch: 1334 loss_train: 2.3228 acc_train: 0.1438 loss_val: 2.3117 acc_val: 0.1546 time: 0.4089s\n",
      "Epoch: 1335 loss_train: 2.3220 acc_train: 0.1459 loss_val: 2.3120 acc_val: 0.1500 time: 0.4015s\n",
      "Epoch: 1336 loss_train: 2.3211 acc_train: 0.1490 loss_val: 2.3123 acc_val: 0.1495 time: 0.4019s\n",
      "Epoch: 1337 loss_train: 2.3154 acc_train: 0.1387 loss_val: 2.3124 acc_val: 0.1526 time: 0.3692s\n",
      "Epoch: 1338 loss_train: 2.3167 acc_train: 0.1473 loss_val: 2.3125 acc_val: 0.1521 time: 0.3914s\n",
      "Epoch: 1339 loss_train: 2.3177 acc_train: 0.1443 loss_val: 2.3125 acc_val: 0.1516 time: 0.3777s\n",
      "Epoch: 1340 loss_train: 2.3184 acc_train: 0.1440 loss_val: 2.3123 acc_val: 0.1505 time: 0.4009s\n",
      "Epoch: 1341 loss_train: 2.3147 acc_train: 0.1429 loss_val: 2.3122 acc_val: 0.1485 time: 0.3638s\n",
      "Epoch: 1342 loss_train: 2.3160 acc_train: 0.1366 loss_val: 2.3119 acc_val: 0.1510 time: 0.3613s\n",
      "Epoch: 1343 loss_train: 2.3239 acc_train: 0.1400 loss_val: 2.3116 acc_val: 0.1495 time: 0.3759s\n",
      "Epoch: 1344 loss_train: 2.3173 acc_train: 0.1564 loss_val: 2.3112 acc_val: 0.1526 time: 0.3394s\n",
      "Epoch: 1345 loss_train: 2.3184 acc_train: 0.1416 loss_val: 2.3110 acc_val: 0.1536 time: 0.3429s\n",
      "Epoch: 1346 loss_train: 2.3184 acc_train: 0.1426 loss_val: 2.3109 acc_val: 0.1536 time: 0.3721s\n",
      "Epoch: 1347 loss_train: 2.3192 acc_train: 0.1382 loss_val: 2.3110 acc_val: 0.1531 time: 0.3750s\n",
      "Epoch: 1348 loss_train: 2.3220 acc_train: 0.1454 loss_val: 2.3113 acc_val: 0.1510 time: 0.3986s\n",
      "Epoch: 1349 loss_train: 2.3220 acc_train: 0.1457 loss_val: 2.3116 acc_val: 0.1510 time: 0.3979s\n",
      "Epoch: 1350 loss_train: 2.3141 acc_train: 0.1429 loss_val: 2.3118 acc_val: 0.1510 time: 0.3759s\n",
      "Epoch: 1351 loss_train: 2.3213 acc_train: 0.1473 loss_val: 2.3119 acc_val: 0.1521 time: 0.4012s\n",
      "Epoch: 1352 loss_train: 2.3192 acc_train: 0.1415 loss_val: 2.3120 acc_val: 0.1526 time: 0.4198s\n",
      "Epoch: 1353 loss_train: 2.3184 acc_train: 0.1432 loss_val: 2.3120 acc_val: 0.1521 time: 0.3639s\n",
      "Epoch: 1354 loss_train: 2.3165 acc_train: 0.1501 loss_val: 2.3120 acc_val: 0.1531 time: 0.3780s\n",
      "Epoch: 1355 loss_train: 2.3163 acc_train: 0.1428 loss_val: 2.3120 acc_val: 0.1531 time: 0.3951s\n",
      "Epoch: 1356 loss_train: 2.3157 acc_train: 0.1531 loss_val: 2.3120 acc_val: 0.1531 time: 0.4075s\n",
      "Epoch: 1357 loss_train: 2.3215 acc_train: 0.1406 loss_val: 2.3120 acc_val: 0.1531 time: 0.3926s\n",
      "Epoch: 1358 loss_train: 2.3213 acc_train: 0.1382 loss_val: 2.3121 acc_val: 0.1521 time: 0.3951s\n",
      "Epoch: 1359 loss_train: 2.3176 acc_train: 0.1407 loss_val: 2.3123 acc_val: 0.1510 time: 0.4291s\n",
      "Epoch: 1360 loss_train: 2.3133 acc_train: 0.1586 loss_val: 2.3122 acc_val: 0.1510 time: 0.4027s\n",
      "Epoch: 1361 loss_train: 2.3176 acc_train: 0.1396 loss_val: 2.3120 acc_val: 0.1510 time: 0.3813s\n",
      "Epoch: 1362 loss_train: 2.3170 acc_train: 0.1516 loss_val: 2.3116 acc_val: 0.1510 time: 0.3896s\n",
      "Epoch: 1363 loss_train: 2.3211 acc_train: 0.1468 loss_val: 2.3114 acc_val: 0.1510 time: 0.4056s\n",
      "Epoch: 1364 loss_train: 2.3195 acc_train: 0.1497 loss_val: 2.3113 acc_val: 0.1521 time: 0.3706s\n",
      "Epoch: 1365 loss_train: 2.3153 acc_train: 0.1434 loss_val: 2.3112 acc_val: 0.1526 time: 0.4018s\n",
      "Epoch: 1366 loss_train: 2.3204 acc_train: 0.1541 loss_val: 2.3112 acc_val: 0.1531 time: 0.3887s\n",
      "Epoch: 1367 loss_train: 2.3212 acc_train: 0.1465 loss_val: 2.3114 acc_val: 0.1521 time: 0.3924s\n",
      "Epoch: 1368 loss_train: 2.3228 acc_train: 0.1381 loss_val: 2.3115 acc_val: 0.1521 time: 0.3871s\n",
      "Epoch: 1369 loss_train: 2.3222 acc_train: 0.1424 loss_val: 2.3116 acc_val: 0.1526 time: 0.4029s\n",
      "Epoch: 1370 loss_train: 2.3195 acc_train: 0.1413 loss_val: 2.3116 acc_val: 0.1521 time: 0.3949s\n",
      "Epoch: 1371 loss_train: 2.3214 acc_train: 0.1460 loss_val: 2.3116 acc_val: 0.1510 time: 0.3931s\n",
      "Epoch: 1372 loss_train: 2.3218 acc_train: 0.1410 loss_val: 2.3116 acc_val: 0.1510 time: 0.4404s\n",
      "Epoch: 1373 loss_train: 2.3171 acc_train: 0.1432 loss_val: 2.3115 acc_val: 0.1521 time: 0.4033s\n",
      "Epoch: 1374 loss_train: 2.3241 acc_train: 0.1426 loss_val: 2.3114 acc_val: 0.1521 time: 0.3776s\n",
      "Epoch: 1375 loss_train: 2.3160 acc_train: 0.1431 loss_val: 2.3113 acc_val: 0.1531 time: 0.3810s\n",
      "Epoch: 1376 loss_train: 2.3176 acc_train: 0.1396 loss_val: 2.3113 acc_val: 0.1521 time: 0.3941s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1377 loss_train: 2.3219 acc_train: 0.1391 loss_val: 2.3114 acc_val: 0.1536 time: 0.3967s\n",
      "Epoch: 1378 loss_train: 2.3171 acc_train: 0.1424 loss_val: 2.3118 acc_val: 0.1536 time: 0.3704s\n",
      "Epoch: 1379 loss_train: 2.3223 acc_train: 0.1365 loss_val: 2.3122 acc_val: 0.1531 time: 0.3885s\n",
      "Epoch: 1380 loss_train: 2.3195 acc_train: 0.1501 loss_val: 2.3125 acc_val: 0.1531 time: 0.3939s\n",
      "Epoch: 1381 loss_train: 2.3230 acc_train: 0.1478 loss_val: 2.3127 acc_val: 0.1526 time: 0.3635s\n",
      "Epoch: 1382 loss_train: 2.3200 acc_train: 0.1479 loss_val: 2.3127 acc_val: 0.1510 time: 0.3749s\n",
      "Epoch: 1383 loss_train: 2.3231 acc_train: 0.1451 loss_val: 2.3127 acc_val: 0.1521 time: 0.3888s\n",
      "Epoch: 1384 loss_train: 2.3185 acc_train: 0.1525 loss_val: 2.3126 acc_val: 0.1521 time: 0.3660s\n",
      "Epoch: 1385 loss_train: 2.3228 acc_train: 0.1478 loss_val: 2.3124 acc_val: 0.1510 time: 0.3704s\n",
      "Epoch: 1386 loss_train: 2.3212 acc_train: 0.1397 loss_val: 2.3121 acc_val: 0.1516 time: 0.3812s\n",
      "Epoch: 1387 loss_train: 2.3139 acc_train: 0.1516 loss_val: 2.3117 acc_val: 0.1531 time: 0.3693s\n",
      "Epoch: 1388 loss_train: 2.3192 acc_train: 0.1437 loss_val: 2.3114 acc_val: 0.1521 time: 0.3781s\n",
      "Epoch: 1389 loss_train: 2.3146 acc_train: 0.1515 loss_val: 2.3113 acc_val: 0.1521 time: 0.3560s\n",
      "Epoch: 1390 loss_train: 2.3215 acc_train: 0.1463 loss_val: 2.3111 acc_val: 0.1531 time: 0.3604s\n",
      "Epoch: 1391 loss_train: 2.3198 acc_train: 0.1476 loss_val: 2.3109 acc_val: 0.1521 time: 0.3872s\n",
      "Epoch: 1392 loss_train: 2.3124 acc_train: 0.1556 loss_val: 2.3107 acc_val: 0.1521 time: 0.3983s\n",
      "Epoch: 1393 loss_train: 2.3192 acc_train: 0.1475 loss_val: 2.3109 acc_val: 0.1521 time: 0.3871s\n",
      "Epoch: 1394 loss_train: 2.3120 acc_train: 0.1528 loss_val: 2.3111 acc_val: 0.1531 time: 0.3705s\n",
      "Epoch: 1395 loss_train: 2.3179 acc_train: 0.1407 loss_val: 2.3114 acc_val: 0.1536 time: 0.3840s\n",
      "Epoch: 1396 loss_train: 2.3229 acc_train: 0.1443 loss_val: 2.3118 acc_val: 0.1536 time: 0.3988s\n",
      "Epoch: 1397 loss_train: 2.3218 acc_train: 0.1416 loss_val: 2.3123 acc_val: 0.1531 time: 0.3935s\n",
      "Epoch: 1398 loss_train: 2.3207 acc_train: 0.1422 loss_val: 2.3125 acc_val: 0.1536 time: 0.3449s\n",
      "Epoch: 1399 loss_train: 2.3230 acc_train: 0.1360 loss_val: 2.3122 acc_val: 0.1521 time: 0.3868s\n",
      "Epoch: 1400 loss_train: 2.3173 acc_train: 0.1404 loss_val: 2.3120 acc_val: 0.1521 time: 0.3655s\n",
      "Epoch: 1401 loss_train: 2.3219 acc_train: 0.1434 loss_val: 2.3117 acc_val: 0.1510 time: 0.3709s\n",
      "Epoch: 1402 loss_train: 2.3152 acc_train: 0.1440 loss_val: 2.3115 acc_val: 0.1510 time: 0.3642s\n",
      "Epoch: 1403 loss_train: 2.3201 acc_train: 0.1360 loss_val: 2.3112 acc_val: 0.1521 time: 0.4082s\n",
      "Epoch: 1404 loss_train: 2.3165 acc_train: 0.1431 loss_val: 2.3109 acc_val: 0.1516 time: 0.3883s\n",
      "Epoch: 1405 loss_train: 2.3179 acc_train: 0.1450 loss_val: 2.3107 acc_val: 0.1521 time: 0.3784s\n",
      "Epoch: 1406 loss_train: 2.3155 acc_train: 0.1475 loss_val: 2.3107 acc_val: 0.1526 time: 0.3768s\n",
      "Epoch: 1407 loss_train: 2.3223 acc_train: 0.1406 loss_val: 2.3106 acc_val: 0.1531 time: 0.4023s\n",
      "Epoch: 1408 loss_train: 2.3229 acc_train: 0.1404 loss_val: 2.3107 acc_val: 0.1521 time: 0.4080s\n",
      "Epoch: 1409 loss_train: 2.3186 acc_train: 0.1460 loss_val: 2.3108 acc_val: 0.1510 time: 0.4266s\n",
      "Epoch: 1410 loss_train: 2.3175 acc_train: 0.1488 loss_val: 2.3107 acc_val: 0.1510 time: 0.3569s\n",
      "Epoch: 1411 loss_train: 2.3195 acc_train: 0.1451 loss_val: 2.3105 acc_val: 0.1510 time: 0.3299s\n",
      "Epoch: 1412 loss_train: 2.3184 acc_train: 0.1513 loss_val: 2.3104 acc_val: 0.1510 time: 0.3938s\n",
      "Epoch: 1413 loss_train: 2.3181 acc_train: 0.1487 loss_val: 2.3103 acc_val: 0.1510 time: 0.3693s\n",
      "Epoch: 1414 loss_train: 2.3206 acc_train: 0.1469 loss_val: 2.3104 acc_val: 0.1516 time: 0.4002s\n",
      "Epoch: 1415 loss_train: 2.3173 acc_train: 0.1397 loss_val: 2.3106 acc_val: 0.1521 time: 0.3731s\n",
      "Epoch: 1416 loss_train: 2.3133 acc_train: 0.1495 loss_val: 2.3109 acc_val: 0.1526 time: 0.3513s\n",
      "Epoch: 1417 loss_train: 2.3258 acc_train: 0.1510 loss_val: 2.3113 acc_val: 0.1536 time: 0.3688s\n",
      "Epoch: 1418 loss_train: 2.3200 acc_train: 0.1435 loss_val: 2.3116 acc_val: 0.1531 time: 0.3601s\n",
      "Epoch: 1419 loss_train: 2.3127 acc_train: 0.1400 loss_val: 2.3118 acc_val: 0.1531 time: 0.3785s\n",
      "Epoch: 1420 loss_train: 2.3176 acc_train: 0.1545 loss_val: 2.3120 acc_val: 0.1521 time: 0.3694s\n",
      "Epoch: 1421 loss_train: 2.3163 acc_train: 0.1438 loss_val: 2.3119 acc_val: 0.1521 time: 0.3971s\n",
      "Epoch: 1422 loss_train: 2.3149 acc_train: 0.1491 loss_val: 2.3118 acc_val: 0.1521 time: 0.3809s\n",
      "Epoch: 1423 loss_train: 2.3187 acc_train: 0.1350 loss_val: 2.3116 acc_val: 0.1521 time: 0.3691s\n",
      "Epoch: 1424 loss_train: 2.3208 acc_train: 0.1468 loss_val: 2.3114 acc_val: 0.1510 time: 0.3907s\n",
      "Epoch: 1425 loss_train: 2.3205 acc_train: 0.1454 loss_val: 2.3111 acc_val: 0.1510 time: 0.3595s\n",
      "Epoch: 1426 loss_train: 2.3221 acc_train: 0.1360 loss_val: 2.3110 acc_val: 0.1510 time: 0.3831s\n",
      "Epoch: 1427 loss_train: 2.3205 acc_train: 0.1378 loss_val: 2.3109 acc_val: 0.1510 time: 0.3837s\n",
      "Epoch: 1428 loss_train: 2.3203 acc_train: 0.1516 loss_val: 2.3108 acc_val: 0.1516 time: 0.3827s\n",
      "Epoch: 1429 loss_train: 2.3207 acc_train: 0.1441 loss_val: 2.3108 acc_val: 0.1521 time: 0.3903s\n",
      "Epoch: 1430 loss_train: 2.3226 acc_train: 0.1428 loss_val: 2.3108 acc_val: 0.1516 time: 0.4086s\n",
      "Epoch: 1431 loss_train: 2.3188 acc_train: 0.1473 loss_val: 2.3108 acc_val: 0.1526 time: 0.3693s\n",
      "Epoch: 1432 loss_train: 2.3165 acc_train: 0.1456 loss_val: 2.3107 acc_val: 0.1526 time: 0.4048s\n",
      "Epoch: 1433 loss_train: 2.3168 acc_train: 0.1441 loss_val: 2.3108 acc_val: 0.1526 time: 0.4020s\n",
      "Epoch: 1434 loss_train: 2.3149 acc_train: 0.1501 loss_val: 2.3108 acc_val: 0.1541 time: 0.3763s\n",
      "Epoch: 1435 loss_train: 2.3165 acc_train: 0.1443 loss_val: 2.3108 acc_val: 0.1536 time: 0.3992s\n",
      "Epoch: 1436 loss_train: 2.3247 acc_train: 0.1324 loss_val: 2.3109 acc_val: 0.1536 time: 0.3899s\n",
      "Epoch: 1437 loss_train: 2.3161 acc_train: 0.1509 loss_val: 2.3108 acc_val: 0.1521 time: 0.4046s\n",
      "Epoch: 1438 loss_train: 2.3127 acc_train: 0.1559 loss_val: 2.3110 acc_val: 0.1521 time: 0.4232s\n",
      "Epoch: 1439 loss_train: 2.3207 acc_train: 0.1390 loss_val: 2.3111 acc_val: 0.1516 time: 0.3976s\n",
      "Epoch: 1440 loss_train: 2.3224 acc_train: 0.1407 loss_val: 2.3113 acc_val: 0.1505 time: 0.3694s\n",
      "Epoch: 1441 loss_train: 2.3205 acc_train: 0.1501 loss_val: 2.3115 acc_val: 0.1521 time: 0.4016s\n",
      "Epoch: 1442 loss_train: 2.3154 acc_train: 0.1475 loss_val: 2.3115 acc_val: 0.1510 time: 0.3990s\n",
      "Epoch: 1443 loss_train: 2.3192 acc_train: 0.1503 loss_val: 2.3113 acc_val: 0.1510 time: 0.4070s\n",
      "Epoch: 1444 loss_train: 2.3145 acc_train: 0.1418 loss_val: 2.3113 acc_val: 0.1531 time: 0.4197s\n",
      "Epoch: 1445 loss_train: 2.3197 acc_train: 0.1406 loss_val: 2.3112 acc_val: 0.1536 time: 0.4111s\n",
      "Epoch: 1446 loss_train: 2.3210 acc_train: 0.1319 loss_val: 2.3113 acc_val: 0.1541 time: 0.3913s\n",
      "Epoch: 1447 loss_train: 2.3190 acc_train: 0.1396 loss_val: 2.3114 acc_val: 0.1536 time: 0.3822s\n",
      "Epoch: 1448 loss_train: 2.3137 acc_train: 0.1506 loss_val: 2.3115 acc_val: 0.1526 time: 0.3725s\n",
      "Epoch: 1449 loss_train: 2.3154 acc_train: 0.1497 loss_val: 2.3115 acc_val: 0.1516 time: 0.3932s\n",
      "Epoch: 1450 loss_train: 2.3199 acc_train: 0.1501 loss_val: 2.3115 acc_val: 0.1521 time: 0.3472s\n",
      "Epoch: 1451 loss_train: 2.3221 acc_train: 0.1394 loss_val: 2.3114 acc_val: 0.1521 time: 0.3792s\n",
      "Epoch: 1452 loss_train: 2.3139 acc_train: 0.1476 loss_val: 2.3112 acc_val: 0.1521 time: 0.3726s\n",
      "Epoch: 1453 loss_train: 2.3205 acc_train: 0.1456 loss_val: 2.3109 acc_val: 0.1510 time: 0.3751s\n",
      "Epoch: 1454 loss_train: 2.3179 acc_train: 0.1372 loss_val: 2.3107 acc_val: 0.1510 time: 0.3722s\n",
      "Epoch: 1455 loss_train: 2.3171 acc_train: 0.1506 loss_val: 2.3106 acc_val: 0.1510 time: 0.3884s\n",
      "Epoch: 1456 loss_train: 2.3171 acc_train: 0.1513 loss_val: 2.3105 acc_val: 0.1521 time: 0.3990s\n",
      "Epoch: 1457 loss_train: 2.3217 acc_train: 0.1399 loss_val: 2.3106 acc_val: 0.1536 time: 0.4062s\n",
      "Epoch: 1458 loss_train: 2.3159 acc_train: 0.1481 loss_val: 2.3107 acc_val: 0.1536 time: 0.4090s\n",
      "Epoch: 1459 loss_train: 2.3176 acc_train: 0.1498 loss_val: 2.3109 acc_val: 0.1536 time: 0.3792s\n",
      "Epoch: 1460 loss_train: 2.3190 acc_train: 0.1456 loss_val: 2.3110 acc_val: 0.1531 time: 0.3773s\n",
      "Epoch: 1461 loss_train: 2.3202 acc_train: 0.1444 loss_val: 2.3111 acc_val: 0.1516 time: 0.4097s\n",
      "Epoch: 1462 loss_train: 2.3164 acc_train: 0.1495 loss_val: 2.3111 acc_val: 0.1526 time: 0.3944s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1463 loss_train: 2.3200 acc_train: 0.1365 loss_val: 2.3112 acc_val: 0.1531 time: 0.4070s\n",
      "Epoch: 1464 loss_train: 2.3184 acc_train: 0.1382 loss_val: 2.3112 acc_val: 0.1510 time: 0.4050s\n",
      "Epoch: 1465 loss_train: 2.3158 acc_train: 0.1491 loss_val: 2.3112 acc_val: 0.1521 time: 0.3684s\n",
      "Epoch: 1466 loss_train: 2.3144 acc_train: 0.1402 loss_val: 2.3113 acc_val: 0.1536 time: 0.3974s\n",
      "Epoch: 1467 loss_train: 2.3193 acc_train: 0.1512 loss_val: 2.3115 acc_val: 0.1526 time: 0.4043s\n",
      "Epoch: 1468 loss_train: 2.3199 acc_train: 0.1410 loss_val: 2.3117 acc_val: 0.1526 time: 0.3931s\n",
      "Epoch: 1469 loss_train: 2.3189 acc_train: 0.1438 loss_val: 2.3118 acc_val: 0.1521 time: 0.4017s\n",
      "Epoch: 1470 loss_train: 2.3202 acc_train: 0.1331 loss_val: 2.3118 acc_val: 0.1521 time: 0.3747s\n",
      "Epoch: 1471 loss_train: 2.3183 acc_train: 0.1498 loss_val: 2.3118 acc_val: 0.1510 time: 0.4080s\n",
      "Epoch: 1472 loss_train: 2.3163 acc_train: 0.1457 loss_val: 2.3117 acc_val: 0.1510 time: 0.3909s\n",
      "Epoch: 1473 loss_train: 2.3177 acc_train: 0.1447 loss_val: 2.3116 acc_val: 0.1510 time: 0.3680s\n",
      "Epoch: 1474 loss_train: 2.3176 acc_train: 0.1435 loss_val: 2.3115 acc_val: 0.1521 time: 0.3714s\n",
      "Epoch: 1475 loss_train: 2.3161 acc_train: 0.1438 loss_val: 2.3114 acc_val: 0.1516 time: 0.3726s\n",
      "Epoch: 1476 loss_train: 2.3187 acc_train: 0.1375 loss_val: 2.3114 acc_val: 0.1526 time: 0.3986s\n",
      "Epoch: 1477 loss_train: 2.3191 acc_train: 0.1448 loss_val: 2.3115 acc_val: 0.1526 time: 0.4153s\n",
      "Epoch: 1478 loss_train: 2.3229 acc_train: 0.1368 loss_val: 2.3116 acc_val: 0.1516 time: 0.3960s\n",
      "Epoch: 1479 loss_train: 2.3190 acc_train: 0.1390 loss_val: 2.3118 acc_val: 0.1521 time: 0.3833s\n",
      "Epoch: 1480 loss_train: 2.3212 acc_train: 0.1476 loss_val: 2.3119 acc_val: 0.1510 time: 0.3942s\n",
      "Epoch: 1481 loss_train: 2.3193 acc_train: 0.1429 loss_val: 2.3120 acc_val: 0.1510 time: 0.4180s\n",
      "Epoch: 1482 loss_train: 2.3227 acc_train: 0.1438 loss_val: 2.3120 acc_val: 0.1510 time: 0.3919s\n",
      "Epoch: 1483 loss_train: 2.3166 acc_train: 0.1466 loss_val: 2.3119 acc_val: 0.1510 time: 0.3889s\n",
      "Epoch: 1484 loss_train: 2.3165 acc_train: 0.1450 loss_val: 2.3118 acc_val: 0.1510 time: 0.4050s\n",
      "Epoch: 1485 loss_train: 2.3153 acc_train: 0.1448 loss_val: 2.3115 acc_val: 0.1510 time: 0.3710s\n",
      "Epoch: 1486 loss_train: 2.3211 acc_train: 0.1437 loss_val: 2.3110 acc_val: 0.1516 time: 0.3803s\n",
      "Epoch: 1487 loss_train: 2.3151 acc_train: 0.1479 loss_val: 2.3107 acc_val: 0.1531 time: 0.4030s\n",
      "Epoch: 1488 loss_train: 2.3218 acc_train: 0.1419 loss_val: 2.3105 acc_val: 0.1531 time: 0.3991s\n",
      "Epoch: 1489 loss_train: 2.3177 acc_train: 0.1357 loss_val: 2.3105 acc_val: 0.1536 time: 0.3922s\n",
      "Epoch: 1490 loss_train: 2.3179 acc_train: 0.1519 loss_val: 2.3106 acc_val: 0.1536 time: 0.3777s\n",
      "Epoch: 1491 loss_train: 2.3218 acc_train: 0.1441 loss_val: 2.3108 acc_val: 0.1536 time: 0.4066s\n",
      "Epoch: 1492 loss_train: 2.3132 acc_train: 0.1494 loss_val: 2.3109 acc_val: 0.1536 time: 0.4049s\n",
      "Epoch: 1493 loss_train: 2.3173 acc_train: 0.1503 loss_val: 2.3111 acc_val: 0.1531 time: 0.3968s\n",
      "Epoch: 1494 loss_train: 2.3192 acc_train: 0.1413 loss_val: 2.3113 acc_val: 0.1521 time: 0.4227s\n",
      "Epoch: 1495 loss_train: 2.3211 acc_train: 0.1478 loss_val: 2.3115 acc_val: 0.1521 time: 0.4221s\n",
      "Epoch: 1496 loss_train: 2.3194 acc_train: 0.1469 loss_val: 2.3114 acc_val: 0.1505 time: 0.4023s\n",
      "Epoch: 1497 loss_train: 2.3228 acc_train: 0.1365 loss_val: 2.3114 acc_val: 0.1510 time: 0.3974s\n",
      "Epoch: 1498 loss_train: 2.3169 acc_train: 0.1465 loss_val: 2.3112 acc_val: 0.1521 time: 0.3807s\n",
      "Epoch: 1499 loss_train: 2.3214 acc_train: 0.1397 loss_val: 2.3110 acc_val: 0.1521 time: 0.3927s\n",
      "Epoch: 1500 loss_train: 2.3199 acc_train: 0.1498 loss_val: 2.3109 acc_val: 0.1531 time: 0.3941s\n",
      "Epoch: 1501 loss_train: 2.3160 acc_train: 0.1444 loss_val: 2.3109 acc_val: 0.1536 time: 0.3840s\n",
      "Epoch: 1502 loss_train: 2.3225 acc_train: 0.1466 loss_val: 2.3111 acc_val: 0.1526 time: 0.4067s\n",
      "Epoch: 1503 loss_train: 2.3139 acc_train: 0.1457 loss_val: 2.3115 acc_val: 0.1536 time: 0.4193s\n",
      "Epoch: 1504 loss_train: 2.3209 acc_train: 0.1446 loss_val: 2.3117 acc_val: 0.1531 time: 0.4266s\n",
      "Epoch: 1505 loss_train: 2.3204 acc_train: 0.1374 loss_val: 2.3122 acc_val: 0.1521 time: 0.4131s\n",
      "Epoch: 1506 loss_train: 2.3203 acc_train: 0.1473 loss_val: 2.3126 acc_val: 0.1516 time: 0.4354s\n",
      "Epoch: 1507 loss_train: 2.3191 acc_train: 0.1396 loss_val: 2.3127 acc_val: 0.1500 time: 0.3840s\n",
      "Epoch: 1508 loss_train: 2.3238 acc_train: 0.1390 loss_val: 2.3129 acc_val: 0.1505 time: 0.3980s\n",
      "Epoch: 1509 loss_train: 2.3151 acc_train: 0.1515 loss_val: 2.3127 acc_val: 0.1521 time: 0.3911s\n",
      "Epoch: 1510 loss_train: 2.3194 acc_train: 0.1447 loss_val: 2.3121 acc_val: 0.1521 time: 0.4069s\n",
      "Epoch: 1511 loss_train: 2.3185 acc_train: 0.1488 loss_val: 2.3115 acc_val: 0.1521 time: 0.4099s\n",
      "Epoch: 1512 loss_train: 2.3190 acc_train: 0.1422 loss_val: 2.3110 acc_val: 0.1526 time: 0.4370s\n",
      "Epoch: 1513 loss_train: 2.3239 acc_train: 0.1335 loss_val: 2.3106 acc_val: 0.1526 time: 0.3876s\n",
      "Epoch: 1514 loss_train: 2.3165 acc_train: 0.1471 loss_val: 2.3105 acc_val: 0.1531 time: 0.4065s\n",
      "Epoch: 1515 loss_train: 2.3231 acc_train: 0.1368 loss_val: 2.3105 acc_val: 0.1521 time: 0.4013s\n",
      "Epoch: 1516 loss_train: 2.3187 acc_train: 0.1463 loss_val: 2.3105 acc_val: 0.1516 time: 0.4105s\n",
      "Epoch: 1517 loss_train: 2.3167 acc_train: 0.1497 loss_val: 2.3107 acc_val: 0.1521 time: 0.4031s\n",
      "Epoch: 1518 loss_train: 2.3222 acc_train: 0.1390 loss_val: 2.3109 acc_val: 0.1521 time: 0.4161s\n",
      "Epoch: 1519 loss_train: 2.3135 acc_train: 0.1537 loss_val: 2.3112 acc_val: 0.1521 time: 0.3986s\n",
      "Epoch: 1520 loss_train: 2.3199 acc_train: 0.1415 loss_val: 2.3115 acc_val: 0.1516 time: 0.4105s\n",
      "Epoch: 1521 loss_train: 2.3171 acc_train: 0.1495 loss_val: 2.3120 acc_val: 0.1531 time: 0.3664s\n",
      "Epoch: 1522 loss_train: 2.3190 acc_train: 0.1487 loss_val: 2.3122 acc_val: 0.1526 time: 0.3915s\n",
      "Epoch: 1523 loss_train: 2.3233 acc_train: 0.1368 loss_val: 2.3124 acc_val: 0.1526 time: 0.3795s\n",
      "Epoch: 1524 loss_train: 2.3179 acc_train: 0.1368 loss_val: 2.3125 acc_val: 0.1536 time: 0.4248s\n",
      "Epoch: 1525 loss_train: 2.3218 acc_train: 0.1429 loss_val: 2.3127 acc_val: 0.1536 time: 0.4065s\n",
      "Epoch: 1526 loss_train: 2.3180 acc_train: 0.1377 loss_val: 2.3127 acc_val: 0.1531 time: 0.4174s\n",
      "Epoch: 1527 loss_train: 2.3176 acc_train: 0.1453 loss_val: 2.3127 acc_val: 0.1521 time: 0.4040s\n",
      "Epoch: 1528 loss_train: 2.3180 acc_train: 0.1482 loss_val: 2.3126 acc_val: 0.1516 time: 0.4249s\n",
      "Epoch: 1529 loss_train: 2.3246 acc_train: 0.1507 loss_val: 2.3125 acc_val: 0.1521 time: 0.4256s\n",
      "Epoch: 1530 loss_train: 2.3152 acc_train: 0.1487 loss_val: 2.3122 acc_val: 0.1521 time: 0.3794s\n",
      "Epoch: 1531 loss_train: 2.3154 acc_train: 0.1416 loss_val: 2.3120 acc_val: 0.1521 time: 0.4137s\n",
      "Epoch: 1532 loss_train: 2.3125 acc_train: 0.1495 loss_val: 2.3115 acc_val: 0.1521 time: 0.3988s\n",
      "Epoch: 1533 loss_train: 2.3191 acc_train: 0.1475 loss_val: 2.3112 acc_val: 0.1521 time: 0.4073s\n",
      "Epoch: 1534 loss_train: 2.3164 acc_train: 0.1385 loss_val: 2.3109 acc_val: 0.1536 time: 0.3839s\n",
      "Epoch: 1535 loss_train: 2.3166 acc_train: 0.1484 loss_val: 2.3108 acc_val: 0.1526 time: 0.3864s\n",
      "Epoch: 1536 loss_train: 2.3215 acc_train: 0.1429 loss_val: 2.3107 acc_val: 0.1526 time: 0.3808s\n",
      "Epoch: 1537 loss_train: 2.3167 acc_train: 0.1412 loss_val: 2.3107 acc_val: 0.1531 time: 0.4032s\n",
      "Epoch: 1538 loss_train: 2.3186 acc_train: 0.1453 loss_val: 2.3108 acc_val: 0.1531 time: 0.4071s\n",
      "Epoch: 1539 loss_train: 2.3167 acc_train: 0.1522 loss_val: 2.3107 acc_val: 0.1521 time: 0.3763s\n",
      "Epoch: 1540 loss_train: 2.3174 acc_train: 0.1390 loss_val: 2.3107 acc_val: 0.1516 time: 0.4022s\n",
      "Epoch: 1541 loss_train: 2.3212 acc_train: 0.1500 loss_val: 2.3107 acc_val: 0.1521 time: 0.3878s\n",
      "Epoch: 1542 loss_train: 2.3217 acc_train: 0.1488 loss_val: 2.3108 acc_val: 0.1510 time: 0.4174s\n",
      "Epoch: 1543 loss_train: 2.3199 acc_train: 0.1435 loss_val: 2.3107 acc_val: 0.1510 time: 0.3842s\n",
      "Epoch: 1544 loss_train: 2.3171 acc_train: 0.1380 loss_val: 2.3107 acc_val: 0.1510 time: 0.3948s\n",
      "Epoch: 1545 loss_train: 2.3227 acc_train: 0.1503 loss_val: 2.3107 acc_val: 0.1510 time: 0.4469s\n",
      "Epoch: 1546 loss_train: 2.3190 acc_train: 0.1422 loss_val: 2.3108 acc_val: 0.1510 time: 0.3872s\n",
      "Epoch: 1547 loss_train: 2.3187 acc_train: 0.1472 loss_val: 2.3111 acc_val: 0.1510 time: 0.3926s\n",
      "Epoch: 1548 loss_train: 2.3145 acc_train: 0.1510 loss_val: 2.3114 acc_val: 0.1521 time: 0.4154s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1549 loss_train: 2.3179 acc_train: 0.1441 loss_val: 2.3118 acc_val: 0.1526 time: 0.4029s\n",
      "Epoch: 1550 loss_train: 2.3167 acc_train: 0.1523 loss_val: 2.3122 acc_val: 0.1536 time: 0.4057s\n",
      "Epoch: 1551 loss_train: 2.3154 acc_train: 0.1481 loss_val: 2.3124 acc_val: 0.1541 time: 0.4041s\n",
      "Epoch: 1552 loss_train: 2.3195 acc_train: 0.1415 loss_val: 2.3126 acc_val: 0.1531 time: 0.4037s\n",
      "Epoch: 1553 loss_train: 2.3234 acc_train: 0.1491 loss_val: 2.3126 acc_val: 0.1505 time: 0.4182s\n",
      "Epoch: 1554 loss_train: 2.3158 acc_train: 0.1451 loss_val: 2.3125 acc_val: 0.1521 time: 0.4298s\n",
      "Epoch: 1555 loss_train: 2.3134 acc_train: 0.1448 loss_val: 2.3124 acc_val: 0.1510 time: 0.4035s\n",
      "Epoch: 1556 loss_train: 2.3178 acc_train: 0.1468 loss_val: 2.3123 acc_val: 0.1505 time: 0.4189s\n",
      "Epoch: 1557 loss_train: 2.3178 acc_train: 0.1381 loss_val: 2.3120 acc_val: 0.1521 time: 0.4308s\n",
      "Epoch: 1558 loss_train: 2.3252 acc_train: 0.1363 loss_val: 2.3116 acc_val: 0.1510 time: 0.4431s\n",
      "Epoch: 1559 loss_train: 2.3201 acc_train: 0.1444 loss_val: 2.3112 acc_val: 0.1521 time: 0.4143s\n",
      "Epoch: 1560 loss_train: 2.3130 acc_train: 0.1425 loss_val: 2.3109 acc_val: 0.1521 time: 0.4356s\n",
      "Epoch: 1561 loss_train: 2.3173 acc_train: 0.1462 loss_val: 2.3107 acc_val: 0.1526 time: 0.4179s\n",
      "Epoch: 1562 loss_train: 2.3223 acc_train: 0.1487 loss_val: 2.3107 acc_val: 0.1531 time: 0.4313s\n",
      "Epoch: 1563 loss_train: 2.3206 acc_train: 0.1375 loss_val: 2.3109 acc_val: 0.1521 time: 0.3780s\n",
      "Epoch: 1564 loss_train: 2.3182 acc_train: 0.1460 loss_val: 2.3113 acc_val: 0.1521 time: 0.3996s\n",
      "Epoch: 1565 loss_train: 2.3195 acc_train: 0.1407 loss_val: 2.3120 acc_val: 0.1516 time: 0.4172s\n",
      "Epoch: 1566 loss_train: 2.3161 acc_train: 0.1503 loss_val: 2.3127 acc_val: 0.1526 time: 0.4064s\n",
      "Epoch: 1567 loss_train: 2.3213 acc_train: 0.1391 loss_val: 2.3132 acc_val: 0.1505 time: 0.3984s\n",
      "Epoch: 1568 loss_train: 2.3122 acc_train: 0.1431 loss_val: 2.3135 acc_val: 0.1536 time: 0.4012s\n",
      "Epoch: 1569 loss_train: 2.3151 acc_train: 0.1472 loss_val: 2.3136 acc_val: 0.1541 time: 0.4214s\n",
      "Epoch: 1570 loss_train: 2.3154 acc_train: 0.1553 loss_val: 2.3132 acc_val: 0.1521 time: 0.4242s\n",
      "Epoch: 1571 loss_train: 2.3205 acc_train: 0.1437 loss_val: 2.3127 acc_val: 0.1516 time: 0.4156s\n",
      "Epoch: 1572 loss_train: 2.3197 acc_train: 0.1378 loss_val: 2.3120 acc_val: 0.1510 time: 0.4011s\n",
      "Epoch: 1573 loss_train: 2.3246 acc_train: 0.1421 loss_val: 2.3113 acc_val: 0.1521 time: 0.4237s\n",
      "Epoch: 1574 loss_train: 2.3194 acc_train: 0.1476 loss_val: 2.3107 acc_val: 0.1510 time: 0.3956s\n",
      "Epoch: 1575 loss_train: 2.3163 acc_train: 0.1385 loss_val: 2.3102 acc_val: 0.1510 time: 0.4237s\n",
      "Epoch: 1576 loss_train: 2.3184 acc_train: 0.1485 loss_val: 2.3099 acc_val: 0.1531 time: 0.4057s\n",
      "Epoch: 1577 loss_train: 2.3183 acc_train: 0.1466 loss_val: 2.3099 acc_val: 0.1526 time: 0.4136s\n",
      "Epoch: 1578 loss_train: 2.3178 acc_train: 0.1528 loss_val: 2.3099 acc_val: 0.1526 time: 0.4304s\n",
      "Epoch: 1579 loss_train: 2.3194 acc_train: 0.1409 loss_val: 2.3100 acc_val: 0.1536 time: 0.3979s\n",
      "Epoch: 1580 loss_train: 2.3119 acc_train: 0.1446 loss_val: 2.3104 acc_val: 0.1531 time: 0.4162s\n",
      "Epoch: 1581 loss_train: 2.3193 acc_train: 0.1359 loss_val: 2.3109 acc_val: 0.1516 time: 0.4048s\n",
      "Epoch: 1582 loss_train: 2.3157 acc_train: 0.1441 loss_val: 2.3114 acc_val: 0.1505 time: 0.4114s\n",
      "Epoch: 1583 loss_train: 2.3183 acc_train: 0.1465 loss_val: 2.3120 acc_val: 0.1521 time: 0.3814s\n",
      "Epoch: 1584 loss_train: 2.3167 acc_train: 0.1510 loss_val: 2.3126 acc_val: 0.1521 time: 0.3923s\n",
      "Epoch: 1585 loss_train: 2.3167 acc_train: 0.1501 loss_val: 2.3129 acc_val: 0.1521 time: 0.4230s\n",
      "Epoch: 1586 loss_train: 2.3204 acc_train: 0.1462 loss_val: 2.3128 acc_val: 0.1510 time: 0.4263s\n",
      "Epoch: 1587 loss_train: 2.3174 acc_train: 0.1424 loss_val: 2.3124 acc_val: 0.1516 time: 0.4296s\n",
      "Epoch: 1588 loss_train: 2.3166 acc_train: 0.1534 loss_val: 2.3118 acc_val: 0.1521 time: 0.3942s\n",
      "Epoch: 1589 loss_train: 2.3207 acc_train: 0.1468 loss_val: 2.3110 acc_val: 0.1521 time: 0.4294s\n",
      "Epoch: 1590 loss_train: 2.3203 acc_train: 0.1478 loss_val: 2.3103 acc_val: 0.1521 time: 0.4247s\n",
      "Epoch: 1591 loss_train: 2.3223 acc_train: 0.1438 loss_val: 2.3097 acc_val: 0.1521 time: 0.4090s\n",
      "Epoch: 1592 loss_train: 2.3181 acc_train: 0.1541 loss_val: 2.3095 acc_val: 0.1521 time: 0.4153s\n",
      "Epoch: 1593 loss_train: 2.3221 acc_train: 0.1475 loss_val: 2.3095 acc_val: 0.1536 time: 0.4403s\n",
      "Epoch: 1594 loss_train: 2.3151 acc_train: 0.1441 loss_val: 2.3097 acc_val: 0.1536 time: 0.3935s\n",
      "Epoch: 1595 loss_train: 2.3185 acc_train: 0.1539 loss_val: 2.3099 acc_val: 0.1541 time: 0.4114s\n",
      "Epoch: 1596 loss_train: 2.3204 acc_train: 0.1399 loss_val: 2.3103 acc_val: 0.1516 time: 0.4044s\n",
      "Epoch: 1597 loss_train: 2.3200 acc_train: 0.1500 loss_val: 2.3106 acc_val: 0.1505 time: 0.3976s\n",
      "Epoch: 1598 loss_train: 2.3150 acc_train: 0.1488 loss_val: 2.3109 acc_val: 0.1521 time: 0.3844s\n",
      "Epoch: 1599 loss_train: 2.3187 acc_train: 0.1463 loss_val: 2.3109 acc_val: 0.1516 time: 0.4040s\n",
      "Epoch: 1600 loss_train: 2.3180 acc_train: 0.1357 loss_val: 2.3109 acc_val: 0.1510 time: 0.4064s\n",
      "Epoch: 1601 loss_train: 2.3214 acc_train: 0.1416 loss_val: 2.3105 acc_val: 0.1510 time: 0.3755s\n",
      "Epoch: 1602 loss_train: 2.3204 acc_train: 0.1396 loss_val: 2.3103 acc_val: 0.1510 time: 0.3927s\n",
      "Epoch: 1603 loss_train: 2.3219 acc_train: 0.1519 loss_val: 2.3101 acc_val: 0.1521 time: 0.3944s\n",
      "Epoch: 1604 loss_train: 2.3183 acc_train: 0.1368 loss_val: 2.3103 acc_val: 0.1526 time: 0.4408s\n",
      "Epoch: 1605 loss_train: 2.3189 acc_train: 0.1450 loss_val: 2.3104 acc_val: 0.1516 time: 0.3730s\n",
      "Epoch: 1606 loss_train: 2.3211 acc_train: 0.1394 loss_val: 2.3105 acc_val: 0.1505 time: 0.3784s\n",
      "Epoch: 1607 loss_train: 2.3186 acc_train: 0.1429 loss_val: 2.3106 acc_val: 0.1516 time: 0.4044s\n",
      "Epoch: 1608 loss_train: 2.3152 acc_train: 0.1488 loss_val: 2.3106 acc_val: 0.1505 time: 0.4191s\n",
      "Epoch: 1609 loss_train: 2.3131 acc_train: 0.1446 loss_val: 2.3106 acc_val: 0.1505 time: 0.4027s\n",
      "Epoch: 1610 loss_train: 2.3143 acc_train: 0.1451 loss_val: 2.3107 acc_val: 0.1521 time: 0.4313s\n",
      "Epoch: 1611 loss_train: 2.3183 acc_train: 0.1444 loss_val: 2.3106 acc_val: 0.1521 time: 0.4005s\n",
      "Epoch: 1612 loss_train: 2.3203 acc_train: 0.1463 loss_val: 2.3106 acc_val: 0.1510 time: 0.4209s\n",
      "Epoch: 1613 loss_train: 2.3151 acc_train: 0.1429 loss_val: 2.3107 acc_val: 0.1521 time: 0.4178s\n",
      "Epoch: 1614 loss_train: 2.3153 acc_train: 0.1393 loss_val: 2.3109 acc_val: 0.1521 time: 0.3888s\n",
      "Epoch: 1615 loss_train: 2.3226 acc_train: 0.1491 loss_val: 2.3111 acc_val: 0.1526 time: 0.3971s\n",
      "Epoch: 1616 loss_train: 2.3201 acc_train: 0.1491 loss_val: 2.3112 acc_val: 0.1521 time: 0.4194s\n",
      "Epoch: 1617 loss_train: 2.3177 acc_train: 0.1382 loss_val: 2.3113 acc_val: 0.1521 time: 0.4151s\n",
      "Epoch: 1618 loss_train: 2.3209 acc_train: 0.1526 loss_val: 2.3114 acc_val: 0.1521 time: 0.3938s\n",
      "Epoch: 1619 loss_train: 2.3170 acc_train: 0.1419 loss_val: 2.3114 acc_val: 0.1521 time: 0.4116s\n",
      "Epoch: 1620 loss_train: 2.3207 acc_train: 0.1426 loss_val: 2.3113 acc_val: 0.1521 time: 0.3997s\n",
      "Epoch: 1621 loss_train: 2.3136 acc_train: 0.1529 loss_val: 2.3111 acc_val: 0.1521 time: 0.3937s\n",
      "Epoch: 1622 loss_train: 2.3203 acc_train: 0.1465 loss_val: 2.3109 acc_val: 0.1521 time: 0.3953s\n",
      "Epoch: 1623 loss_train: 2.3198 acc_train: 0.1462 loss_val: 2.3108 acc_val: 0.1521 time: 0.4043s\n",
      "Epoch: 1624 loss_train: 2.3229 acc_train: 0.1471 loss_val: 2.3108 acc_val: 0.1531 time: 0.4110s\n",
      "Epoch: 1625 loss_train: 2.3181 acc_train: 0.1498 loss_val: 2.3109 acc_val: 0.1531 time: 0.4053s\n",
      "Epoch: 1626 loss_train: 2.3180 acc_train: 0.1390 loss_val: 2.3111 acc_val: 0.1526 time: 0.4246s\n",
      "Epoch: 1627 loss_train: 2.3142 acc_train: 0.1501 loss_val: 2.3113 acc_val: 0.1521 time: 0.3827s\n",
      "Epoch: 1628 loss_train: 2.3174 acc_train: 0.1418 loss_val: 2.3114 acc_val: 0.1521 time: 0.4084s\n",
      "Epoch: 1629 loss_train: 2.3214 acc_train: 0.1368 loss_val: 2.3116 acc_val: 0.1531 time: 0.3760s\n",
      "Epoch: 1630 loss_train: 2.3148 acc_train: 0.1516 loss_val: 2.3117 acc_val: 0.1531 time: 0.3655s\n",
      "Epoch: 1631 loss_train: 2.3257 acc_train: 0.1446 loss_val: 2.3118 acc_val: 0.1531 time: 0.4003s\n",
      "Epoch: 1632 loss_train: 2.3181 acc_train: 0.1450 loss_val: 2.3119 acc_val: 0.1521 time: 0.4269s\n",
      "Epoch: 1633 loss_train: 2.3200 acc_train: 0.1396 loss_val: 2.3119 acc_val: 0.1521 time: 0.4120s\n",
      "Epoch: 1634 loss_train: 2.3172 acc_train: 0.1384 loss_val: 2.3118 acc_val: 0.1516 time: 0.4128s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1635 loss_train: 2.3241 acc_train: 0.1446 loss_val: 2.3116 acc_val: 0.1521 time: 0.4483s\n",
      "Epoch: 1636 loss_train: 2.3253 acc_train: 0.1357 loss_val: 2.3114 acc_val: 0.1510 time: 0.3728s\n",
      "Epoch: 1637 loss_train: 2.3144 acc_train: 0.1559 loss_val: 2.3111 acc_val: 0.1510 time: 0.4010s\n",
      "Epoch: 1638 loss_train: 2.3192 acc_train: 0.1432 loss_val: 2.3107 acc_val: 0.1516 time: 0.4249s\n",
      "Epoch: 1639 loss_train: 2.3194 acc_train: 0.1448 loss_val: 2.3104 acc_val: 0.1521 time: 0.4042s\n",
      "Epoch: 1640 loss_train: 2.3134 acc_train: 0.1363 loss_val: 2.3103 acc_val: 0.1516 time: 0.3899s\n",
      "Epoch: 1641 loss_train: 2.3192 acc_train: 0.1509 loss_val: 2.3105 acc_val: 0.1516 time: 0.4029s\n",
      "Epoch: 1642 loss_train: 2.3196 acc_train: 0.1446 loss_val: 2.3107 acc_val: 0.1521 time: 0.4010s\n",
      "Epoch: 1643 loss_train: 2.3173 acc_train: 0.1404 loss_val: 2.3110 acc_val: 0.1521 time: 0.4160s\n",
      "Epoch: 1644 loss_train: 2.3192 acc_train: 0.1476 loss_val: 2.3113 acc_val: 0.1510 time: 0.3982s\n",
      "Epoch: 1645 loss_train: 2.3183 acc_train: 0.1468 loss_val: 2.3117 acc_val: 0.1510 time: 0.4209s\n",
      "Epoch: 1646 loss_train: 2.3170 acc_train: 0.1378 loss_val: 2.3119 acc_val: 0.1510 time: 0.4166s\n",
      "Epoch: 1647 loss_train: 2.3159 acc_train: 0.1507 loss_val: 2.3119 acc_val: 0.1510 time: 0.3858s\n",
      "Epoch: 1648 loss_train: 2.3185 acc_train: 0.1476 loss_val: 2.3117 acc_val: 0.1510 time: 0.4112s\n",
      "Epoch: 1649 loss_train: 2.3225 acc_train: 0.1368 loss_val: 2.3115 acc_val: 0.1510 time: 0.4081s\n",
      "Epoch: 1650 loss_train: 2.3143 acc_train: 0.1456 loss_val: 2.3112 acc_val: 0.1510 time: 0.4259s\n",
      "Epoch: 1651 loss_train: 2.3217 acc_train: 0.1394 loss_val: 2.3109 acc_val: 0.1521 time: 0.4374s\n",
      "Epoch: 1652 loss_train: 2.3216 acc_train: 0.1368 loss_val: 2.3108 acc_val: 0.1521 time: 0.4156s\n",
      "Epoch: 1653 loss_train: 2.3208 acc_train: 0.1490 loss_val: 2.3108 acc_val: 0.1521 time: 0.4080s\n",
      "Epoch: 1654 loss_train: 2.3208 acc_train: 0.1403 loss_val: 2.3108 acc_val: 0.1521 time: 0.4284s\n",
      "Epoch: 1655 loss_train: 2.3191 acc_train: 0.1412 loss_val: 2.3111 acc_val: 0.1521 time: 0.4210s\n",
      "Epoch: 1656 loss_train: 2.3259 acc_train: 0.1371 loss_val: 2.3114 acc_val: 0.1521 time: 0.4311s\n",
      "Epoch: 1657 loss_train: 2.3143 acc_train: 0.1438 loss_val: 2.3115 acc_val: 0.1521 time: 0.3859s\n",
      "Epoch: 1658 loss_train: 2.3204 acc_train: 0.1356 loss_val: 2.3119 acc_val: 0.1521 time: 0.4092s\n",
      "Epoch: 1659 loss_train: 2.3196 acc_train: 0.1456 loss_val: 2.3120 acc_val: 0.1510 time: 0.4032s\n",
      "Epoch: 1660 loss_train: 2.3185 acc_train: 0.1451 loss_val: 2.3121 acc_val: 0.1510 time: 0.4305s\n",
      "Epoch: 1661 loss_train: 2.3142 acc_train: 0.1478 loss_val: 2.3120 acc_val: 0.1551 time: 0.4527s\n",
      "Epoch: 1662 loss_train: 2.3212 acc_train: 0.1412 loss_val: 2.3119 acc_val: 0.1521 time: 0.4462s\n",
      "Epoch: 1663 loss_train: 2.3212 acc_train: 0.1466 loss_val: 2.3116 acc_val: 0.1521 time: 0.4338s\n",
      "Epoch: 1664 loss_train: 2.3209 acc_train: 0.1424 loss_val: 2.3116 acc_val: 0.1521 time: 0.4586s\n",
      "Epoch: 1665 loss_train: 2.3233 acc_train: 0.1425 loss_val: 2.3116 acc_val: 0.1536 time: 0.4431s\n",
      "Epoch: 1666 loss_train: 2.3184 acc_train: 0.1446 loss_val: 2.3117 acc_val: 0.1526 time: 0.4890s\n",
      "Epoch: 1667 loss_train: 2.3188 acc_train: 0.1404 loss_val: 2.3118 acc_val: 0.1526 time: 0.4546s\n",
      "Epoch: 1668 loss_train: 2.3203 acc_train: 0.1440 loss_val: 2.3120 acc_val: 0.1526 time: 0.4379s\n",
      "Epoch: 1669 loss_train: 2.3182 acc_train: 0.1322 loss_val: 2.3122 acc_val: 0.1521 time: 0.4521s\n",
      "Epoch: 1670 loss_train: 2.3194 acc_train: 0.1409 loss_val: 2.3123 acc_val: 0.1521 time: 0.4168s\n",
      "Epoch: 1671 loss_train: 2.3198 acc_train: 0.1468 loss_val: 2.3124 acc_val: 0.1510 time: 0.3932s\n",
      "Epoch: 1672 loss_train: 2.3154 acc_train: 0.1393 loss_val: 2.3124 acc_val: 0.1521 time: 0.3941s\n",
      "Epoch: 1673 loss_train: 2.3178 acc_train: 0.1412 loss_val: 2.3125 acc_val: 0.1526 time: 0.4007s\n",
      "Epoch: 1674 loss_train: 2.3212 acc_train: 0.1409 loss_val: 2.3126 acc_val: 0.1521 time: 0.4176s\n",
      "Epoch: 1675 loss_train: 2.3187 acc_train: 0.1428 loss_val: 2.3125 acc_val: 0.1521 time: 0.4095s\n",
      "Epoch: 1676 loss_train: 2.3171 acc_train: 0.1554 loss_val: 2.3121 acc_val: 0.1521 time: 0.4264s\n",
      "Epoch: 1677 loss_train: 2.3181 acc_train: 0.1473 loss_val: 2.3117 acc_val: 0.1521 time: 0.4204s\n",
      "Epoch: 1678 loss_train: 2.3218 acc_train: 0.1516 loss_val: 2.3114 acc_val: 0.1516 time: 0.4044s\n",
      "Epoch: 1679 loss_train: 2.3219 acc_train: 0.1443 loss_val: 2.3111 acc_val: 0.1521 time: 0.4450s\n",
      "Epoch: 1680 loss_train: 2.3151 acc_train: 0.1479 loss_val: 2.3109 acc_val: 0.1521 time: 0.3881s\n",
      "Epoch: 1681 loss_train: 2.3199 acc_train: 0.1412 loss_val: 2.3110 acc_val: 0.1531 time: 0.4000s\n",
      "Epoch: 1682 loss_train: 2.3201 acc_train: 0.1406 loss_val: 2.3113 acc_val: 0.1526 time: 0.4000s\n",
      "Epoch: 1683 loss_train: 2.3235 acc_train: 0.1394 loss_val: 2.3115 acc_val: 0.1526 time: 0.3605s\n",
      "Epoch: 1684 loss_train: 2.3195 acc_train: 0.1482 loss_val: 2.3118 acc_val: 0.1521 time: 0.3873s\n",
      "Epoch: 1685 loss_train: 2.3216 acc_train: 0.1425 loss_val: 2.3120 acc_val: 0.1521 time: 0.3785s\n",
      "Epoch: 1686 loss_train: 2.3194 acc_train: 0.1475 loss_val: 2.3122 acc_val: 0.1521 time: 0.3990s\n",
      "Epoch: 1687 loss_train: 2.3148 acc_train: 0.1415 loss_val: 2.3122 acc_val: 0.1526 time: 0.3785s\n",
      "Epoch: 1688 loss_train: 2.3193 acc_train: 0.1475 loss_val: 2.3120 acc_val: 0.1521 time: 0.3819s\n",
      "Epoch: 1689 loss_train: 2.3194 acc_train: 0.1438 loss_val: 2.3118 acc_val: 0.1521 time: 0.3682s\n",
      "Epoch: 1690 loss_train: 2.3225 acc_train: 0.1431 loss_val: 2.3117 acc_val: 0.1516 time: 0.3957s\n",
      "Epoch: 1691 loss_train: 2.3215 acc_train: 0.1465 loss_val: 2.3116 acc_val: 0.1526 time: 0.3851s\n",
      "Epoch: 1692 loss_train: 2.3235 acc_train: 0.1476 loss_val: 2.3115 acc_val: 0.1526 time: 0.3984s\n",
      "Epoch: 1693 loss_train: 2.3233 acc_train: 0.1390 loss_val: 2.3113 acc_val: 0.1510 time: 0.3928s\n",
      "Epoch: 1694 loss_train: 2.3227 acc_train: 0.1378 loss_val: 2.3112 acc_val: 0.1510 time: 0.4087s\n",
      "Epoch: 1695 loss_train: 2.3155 acc_train: 0.1412 loss_val: 2.3110 acc_val: 0.1516 time: 0.4435s\n",
      "Epoch: 1696 loss_train: 2.3169 acc_train: 0.1403 loss_val: 2.3109 acc_val: 0.1516 time: 0.4109s\n",
      "Epoch: 1697 loss_train: 2.3175 acc_train: 0.1485 loss_val: 2.3109 acc_val: 0.1510 time: 0.4206s\n",
      "Epoch: 1698 loss_train: 2.3190 acc_train: 0.1422 loss_val: 2.3110 acc_val: 0.1526 time: 0.4034s\n",
      "Epoch: 1699 loss_train: 2.3163 acc_train: 0.1416 loss_val: 2.3113 acc_val: 0.1531 time: 0.4487s\n",
      "Epoch: 1700 loss_train: 2.3206 acc_train: 0.1384 loss_val: 2.3116 acc_val: 0.1536 time: 0.4093s\n",
      "Epoch: 1701 loss_train: 2.3200 acc_train: 0.1426 loss_val: 2.3117 acc_val: 0.1531 time: 0.4233s\n",
      "Epoch: 1702 loss_train: 2.3180 acc_train: 0.1446 loss_val: 2.3119 acc_val: 0.1521 time: 0.4211s\n",
      "Epoch: 1703 loss_train: 2.3218 acc_train: 0.1438 loss_val: 2.3121 acc_val: 0.1521 time: 0.4377s\n",
      "Epoch: 1704 loss_train: 2.3209 acc_train: 0.1478 loss_val: 2.3121 acc_val: 0.1521 time: 0.3961s\n",
      "Epoch: 1705 loss_train: 2.3134 acc_train: 0.1463 loss_val: 2.3121 acc_val: 0.1521 time: 0.3927s\n",
      "Epoch: 1706 loss_train: 2.3163 acc_train: 0.1451 loss_val: 2.3120 acc_val: 0.1521 time: 0.4127s\n",
      "Epoch: 1707 loss_train: 2.3190 acc_train: 0.1485 loss_val: 2.3119 acc_val: 0.1521 time: 0.4145s\n",
      "Epoch: 1708 loss_train: 2.3177 acc_train: 0.1471 loss_val: 2.3120 acc_val: 0.1531 time: 0.4319s\n",
      "Epoch: 1709 loss_train: 2.3165 acc_train: 0.1523 loss_val: 2.3121 acc_val: 0.1526 time: 0.4197s\n",
      "Epoch: 1710 loss_train: 2.3166 acc_train: 0.1446 loss_val: 2.3123 acc_val: 0.1536 time: 0.4095s\n",
      "Epoch: 1711 loss_train: 2.3123 acc_train: 0.1438 loss_val: 2.3125 acc_val: 0.1521 time: 0.4034s\n",
      "Epoch: 1712 loss_train: 2.3174 acc_train: 0.1434 loss_val: 2.3127 acc_val: 0.1531 time: 0.4157s\n",
      "Epoch: 1713 loss_train: 2.3207 acc_train: 0.1517 loss_val: 2.3128 acc_val: 0.1521 time: 0.3886s\n",
      "Epoch: 1714 loss_train: 2.3147 acc_train: 0.1498 loss_val: 2.3129 acc_val: 0.1521 time: 0.3907s\n",
      "Epoch: 1715 loss_train: 2.3220 acc_train: 0.1407 loss_val: 2.3128 acc_val: 0.1521 time: 0.3826s\n",
      "Epoch: 1716 loss_train: 2.3163 acc_train: 0.1441 loss_val: 2.3128 acc_val: 0.1521 time: 0.4187s\n",
      "Epoch: 1717 loss_train: 2.3157 acc_train: 0.1538 loss_val: 2.3128 acc_val: 0.1521 time: 0.4390s\n",
      "Epoch: 1718 loss_train: 2.3206 acc_train: 0.1380 loss_val: 2.3126 acc_val: 0.1531 time: 0.3724s\n",
      "Epoch: 1719 loss_train: 2.3201 acc_train: 0.1418 loss_val: 2.3124 acc_val: 0.1536 time: 0.4150s\n",
      "Epoch: 1720 loss_train: 2.3176 acc_train: 0.1493 loss_val: 2.3120 acc_val: 0.1531 time: 0.3879s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1721 loss_train: 2.3179 acc_train: 0.1456 loss_val: 2.3116 acc_val: 0.1531 time: 0.3936s\n",
      "Epoch: 1722 loss_train: 2.3169 acc_train: 0.1551 loss_val: 2.3113 acc_val: 0.1510 time: 0.3983s\n",
      "Epoch: 1723 loss_train: 2.3249 acc_train: 0.1325 loss_val: 2.3111 acc_val: 0.1510 time: 0.3913s\n",
      "Epoch: 1724 loss_train: 2.3216 acc_train: 0.1363 loss_val: 2.3110 acc_val: 0.1510 time: 0.3998s\n",
      "Epoch: 1725 loss_train: 2.3239 acc_train: 0.1450 loss_val: 2.3111 acc_val: 0.1510 time: 0.3917s\n",
      "Epoch: 1726 loss_train: 2.3205 acc_train: 0.1381 loss_val: 2.3114 acc_val: 0.1510 time: 0.3790s\n",
      "Epoch: 1727 loss_train: 2.3186 acc_train: 0.1473 loss_val: 2.3113 acc_val: 0.1510 time: 0.3879s\n",
      "Epoch: 1728 loss_train: 2.3197 acc_train: 0.1462 loss_val: 2.3113 acc_val: 0.1516 time: 0.3956s\n",
      "Epoch: 1729 loss_train: 2.3155 acc_train: 0.1444 loss_val: 2.3114 acc_val: 0.1521 time: 0.3821s\n",
      "Epoch: 1730 loss_train: 2.3221 acc_train: 0.1390 loss_val: 2.3117 acc_val: 0.1516 time: 0.3940s\n",
      "Epoch: 1731 loss_train: 2.3173 acc_train: 0.1485 loss_val: 2.3119 acc_val: 0.1505 time: 0.4065s\n",
      "Epoch: 1732 loss_train: 2.3226 acc_train: 0.1472 loss_val: 2.3119 acc_val: 0.1516 time: 0.4301s\n",
      "Epoch: 1733 loss_train: 2.3159 acc_train: 0.1359 loss_val: 2.3119 acc_val: 0.1521 time: 0.4268s\n",
      "Epoch: 1734 loss_train: 2.3145 acc_train: 0.1515 loss_val: 2.3117 acc_val: 0.1526 time: 0.3983s\n",
      "Epoch: 1735 loss_train: 2.3191 acc_train: 0.1424 loss_val: 2.3114 acc_val: 0.1521 time: 0.3712s\n",
      "Epoch: 1736 loss_train: 2.3215 acc_train: 0.1454 loss_val: 2.3111 acc_val: 0.1536 time: 0.4173s\n",
      "Epoch: 1737 loss_train: 2.3233 acc_train: 0.1428 loss_val: 2.3109 acc_val: 0.1536 time: 0.4053s\n",
      "Epoch: 1738 loss_train: 2.3157 acc_train: 0.1475 loss_val: 2.3108 acc_val: 0.1526 time: 0.3844s\n",
      "Epoch: 1739 loss_train: 2.3227 acc_train: 0.1450 loss_val: 2.3107 acc_val: 0.1531 time: 0.4209s\n",
      "Epoch: 1740 loss_train: 2.3177 acc_train: 0.1563 loss_val: 2.3106 acc_val: 0.1521 time: 0.4068s\n",
      "Epoch: 1741 loss_train: 2.3162 acc_train: 0.1381 loss_val: 2.3105 acc_val: 0.1505 time: 0.3994s\n",
      "Epoch: 1742 loss_train: 2.3171 acc_train: 0.1469 loss_val: 2.3104 acc_val: 0.1500 time: 0.4046s\n",
      "Epoch: 1743 loss_train: 2.3153 acc_train: 0.1490 loss_val: 2.3105 acc_val: 0.1505 time: 0.4226s\n",
      "Epoch: 1744 loss_train: 2.3181 acc_train: 0.1481 loss_val: 2.3106 acc_val: 0.1521 time: 0.3847s\n",
      "Epoch: 1745 loss_train: 2.3166 acc_train: 0.1385 loss_val: 2.3106 acc_val: 0.1510 time: 0.4002s\n",
      "Epoch: 1746 loss_train: 2.3201 acc_train: 0.1503 loss_val: 2.3106 acc_val: 0.1521 time: 0.4364s\n",
      "Epoch: 1747 loss_train: 2.3192 acc_train: 0.1466 loss_val: 2.3107 acc_val: 0.1531 time: 0.4103s\n",
      "Epoch: 1748 loss_train: 2.3156 acc_train: 0.1462 loss_val: 2.3107 acc_val: 0.1531 time: 0.3883s\n",
      "Epoch: 1749 loss_train: 2.3218 acc_train: 0.1466 loss_val: 2.3108 acc_val: 0.1521 time: 0.3867s\n",
      "Epoch: 1750 loss_train: 2.3190 acc_train: 0.1399 loss_val: 2.3111 acc_val: 0.1526 time: 0.4196s\n",
      "Epoch: 1751 loss_train: 2.3189 acc_train: 0.1403 loss_val: 2.3114 acc_val: 0.1531 time: 0.3860s\n",
      "Epoch: 1752 loss_train: 2.3200 acc_train: 0.1391 loss_val: 2.3116 acc_val: 0.1516 time: 0.3955s\n",
      "Epoch: 1753 loss_train: 2.3149 acc_train: 0.1460 loss_val: 2.3119 acc_val: 0.1531 time: 0.3904s\n",
      "Epoch: 1754 loss_train: 2.3244 acc_train: 0.1418 loss_val: 2.3123 acc_val: 0.1526 time: 0.3778s\n",
      "Epoch: 1755 loss_train: 2.3165 acc_train: 0.1457 loss_val: 2.3126 acc_val: 0.1526 time: 0.3865s\n",
      "Epoch: 1756 loss_train: 2.3143 acc_train: 0.1456 loss_val: 2.3127 acc_val: 0.1516 time: 0.4253s\n",
      "Epoch: 1757 loss_train: 2.3158 acc_train: 0.1522 loss_val: 2.3126 acc_val: 0.1521 time: 0.4325s\n",
      "Epoch: 1758 loss_train: 2.3244 acc_train: 0.1465 loss_val: 2.3122 acc_val: 0.1521 time: 0.3923s\n",
      "Epoch: 1759 loss_train: 2.3232 acc_train: 0.1374 loss_val: 2.3117 acc_val: 0.1521 time: 0.3907s\n",
      "Epoch: 1760 loss_train: 2.3167 acc_train: 0.1547 loss_val: 2.3114 acc_val: 0.1536 time: 0.4325s\n",
      "Epoch: 1761 loss_train: 2.3213 acc_train: 0.1290 loss_val: 2.3111 acc_val: 0.1546 time: 0.3717s\n",
      "Epoch: 1762 loss_train: 2.3148 acc_train: 0.1406 loss_val: 2.3109 acc_val: 0.1526 time: 0.4007s\n",
      "Epoch: 1763 loss_train: 2.3184 acc_train: 0.1493 loss_val: 2.3109 acc_val: 0.1521 time: 0.3934s\n",
      "Epoch: 1764 loss_train: 2.3187 acc_train: 0.1510 loss_val: 2.3109 acc_val: 0.1521 time: 0.4096s\n",
      "Epoch: 1765 loss_train: 2.3150 acc_train: 0.1369 loss_val: 2.3110 acc_val: 0.1521 time: 0.4138s\n",
      "Epoch: 1766 loss_train: 2.3194 acc_train: 0.1513 loss_val: 2.3111 acc_val: 0.1505 time: 0.4233s\n",
      "Epoch: 1767 loss_train: 2.3159 acc_train: 0.1481 loss_val: 2.3111 acc_val: 0.1516 time: 0.3971s\n",
      "Epoch: 1768 loss_train: 2.3206 acc_train: 0.1384 loss_val: 2.3112 acc_val: 0.1505 time: 0.4010s\n",
      "Epoch: 1769 loss_train: 2.3189 acc_train: 0.1438 loss_val: 2.3114 acc_val: 0.1516 time: 0.4189s\n",
      "Epoch: 1770 loss_train: 2.3261 acc_train: 0.1507 loss_val: 2.3116 acc_val: 0.1521 time: 0.4403s\n",
      "Epoch: 1771 loss_train: 2.3158 acc_train: 0.1446 loss_val: 2.3119 acc_val: 0.1516 time: 0.3980s\n",
      "Epoch: 1772 loss_train: 2.3139 acc_train: 0.1472 loss_val: 2.3119 acc_val: 0.1521 time: 0.4451s\n",
      "Epoch: 1773 loss_train: 2.3208 acc_train: 0.1419 loss_val: 2.3119 acc_val: 0.1531 time: 0.3486s\n",
      "Epoch: 1774 loss_train: 2.3187 acc_train: 0.1393 loss_val: 2.3119 acc_val: 0.1531 time: 0.4099s\n",
      "Epoch: 1775 loss_train: 2.3189 acc_train: 0.1390 loss_val: 2.3119 acc_val: 0.1521 time: 0.4241s\n",
      "Epoch: 1776 loss_train: 2.3171 acc_train: 0.1468 loss_val: 2.3119 acc_val: 0.1521 time: 0.4195s\n",
      "Epoch: 1777 loss_train: 2.3183 acc_train: 0.1366 loss_val: 2.3119 acc_val: 0.1531 time: 0.3993s\n",
      "Epoch: 1778 loss_train: 2.3220 acc_train: 0.1459 loss_val: 2.3119 acc_val: 0.1521 time: 0.3751s\n",
      "Epoch: 1779 loss_train: 2.3182 acc_train: 0.1387 loss_val: 2.3120 acc_val: 0.1516 time: 0.4126s\n",
      "Epoch: 1780 loss_train: 2.3195 acc_train: 0.1362 loss_val: 2.3121 acc_val: 0.1505 time: 0.4214s\n",
      "Epoch: 1781 loss_train: 2.3170 acc_train: 0.1380 loss_val: 2.3122 acc_val: 0.1521 time: 0.4251s\n",
      "Epoch: 1782 loss_train: 2.3241 acc_train: 0.1463 loss_val: 2.3124 acc_val: 0.1521 time: 0.3928s\n",
      "Epoch: 1783 loss_train: 2.3181 acc_train: 0.1437 loss_val: 2.3125 acc_val: 0.1531 time: 0.4183s\n",
      "Epoch: 1784 loss_train: 2.3172 acc_train: 0.1509 loss_val: 2.3126 acc_val: 0.1536 time: 0.3950s\n",
      "Epoch: 1785 loss_train: 2.3152 acc_train: 0.1402 loss_val: 2.3127 acc_val: 0.1536 time: 0.3749s\n",
      "Epoch: 1786 loss_train: 2.3187 acc_train: 0.1490 loss_val: 2.3127 acc_val: 0.1536 time: 0.4203s\n",
      "Epoch: 1787 loss_train: 2.3158 acc_train: 0.1393 loss_val: 2.3124 acc_val: 0.1536 time: 0.4101s\n",
      "Epoch: 1788 loss_train: 2.3157 acc_train: 0.1473 loss_val: 2.3121 acc_val: 0.1516 time: 0.4188s\n",
      "Epoch: 1789 loss_train: 2.3198 acc_train: 0.1448 loss_val: 2.3117 acc_val: 0.1516 time: 0.4010s\n",
      "Epoch: 1790 loss_train: 2.3232 acc_train: 0.1334 loss_val: 2.3112 acc_val: 0.1510 time: 0.4100s\n",
      "Epoch: 1791 loss_train: 2.3236 acc_train: 0.1482 loss_val: 2.3108 acc_val: 0.1546 time: 0.4225s\n",
      "Epoch: 1792 loss_train: 2.3170 acc_train: 0.1460 loss_val: 2.3106 acc_val: 0.1510 time: 0.3735s\n",
      "Epoch: 1793 loss_train: 2.3194 acc_train: 0.1456 loss_val: 2.3104 acc_val: 0.1521 time: 0.3775s\n",
      "Epoch: 1794 loss_train: 2.3216 acc_train: 0.1362 loss_val: 2.3106 acc_val: 0.1526 time: 0.3903s\n",
      "Epoch: 1795 loss_train: 2.3194 acc_train: 0.1429 loss_val: 2.3111 acc_val: 0.1531 time: 0.4257s\n",
      "Epoch: 1796 loss_train: 2.3189 acc_train: 0.1456 loss_val: 2.3117 acc_val: 0.1521 time: 0.3967s\n",
      "Epoch: 1797 loss_train: 2.3184 acc_train: 0.1429 loss_val: 2.3125 acc_val: 0.1510 time: 0.3859s\n",
      "Epoch: 1798 loss_train: 2.3177 acc_train: 0.1503 loss_val: 2.3130 acc_val: 0.1536 time: 0.3885s\n",
      "Epoch: 1799 loss_train: 2.3225 acc_train: 0.1484 loss_val: 2.3135 acc_val: 0.1536 time: 0.3881s\n",
      "Epoch: 1800 loss_train: 2.3148 acc_train: 0.1539 loss_val: 2.3138 acc_val: 0.1480 time: 0.3980s\n",
      "Epoch: 1801 loss_train: 2.3159 acc_train: 0.1446 loss_val: 2.3138 acc_val: 0.1470 time: 0.4013s\n",
      "Epoch: 1802 loss_train: 2.3187 acc_train: 0.1497 loss_val: 2.3136 acc_val: 0.1490 time: 0.4021s\n",
      "Epoch: 1803 loss_train: 2.3210 acc_train: 0.1377 loss_val: 2.3133 acc_val: 0.1505 time: 0.3779s\n",
      "Epoch: 1804 loss_train: 2.3235 acc_train: 0.1385 loss_val: 2.3129 acc_val: 0.1526 time: 0.4275s\n",
      "Epoch: 1805 loss_train: 2.3238 acc_train: 0.1428 loss_val: 2.3123 acc_val: 0.1521 time: 0.4195s\n",
      "Epoch: 1806 loss_train: 2.3211 acc_train: 0.1522 loss_val: 2.3116 acc_val: 0.1526 time: 0.3866s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1807 loss_train: 2.3163 acc_train: 0.1498 loss_val: 2.3111 acc_val: 0.1526 time: 0.4082s\n",
      "Epoch: 1808 loss_train: 2.3224 acc_train: 0.1416 loss_val: 2.3107 acc_val: 0.1536 time: 0.4210s\n",
      "Epoch: 1809 loss_train: 2.3194 acc_train: 0.1478 loss_val: 2.3104 acc_val: 0.1526 time: 0.3841s\n",
      "Epoch: 1810 loss_train: 2.3178 acc_train: 0.1481 loss_val: 2.3102 acc_val: 0.1510 time: 0.3866s\n",
      "Epoch: 1811 loss_train: 2.3173 acc_train: 0.1525 loss_val: 2.3103 acc_val: 0.1516 time: 0.4170s\n",
      "Epoch: 1812 loss_train: 2.3209 acc_train: 0.1412 loss_val: 2.3106 acc_val: 0.1521 time: 0.4214s\n",
      "Epoch: 1813 loss_train: 2.3224 acc_train: 0.1472 loss_val: 2.3113 acc_val: 0.1516 time: 0.3973s\n",
      "Epoch: 1814 loss_train: 2.3185 acc_train: 0.1491 loss_val: 2.3121 acc_val: 0.1531 time: 0.3979s\n",
      "Epoch: 1815 loss_train: 2.3160 acc_train: 0.1400 loss_val: 2.3130 acc_val: 0.1516 time: 0.3992s\n",
      "Epoch: 1816 loss_train: 2.3173 acc_train: 0.1422 loss_val: 2.3138 acc_val: 0.1505 time: 0.3817s\n",
      "Epoch: 1817 loss_train: 2.3190 acc_train: 0.1484 loss_val: 2.3144 acc_val: 0.1531 time: 0.3822s\n",
      "Epoch: 1818 loss_train: 2.3219 acc_train: 0.1403 loss_val: 2.3143 acc_val: 0.1521 time: 0.3851s\n",
      "Epoch: 1819 loss_train: 2.3197 acc_train: 0.1451 loss_val: 2.3140 acc_val: 0.1521 time: 0.4105s\n",
      "Epoch: 1820 loss_train: 2.3205 acc_train: 0.1412 loss_val: 2.3133 acc_val: 0.1510 time: 0.4070s\n",
      "Epoch: 1821 loss_train: 2.3169 acc_train: 0.1488 loss_val: 2.3127 acc_val: 0.1510 time: 0.3760s\n",
      "Epoch: 1822 loss_train: 2.3221 acc_train: 0.1456 loss_val: 2.3120 acc_val: 0.1526 time: 0.4098s\n",
      "Epoch: 1823 loss_train: 2.3185 acc_train: 0.1520 loss_val: 2.3114 acc_val: 0.1521 time: 0.4051s\n",
      "Epoch: 1824 loss_train: 2.3185 acc_train: 0.1431 loss_val: 2.3110 acc_val: 0.1521 time: 0.3934s\n",
      "Epoch: 1825 loss_train: 2.3185 acc_train: 0.1413 loss_val: 2.3108 acc_val: 0.1531 time: 0.3983s\n",
      "Epoch: 1826 loss_train: 2.3151 acc_train: 0.1337 loss_val: 2.3107 acc_val: 0.1521 time: 0.3894s\n",
      "Epoch: 1827 loss_train: 2.3183 acc_train: 0.1473 loss_val: 2.3107 acc_val: 0.1505 time: 0.3940s\n",
      "Epoch: 1828 loss_train: 2.3201 acc_train: 0.1424 loss_val: 2.3110 acc_val: 0.1526 time: 0.3956s\n",
      "Epoch: 1829 loss_train: 2.3159 acc_train: 0.1500 loss_val: 2.3114 acc_val: 0.1510 time: 0.4255s\n",
      "Epoch: 1830 loss_train: 2.3122 acc_train: 0.1493 loss_val: 2.3118 acc_val: 0.1521 time: 0.3806s\n",
      "Epoch: 1831 loss_train: 2.3238 acc_train: 0.1434 loss_val: 2.3120 acc_val: 0.1516 time: 0.3733s\n",
      "Epoch: 1832 loss_train: 2.3166 acc_train: 0.1447 loss_val: 2.3123 acc_val: 0.1521 time: 0.3867s\n",
      "Epoch: 1833 loss_train: 2.3193 acc_train: 0.1456 loss_val: 2.3123 acc_val: 0.1526 time: 0.3754s\n",
      "Epoch: 1834 loss_train: 2.3195 acc_train: 0.1500 loss_val: 2.3122 acc_val: 0.1531 time: 0.3903s\n",
      "Epoch: 1835 loss_train: 2.3169 acc_train: 0.1388 loss_val: 2.3120 acc_val: 0.1521 time: 0.4044s\n",
      "Epoch: 1836 loss_train: 2.3197 acc_train: 0.1394 loss_val: 2.3119 acc_val: 0.1510 time: 0.4133s\n",
      "Epoch: 1837 loss_train: 2.3202 acc_train: 0.1385 loss_val: 2.3119 acc_val: 0.1510 time: 0.4140s\n",
      "Epoch: 1838 loss_train: 2.3223 acc_train: 0.1479 loss_val: 2.3116 acc_val: 0.1516 time: 0.3744s\n",
      "Epoch: 1839 loss_train: 2.3202 acc_train: 0.1390 loss_val: 2.3114 acc_val: 0.1521 time: 0.3830s\n",
      "Epoch: 1840 loss_train: 2.3234 acc_train: 0.1406 loss_val: 2.3112 acc_val: 0.1521 time: 0.3934s\n",
      "Epoch: 1841 loss_train: 2.3182 acc_train: 0.1347 loss_val: 2.3113 acc_val: 0.1521 time: 0.4040s\n",
      "Epoch: 1842 loss_train: 2.3183 acc_train: 0.1393 loss_val: 2.3111 acc_val: 0.1526 time: 0.3808s\n",
      "Epoch: 1843 loss_train: 2.3187 acc_train: 0.1406 loss_val: 2.3109 acc_val: 0.1516 time: 0.3880s\n",
      "Epoch: 1844 loss_train: 2.3193 acc_train: 0.1437 loss_val: 2.3110 acc_val: 0.1531 time: 0.3659s\n",
      "Epoch: 1845 loss_train: 2.3169 acc_train: 0.1402 loss_val: 2.3112 acc_val: 0.1541 time: 0.3614s\n",
      "Epoch: 1846 loss_train: 2.3135 acc_train: 0.1494 loss_val: 2.3115 acc_val: 0.1531 time: 0.3524s\n",
      "Epoch: 1847 loss_train: 2.3195 acc_train: 0.1426 loss_val: 2.3120 acc_val: 0.1500 time: 0.4638s\n",
      "Epoch: 1848 loss_train: 2.3167 acc_train: 0.1479 loss_val: 2.3123 acc_val: 0.1505 time: 0.4242s\n",
      "Epoch: 1849 loss_train: 2.3257 acc_train: 0.1434 loss_val: 2.3126 acc_val: 0.1541 time: 0.4002s\n",
      "Epoch: 1850 loss_train: 2.3215 acc_train: 0.1453 loss_val: 2.3126 acc_val: 0.1521 time: 0.3811s\n",
      "Epoch: 1851 loss_train: 2.3209 acc_train: 0.1475 loss_val: 2.3127 acc_val: 0.1521 time: 0.3723s\n",
      "Epoch: 1852 loss_train: 2.3156 acc_train: 0.1440 loss_val: 2.3125 acc_val: 0.1505 time: 0.4151s\n",
      "Epoch: 1853 loss_train: 2.3175 acc_train: 0.1440 loss_val: 2.3123 acc_val: 0.1505 time: 0.3981s\n",
      "Epoch: 1854 loss_train: 2.3206 acc_train: 0.1343 loss_val: 2.3119 acc_val: 0.1505 time: 0.4014s\n",
      "Epoch: 1855 loss_train: 2.3198 acc_train: 0.1369 loss_val: 2.3115 acc_val: 0.1500 time: 0.4075s\n",
      "Epoch: 1856 loss_train: 2.3217 acc_train: 0.1446 loss_val: 2.3112 acc_val: 0.1521 time: 0.3937s\n",
      "Epoch: 1857 loss_train: 2.3212 acc_train: 0.1422 loss_val: 2.3109 acc_val: 0.1541 time: 0.3724s\n",
      "Epoch: 1858 loss_train: 2.3229 acc_train: 0.1432 loss_val: 2.3107 acc_val: 0.1516 time: 0.3812s\n",
      "Epoch: 1859 loss_train: 2.3122 acc_train: 0.1538 loss_val: 2.3107 acc_val: 0.1536 time: 0.3731s\n",
      "Epoch: 1860 loss_train: 2.3211 acc_train: 0.1465 loss_val: 2.3108 acc_val: 0.1510 time: 0.3883s\n",
      "Epoch: 1861 loss_train: 2.3131 acc_train: 0.1429 loss_val: 2.3110 acc_val: 0.1536 time: 0.4223s\n",
      "Epoch: 1862 loss_train: 2.3221 acc_train: 0.1434 loss_val: 2.3111 acc_val: 0.1516 time: 0.4058s\n",
      "Epoch: 1863 loss_train: 2.3179 acc_train: 0.1503 loss_val: 2.3110 acc_val: 0.1526 time: 0.3693s\n",
      "Epoch: 1864 loss_train: 2.3204 acc_train: 0.1497 loss_val: 2.3109 acc_val: 0.1510 time: 0.4091s\n",
      "Epoch: 1865 loss_train: 2.3230 acc_train: 0.1366 loss_val: 2.3107 acc_val: 0.1495 time: 0.4077s\n",
      "Epoch: 1866 loss_train: 2.3186 acc_train: 0.1432 loss_val: 2.3107 acc_val: 0.1505 time: 0.4030s\n",
      "Epoch: 1867 loss_train: 2.3239 acc_train: 0.1357 loss_val: 2.3108 acc_val: 0.1510 time: 0.3857s\n",
      "Epoch: 1868 loss_train: 2.3201 acc_train: 0.1412 loss_val: 2.3110 acc_val: 0.1521 time: 0.3969s\n",
      "Epoch: 1869 loss_train: 2.3167 acc_train: 0.1377 loss_val: 2.3112 acc_val: 0.1541 time: 0.3881s\n",
      "Epoch: 1870 loss_train: 2.3201 acc_train: 0.1365 loss_val: 2.3113 acc_val: 0.1536 time: 0.3999s\n",
      "Epoch: 1871 loss_train: 2.3151 acc_train: 0.1407 loss_val: 2.3113 acc_val: 0.1531 time: 0.3910s\n",
      "Epoch: 1872 loss_train: 2.3160 acc_train: 0.1485 loss_val: 2.3114 acc_val: 0.1536 time: 0.3868s\n",
      "Epoch: 1873 loss_train: 2.3209 acc_train: 0.1365 loss_val: 2.3117 acc_val: 0.1526 time: 0.4038s\n",
      "Epoch: 1874 loss_train: 2.3144 acc_train: 0.1490 loss_val: 2.3121 acc_val: 0.1521 time: 0.3992s\n",
      "Epoch: 1875 loss_train: 2.3172 acc_train: 0.1541 loss_val: 2.3125 acc_val: 0.1516 time: 0.3919s\n",
      "Epoch: 1876 loss_train: 2.3184 acc_train: 0.1466 loss_val: 2.3129 acc_val: 0.1531 time: 0.3936s\n",
      "Epoch: 1877 loss_train: 2.3186 acc_train: 0.1482 loss_val: 2.3131 acc_val: 0.1505 time: 0.4064s\n",
      "Epoch: 1878 loss_train: 2.3159 acc_train: 0.1510 loss_val: 2.3130 acc_val: 0.1521 time: 0.4145s\n",
      "Epoch: 1879 loss_train: 2.3149 acc_train: 0.1472 loss_val: 2.3125 acc_val: 0.1521 time: 0.4318s\n",
      "Epoch: 1880 loss_train: 2.3257 acc_train: 0.1356 loss_val: 2.3119 acc_val: 0.1510 time: 0.4499s\n",
      "Epoch: 1881 loss_train: 2.3242 acc_train: 0.1380 loss_val: 2.3111 acc_val: 0.1510 time: 0.4068s\n",
      "Epoch: 1882 loss_train: 2.3168 acc_train: 0.1473 loss_val: 2.3105 acc_val: 0.1510 time: 0.3268s\n",
      "Epoch: 1883 loss_train: 2.3197 acc_train: 0.1385 loss_val: 2.3100 acc_val: 0.1510 time: 0.4338s\n",
      "Epoch: 1884 loss_train: 2.3238 acc_train: 0.1466 loss_val: 2.3097 acc_val: 0.1510 time: 0.4133s\n",
      "Epoch: 1885 loss_train: 2.3224 acc_train: 0.1407 loss_val: 2.3097 acc_val: 0.1510 time: 0.3822s\n",
      "Epoch: 1886 loss_train: 2.3198 acc_train: 0.1466 loss_val: 2.3100 acc_val: 0.1521 time: 0.4057s\n",
      "Epoch: 1887 loss_train: 2.3168 acc_train: 0.1504 loss_val: 2.3104 acc_val: 0.1531 time: 0.4158s\n",
      "Epoch: 1888 loss_train: 2.3224 acc_train: 0.1357 loss_val: 2.3107 acc_val: 0.1536 time: 0.3823s\n",
      "Epoch: 1889 loss_train: 2.3188 acc_train: 0.1456 loss_val: 2.3110 acc_val: 0.1521 time: 0.4031s\n",
      "Epoch: 1890 loss_train: 2.3170 acc_train: 0.1532 loss_val: 2.3115 acc_val: 0.1536 time: 0.4093s\n",
      "Epoch: 1891 loss_train: 2.3227 acc_train: 0.1380 loss_val: 2.3120 acc_val: 0.1531 time: 0.4349s\n",
      "Epoch: 1892 loss_train: 2.3166 acc_train: 0.1306 loss_val: 2.3125 acc_val: 0.1505 time: 0.3985s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1893 loss_train: 2.3233 acc_train: 0.1465 loss_val: 2.3128 acc_val: 0.1526 time: 0.4263s\n",
      "Epoch: 1894 loss_train: 2.3180 acc_train: 0.1534 loss_val: 2.3128 acc_val: 0.1510 time: 0.3801s\n",
      "Epoch: 1895 loss_train: 2.3198 acc_train: 0.1346 loss_val: 2.3127 acc_val: 0.1521 time: 0.4015s\n",
      "Epoch: 1896 loss_train: 2.3201 acc_train: 0.1435 loss_val: 2.3125 acc_val: 0.1516 time: 0.3858s\n",
      "Epoch: 1897 loss_train: 2.3221 acc_train: 0.1465 loss_val: 2.3121 acc_val: 0.1526 time: 0.3831s\n",
      "Epoch: 1898 loss_train: 2.3185 acc_train: 0.1465 loss_val: 2.3119 acc_val: 0.1536 time: 0.3850s\n",
      "Epoch: 1899 loss_train: 2.3222 acc_train: 0.1331 loss_val: 2.3117 acc_val: 0.1526 time: 0.3847s\n",
      "Epoch: 1900 loss_train: 2.3270 acc_train: 0.1388 loss_val: 2.3115 acc_val: 0.1536 time: 0.4077s\n",
      "Epoch: 1901 loss_train: 2.3145 acc_train: 0.1353 loss_val: 2.3114 acc_val: 0.1531 time: 0.4085s\n",
      "Epoch: 1902 loss_train: 2.3242 acc_train: 0.1494 loss_val: 2.3115 acc_val: 0.1510 time: 0.3807s\n",
      "Epoch: 1903 loss_train: 2.3157 acc_train: 0.1435 loss_val: 2.3115 acc_val: 0.1516 time: 0.3573s\n",
      "Epoch: 1904 loss_train: 2.3168 acc_train: 0.1512 loss_val: 2.3117 acc_val: 0.1516 time: 0.4166s\n",
      "Epoch: 1905 loss_train: 2.3208 acc_train: 0.1509 loss_val: 2.3119 acc_val: 0.1521 time: 0.3863s\n",
      "Epoch: 1906 loss_train: 2.3199 acc_train: 0.1419 loss_val: 2.3121 acc_val: 0.1521 time: 0.3837s\n",
      "Epoch: 1907 loss_train: 2.3205 acc_train: 0.1421 loss_val: 2.3122 acc_val: 0.1526 time: 0.4145s\n",
      "Epoch: 1908 loss_train: 2.3199 acc_train: 0.1501 loss_val: 2.3123 acc_val: 0.1536 time: 0.3747s\n",
      "Epoch: 1909 loss_train: 2.3209 acc_train: 0.1432 loss_val: 2.3123 acc_val: 0.1536 time: 0.4125s\n",
      "Epoch: 1910 loss_train: 2.3209 acc_train: 0.1394 loss_val: 2.3123 acc_val: 0.1521 time: 0.4076s\n",
      "Epoch: 1911 loss_train: 2.3214 acc_train: 0.1425 loss_val: 2.3122 acc_val: 0.1521 time: 0.4044s\n",
      "Epoch: 1912 loss_train: 2.3162 acc_train: 0.1446 loss_val: 2.3123 acc_val: 0.1526 time: 0.4138s\n",
      "Epoch: 1913 loss_train: 2.3175 acc_train: 0.1440 loss_val: 2.3123 acc_val: 0.1521 time: 0.3952s\n",
      "Epoch: 1914 loss_train: 2.3197 acc_train: 0.1328 loss_val: 2.3123 acc_val: 0.1516 time: 0.4284s\n",
      "Epoch: 1915 loss_train: 2.3229 acc_train: 0.1422 loss_val: 2.3122 acc_val: 0.1521 time: 0.4129s\n",
      "Epoch: 1916 loss_train: 2.3206 acc_train: 0.1459 loss_val: 2.3121 acc_val: 0.1521 time: 0.4603s\n",
      "Epoch: 1917 loss_train: 2.3156 acc_train: 0.1501 loss_val: 2.3119 acc_val: 0.1516 time: 0.3796s\n",
      "Epoch: 1918 loss_train: 2.3212 acc_train: 0.1381 loss_val: 2.3119 acc_val: 0.1510 time: 0.4040s\n",
      "Epoch: 1919 loss_train: 2.3157 acc_train: 0.1515 loss_val: 2.3118 acc_val: 0.1526 time: 0.3884s\n",
      "Epoch: 1920 loss_train: 2.3187 acc_train: 0.1491 loss_val: 2.3117 acc_val: 0.1526 time: 0.3937s\n",
      "Epoch: 1921 loss_train: 2.3179 acc_train: 0.1494 loss_val: 2.3116 acc_val: 0.1526 time: 0.3880s\n",
      "Epoch: 1922 loss_train: 2.3212 acc_train: 0.1462 loss_val: 2.3115 acc_val: 0.1536 time: 0.3682s\n",
      "Epoch: 1923 loss_train: 2.3229 acc_train: 0.1421 loss_val: 2.3114 acc_val: 0.1536 time: 0.3901s\n",
      "Epoch: 1924 loss_train: 2.3135 acc_train: 0.1504 loss_val: 2.3115 acc_val: 0.1531 time: 0.3753s\n",
      "Epoch: 1925 loss_train: 2.3232 acc_train: 0.1473 loss_val: 2.3117 acc_val: 0.1521 time: 0.3725s\n",
      "Epoch: 1926 loss_train: 2.3195 acc_train: 0.1421 loss_val: 2.3119 acc_val: 0.1526 time: 0.3979s\n",
      "Epoch: 1927 loss_train: 2.3240 acc_train: 0.1438 loss_val: 2.3121 acc_val: 0.1526 time: 0.3942s\n",
      "Epoch: 1928 loss_train: 2.3173 acc_train: 0.1428 loss_val: 2.3122 acc_val: 0.1521 time: 0.4230s\n",
      "Epoch: 1929 loss_train: 2.3232 acc_train: 0.1450 loss_val: 2.3124 acc_val: 0.1521 time: 0.4252s\n",
      "Epoch: 1930 loss_train: 2.3185 acc_train: 0.1468 loss_val: 2.3123 acc_val: 0.1516 time: 0.3955s\n",
      "Epoch: 1931 loss_train: 2.3090 acc_train: 0.1556 loss_val: 2.3121 acc_val: 0.1510 time: 0.3943s\n",
      "Epoch: 1932 loss_train: 2.3195 acc_train: 0.1435 loss_val: 2.3118 acc_val: 0.1531 time: 0.3789s\n",
      "Epoch: 1933 loss_train: 2.3192 acc_train: 0.1428 loss_val: 2.3116 acc_val: 0.1536 time: 0.3626s\n",
      "Epoch: 1934 loss_train: 2.3240 acc_train: 0.1328 loss_val: 2.3115 acc_val: 0.1536 time: 0.4102s\n",
      "Epoch: 1935 loss_train: 2.3142 acc_train: 0.1512 loss_val: 2.3117 acc_val: 0.1526 time: 0.3962s\n",
      "Epoch: 1936 loss_train: 2.3160 acc_train: 0.1450 loss_val: 2.3121 acc_val: 0.1531 time: 0.3844s\n",
      "Epoch: 1937 loss_train: 2.3163 acc_train: 0.1517 loss_val: 2.3124 acc_val: 0.1521 time: 0.3929s\n",
      "Epoch: 1938 loss_train: 2.3109 acc_train: 0.1453 loss_val: 2.3127 acc_val: 0.1531 time: 0.3876s\n",
      "Epoch: 1939 loss_train: 2.3181 acc_train: 0.1399 loss_val: 2.3129 acc_val: 0.1505 time: 0.4175s\n",
      "Epoch: 1940 loss_train: 2.3139 acc_train: 0.1498 loss_val: 2.3130 acc_val: 0.1505 time: 0.4079s\n",
      "Epoch: 1941 loss_train: 2.3130 acc_train: 0.1382 loss_val: 2.3131 acc_val: 0.1526 time: 0.3804s\n",
      "Epoch: 1942 loss_train: 2.3190 acc_train: 0.1446 loss_val: 2.3131 acc_val: 0.1521 time: 0.3718s\n",
      "Epoch: 1943 loss_train: 2.3172 acc_train: 0.1371 loss_val: 2.3129 acc_val: 0.1516 time: 0.3810s\n",
      "Epoch: 1944 loss_train: 2.3154 acc_train: 0.1432 loss_val: 2.3128 acc_val: 0.1510 time: 0.3835s\n",
      "Epoch: 1945 loss_train: 2.3132 acc_train: 0.1406 loss_val: 2.3127 acc_val: 0.1531 time: 0.4090s\n",
      "Epoch: 1946 loss_train: 2.3164 acc_train: 0.1412 loss_val: 2.3123 acc_val: 0.1526 time: 0.3752s\n",
      "Epoch: 1947 loss_train: 2.3202 acc_train: 0.1396 loss_val: 2.3119 acc_val: 0.1526 time: 0.3962s\n",
      "Epoch: 1948 loss_train: 2.3098 acc_train: 0.1440 loss_val: 2.3116 acc_val: 0.1536 time: 0.4052s\n",
      "Epoch: 1949 loss_train: 2.3147 acc_train: 0.1465 loss_val: 2.3114 acc_val: 0.1536 time: 0.4045s\n",
      "Epoch: 1950 loss_train: 2.3187 acc_train: 0.1526 loss_val: 2.3112 acc_val: 0.1531 time: 0.3911s\n",
      "Epoch: 1951 loss_train: 2.3193 acc_train: 0.1377 loss_val: 2.3112 acc_val: 0.1536 time: 0.4269s\n",
      "Epoch: 1952 loss_train: 2.3202 acc_train: 0.1384 loss_val: 2.3114 acc_val: 0.1516 time: 0.3809s\n",
      "Epoch: 1953 loss_train: 2.3176 acc_train: 0.1475 loss_val: 2.3119 acc_val: 0.1536 time: 0.4298s\n",
      "Epoch: 1954 loss_train: 2.3178 acc_train: 0.1418 loss_val: 2.3123 acc_val: 0.1526 time: 0.4227s\n",
      "Epoch: 1955 loss_train: 2.3240 acc_train: 0.1318 loss_val: 2.3128 acc_val: 0.1521 time: 0.4138s\n",
      "Epoch: 1956 loss_train: 2.3130 acc_train: 0.1466 loss_val: 2.3130 acc_val: 0.1521 time: 0.4003s\n",
      "Epoch: 1957 loss_train: 2.3189 acc_train: 0.1447 loss_val: 2.3132 acc_val: 0.1521 time: 0.3732s\n",
      "Epoch: 1958 loss_train: 2.3221 acc_train: 0.1415 loss_val: 2.3132 acc_val: 0.1510 time: 0.3874s\n",
      "Epoch: 1959 loss_train: 2.3212 acc_train: 0.1443 loss_val: 2.3128 acc_val: 0.1521 time: 0.4133s\n",
      "Epoch: 1960 loss_train: 2.3186 acc_train: 0.1473 loss_val: 2.3124 acc_val: 0.1510 time: 0.3859s\n",
      "Epoch: 1961 loss_train: 2.3246 acc_train: 0.1377 loss_val: 2.3120 acc_val: 0.1510 time: 0.3892s\n",
      "Epoch: 1962 loss_train: 2.3245 acc_train: 0.1416 loss_val: 2.3116 acc_val: 0.1510 time: 0.3791s\n",
      "Epoch: 1963 loss_train: 2.3174 acc_train: 0.1443 loss_val: 2.3111 acc_val: 0.1521 time: 0.3879s\n",
      "Epoch: 1964 loss_train: 2.3195 acc_train: 0.1437 loss_val: 2.3108 acc_val: 0.1521 time: 0.3945s\n",
      "Epoch: 1965 loss_train: 2.3181 acc_train: 0.1494 loss_val: 2.3107 acc_val: 0.1521 time: 0.4075s\n",
      "Epoch: 1966 loss_train: 2.3196 acc_train: 0.1485 loss_val: 2.3109 acc_val: 0.1536 time: 0.3873s\n",
      "Epoch: 1967 loss_train: 2.3218 acc_train: 0.1440 loss_val: 2.3112 acc_val: 0.1541 time: 0.3796s\n",
      "Epoch: 1968 loss_train: 2.3155 acc_train: 0.1569 loss_val: 2.3115 acc_val: 0.1541 time: 0.3914s\n",
      "Epoch: 1969 loss_train: 2.3164 acc_train: 0.1416 loss_val: 2.3118 acc_val: 0.1536 time: 0.3992s\n",
      "Epoch: 1970 loss_train: 2.3164 acc_train: 0.1516 loss_val: 2.3119 acc_val: 0.1531 time: 0.4117s\n",
      "Epoch: 1971 loss_train: 2.3199 acc_train: 0.1387 loss_val: 2.3120 acc_val: 0.1521 time: 0.4074s\n",
      "Epoch: 1972 loss_train: 2.3190 acc_train: 0.1488 loss_val: 2.3119 acc_val: 0.1521 time: 0.3898s\n",
      "Epoch: 1973 loss_train: 2.3178 acc_train: 0.1468 loss_val: 2.3118 acc_val: 0.1526 time: 0.3957s\n",
      "Epoch: 1974 loss_train: 2.3180 acc_train: 0.1446 loss_val: 2.3117 acc_val: 0.1521 time: 0.3948s\n",
      "Epoch: 1975 loss_train: 2.3231 acc_train: 0.1485 loss_val: 2.3116 acc_val: 0.1510 time: 0.4012s\n",
      "Epoch: 1976 loss_train: 2.3147 acc_train: 0.1475 loss_val: 2.3113 acc_val: 0.1510 time: 0.3759s\n",
      "Epoch: 1977 loss_train: 2.3207 acc_train: 0.1346 loss_val: 2.3111 acc_val: 0.1510 time: 0.3786s\n",
      "Epoch: 1978 loss_train: 2.3197 acc_train: 0.1412 loss_val: 2.3108 acc_val: 0.1510 time: 0.4019s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1979 loss_train: 2.3147 acc_train: 0.1440 loss_val: 2.3106 acc_val: 0.1510 time: 0.4250s\n",
      "Epoch: 1980 loss_train: 2.3191 acc_train: 0.1368 loss_val: 2.3105 acc_val: 0.1521 time: 0.4002s\n",
      "Epoch: 1981 loss_train: 2.3158 acc_train: 0.1381 loss_val: 2.3107 acc_val: 0.1516 time: 0.4218s\n",
      "Epoch: 1982 loss_train: 2.3142 acc_train: 0.1447 loss_val: 2.3111 acc_val: 0.1531 time: 0.3751s\n",
      "Epoch: 1983 loss_train: 2.3084 acc_train: 0.1463 loss_val: 2.3114 acc_val: 0.1536 time: 0.4172s\n",
      "Epoch: 1984 loss_train: 2.3182 acc_train: 0.1374 loss_val: 2.3119 acc_val: 0.1536 time: 0.4236s\n",
      "Epoch: 1985 loss_train: 2.3150 acc_train: 0.1620 loss_val: 2.3123 acc_val: 0.1526 time: 0.4163s\n",
      "Epoch: 1986 loss_train: 2.3189 acc_train: 0.1482 loss_val: 2.3126 acc_val: 0.1546 time: 0.3904s\n",
      "Epoch: 1987 loss_train: 2.3167 acc_train: 0.1437 loss_val: 2.3127 acc_val: 0.1541 time: 0.4235s\n",
      "Epoch: 1988 loss_train: 2.3180 acc_train: 0.1404 loss_val: 2.3126 acc_val: 0.1526 time: 0.3945s\n",
      "Epoch: 1989 loss_train: 2.3168 acc_train: 0.1365 loss_val: 2.3122 acc_val: 0.1516 time: 0.3985s\n",
      "Epoch: 1990 loss_train: 2.3190 acc_train: 0.1431 loss_val: 2.3117 acc_val: 0.1510 time: 0.3451s\n",
      "Epoch: 1991 loss_train: 2.3167 acc_train: 0.1493 loss_val: 2.3113 acc_val: 0.1510 time: 0.3948s\n",
      "Epoch: 1992 loss_train: 2.3249 acc_train: 0.1357 loss_val: 2.3109 acc_val: 0.1521 time: 0.3726s\n",
      "Epoch: 1993 loss_train: 2.3127 acc_train: 0.1553 loss_val: 2.3106 acc_val: 0.1516 time: 0.3823s\n",
      "Epoch: 1994 loss_train: 2.3181 acc_train: 0.1498 loss_val: 2.3106 acc_val: 0.1536 time: 0.3996s\n",
      "Epoch: 1995 loss_train: 2.3191 acc_train: 0.1397 loss_val: 2.3106 acc_val: 0.1526 time: 0.3764s\n",
      "Epoch: 1996 loss_train: 2.3221 acc_train: 0.1437 loss_val: 2.3108 acc_val: 0.1505 time: 0.3955s\n",
      "Epoch: 1997 loss_train: 2.3135 acc_train: 0.1444 loss_val: 2.3111 acc_val: 0.1536 time: 0.4296s\n",
      "Epoch: 1998 loss_train: 2.3241 acc_train: 0.1454 loss_val: 2.3115 acc_val: 0.1521 time: 0.3973s\n",
      "Epoch: 1999 loss_train: 2.3138 acc_train: 0.1426 loss_val: 2.3118 acc_val: 0.1521 time: 0.3618s\n",
      "Epoch: 2000 loss_train: 2.3177 acc_train: 0.1371 loss_val: 2.3121 acc_val: 0.1490 time: 0.4169s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 782.4143s\n"
     ]
    }
   ],
   "source": [
    "train_dict = {}\n",
    "train_dict['optimizer'] = optimizer\n",
    "train_dict['features'] = features\n",
    "train_dict['adj'] = adj\n",
    "train_dict['labels'] = labels\n",
    "train_dict['idx_train'] = idx_train\n",
    "train_dict['idx_val'] = idx_dev\n",
    "train_dict['num_epochs'] = num_epochs\n",
    "model = run_train(model, train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 2.5213 accuracy= 0.0725\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_dict = {}\n",
    "test_dict['features'] = features\n",
    "test_dict['adj'] = adj\n",
    "test_dict['labels'] = labels\n",
    "test_dict['idx_test'] = idx_test\n",
    "test(model, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model.pickle'\n",
    "pickle.dump(model, open(result_path+model_name,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
