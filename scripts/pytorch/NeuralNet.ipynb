{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Neural Networks\n",
    "\n",
    "Remember to change the `model_type` and `ver` variables prior to running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/chetai/Documents/Projects/moonGen/')\n",
    "\n",
    "import pdb\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Neural network structure imports\n",
    "from GCN.GCN import GCN\n",
    "from Dense.Dense import Dense\n",
    "\n",
    "# Processing imports\n",
    "from sub_data_process import SubGraphProcess\n",
    "from full_data_process import GraphDataProcess\n",
    "\n",
    "# Utility imports\n",
    "from utils.utils import *\n",
    "from utils.label_functions import *\n",
    "from utils.feature_functions import *\n",
    "from utils.adjacency_functions import *\n",
    "from utils.train_test_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_graph_process(param_dict, full_processed_path, full_redo):\n",
    "    \"\"\"\n",
    "    Wrapper for processing data on the full mined data-set\n",
    "    \n",
    "    Input(s):\n",
    "    - param_dict (dict)\n",
    "    - full_processed_path (string): Save path for processed version of full data\n",
    "    - full_redo (bool): Whether or not to re-compute \n",
    "    \n",
    "    Output(s):\n",
    "    GraphDataProcess object \n",
    "    \"\"\"\n",
    "    if (not os.path.exists(full_processed_path)) or full_redo:\n",
    "        # Parse path to mined data\n",
    "        raw_data_path = param_dict['gen_params']['raw_data_path']\n",
    "        \n",
    "        # Parse main save directory and names of intermediate files\n",
    "        data_dir = param_dict['gen_params']['data_dir']\n",
    "        full_names_dict = param_dict['full_names_dict']\n",
    "        \n",
    "        # Parse flags of redoing calculation\n",
    "        full_redo_dict = param_dict['full_redo_dict']\n",
    "        \n",
    "        # Get processing object and execute\n",
    "        graph_data_obj = GraphDataProcess(raw_data_path, data_dir, full_names_dict, full_redo_dict)\n",
    "        graph_data_obj.run_all()\n",
    "        \n",
    "        save_pickle(graph_data_obj, full_processed_path)\n",
    "    else:\n",
    "        graph_data_obj = load_pickle(full_processed_path)\n",
    "    \n",
    "    return graph_data_obj\n",
    "\n",
    "\n",
    "def sub_graph_process(param_dict, data_path, full_processed_path, sub_processed_path, sub_redo):\n",
    "    \"\"\"\n",
    "    Wrapper for sampling data subset and organizing model input features\n",
    "    \n",
    "    Input(s):\n",
    "    - param_dict (dict)\n",
    "    - data_path (string): Path to intermediate save files (for sub-sampling)\n",
    "    - full_processed_path (string): Path to access full processed data\n",
    "    - sub_processed_path (string): Path to save or load sub-processed object\n",
    "    - sub_redo (bool): Whether or not to re-compute \n",
    "    \"\"\"\n",
    "    if (not os.path.exists(sub_processed_path)) or sub_redo:\n",
    "        # Parse parameters\n",
    "        sampling_params = param_dict['sampling_params']\n",
    "        sub_names_dict = param_dict['sub_names_dict']\n",
    "        sub_redo_dict = param_dict['sub_redo_dict']\n",
    "        \n",
    "        # Dictionary of processing functions\n",
    "        sub_functions_dict = get_func_dict(param_dict['sub_functions_dict'])\n",
    "        \n",
    "        # Get sampling object and execute\n",
    "        subgraph_data_obj = SubGraphProcess(\n",
    "            full_processed_path, \n",
    "            data_path, \n",
    "            sub_names_dict, \n",
    "            sub_redo_dict, \n",
    "            sub_functions_dict, \n",
    "            sampling_params\n",
    "        )\n",
    "        subgraph_data_obj.run_all()\n",
    "        \n",
    "        save_pickle(subgraph_data_obj, sub_processed_path)\n",
    "    else:\n",
    "        subgraph_data_obj = load_pickle(sub_processed_path)\n",
    "        \n",
    "    return subgraph_data_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unwrap and Set General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "model_type = 'GCN'\n",
    "ver = 'v0'\n",
    "\n",
    "param_path = 'C:/Users/chetai/Desktop/moonboard_data/%s/%s/params.json' % (model_type, ver)\n",
    "param_dict = json.load(open(param_path,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing save directories\n",
    "data_dir = param_dict['gen_params']['data_dir']\n",
    "data_path = param_dict['gen_params']['data_subpath']\n",
    "result_path = param_dict['gen_params']['result_subpath']\n",
    "\n",
    "# Parse processed result names and create paths\n",
    "full_processed_name = param_dict['gen_params']['full_processed_name']\n",
    "sub_processed_name = param_dict['gen_params']['sub_processed_name']\n",
    "\n",
    "full_processed_path = data_dir + full_processed_name\n",
    "sub_processed_path = data_path + sub_processed_name\n",
    "\n",
    "# Parse redo settings\n",
    "full_redo = param_dict['gen_params']['full_redo']\n",
    "sub_redo = param_dict['gen_params']['sub_redo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping nodes...\n",
      "Finished mapping nodes!\n",
      "\n",
      "Mapping adjacency...\n",
      "Finished mapping adjacency!\n",
      "\n",
      "Training TFIDF...\n",
      "Finished training TFIDF!\n",
      "Sampling core nodes...\n",
      "Getting samples node features...\n",
      "Getting samples node adjacency...\n",
      "Getting samples node labels...\n"
     ]
    }
   ],
   "source": [
    "# Get full processed data\n",
    "graph_data_obj = full_graph_process(param_dict, full_processed_path, full_redo)\n",
    "\n",
    "# Get sampled processed data\n",
    "subgraph_data_obj = sub_graph_process(param_dict, data_path, full_processed_path, sub_processed_path, sub_redo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse split ratio dictionary\n",
    "split_ratio_dict = param_dict['split_ratio_dict']\n",
    "\n",
    "# Set binary vs. multi-class classification \n",
    "target_grade = -1\n",
    "\n",
    "# Get data for PyTorch training\n",
    "features, adj, labels, idx_train, idx_dev, idx_test = sample_and_load_pytorch_data(\n",
    "    subgraph_data_obj, \n",
    "    split_ratio_dict, \n",
    "    result_path, \n",
    "    target_grade, \n",
    "    sub_redo\n",
    ")\n",
    "\n",
    "# Set number of labels\n",
    "num_labels = len(list(set(list(np.asarray(labels)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train-dev-test indexes\n",
    "idx_train_path = data_path + 'train_idxs.pickle'\n",
    "idx_dev_path = data_path + 'dev_idxs.pickle'\n",
    "idx_test_path = data_path + 'test_idxs.pickle'\n",
    "\n",
    "save_pickle(idx_train.numpy(), idx_train_path)\n",
    "save_pickle(idx_dev.numpy(), idx_dev_path)\n",
    "save_pickle(idx_test.numpy(), idx_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Neural Network Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_params = param_dict['dense_params']\n",
    "if dense_params['on']:\n",
    "    num_epochs = dense_params['num_epochs']\n",
    "    model = Dense(\n",
    "        nfeatures=features.shape[1],\n",
    "        nhidden_layer_list=dense_params['hidden'],\n",
    "        nclass=num_labels,\n",
    "        dropout=dense_params['dropout']\n",
    "    )\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=dense_params['lr'], \n",
    "        weight_decay=dense_params['weight_decay']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Kaiming Initialization\n",
      "| Kaiming Initialization\n"
     ]
    }
   ],
   "source": [
    "gcn_params = param_dict['gcn_params']\n",
    "if gcn_params['on']:\n",
    "    num_epochs = gcn_params['num_epochs']\n",
    "    model = GCN(\n",
    "        nfeatures=features.shape[1],\n",
    "        nhidden_layer_list=gcn_params['hidden'],\n",
    "        nclass=num_labels,\n",
    "        dropout=gcn_params['dropout']\n",
    "    )\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=gcn_params['lr'], \n",
    "        weight_decay=gcn_params['weight_decay']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gc_list): ModuleList(\n",
       "    (0): GraphConvolution (1240 -> 32)\n",
       "    (1): GraphConvolution (32 -> 12)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.4872 acc_train: 0.0903 loss_val: 2.4807 acc_val: 0.1146 time: 0.0890s\n",
      "Epoch: 0002 loss_train: 2.4800 acc_train: 0.0931 loss_val: 2.4796 acc_val: 0.1338 time: 0.0160s\n",
      "Epoch: 0003 loss_train: 2.4728 acc_train: 0.1203 loss_val: 2.4782 acc_val: 0.1274 time: 0.0160s\n",
      "Epoch: 0004 loss_train: 2.4678 acc_train: 0.1074 loss_val: 2.4766 acc_val: 0.1210 time: 0.0150s\n",
      "Epoch: 0005 loss_train: 2.4624 acc_train: 0.1203 loss_val: 2.4745 acc_val: 0.1210 time: 0.0140s\n",
      "Epoch: 0006 loss_train: 2.4554 acc_train: 0.1318 loss_val: 2.4723 acc_val: 0.1210 time: 0.0160s\n",
      "Epoch: 0007 loss_train: 2.4511 acc_train: 0.1318 loss_val: 2.4699 acc_val: 0.1210 time: 0.0140s\n",
      "Epoch: 0008 loss_train: 2.4427 acc_train: 0.1404 loss_val: 2.4672 acc_val: 0.1338 time: 0.0130s\n",
      "Epoch: 0009 loss_train: 2.4374 acc_train: 0.1447 loss_val: 2.4642 acc_val: 0.1338 time: 0.0150s\n",
      "Epoch: 0010 loss_train: 2.4321 acc_train: 0.1490 loss_val: 2.4612 acc_val: 0.1146 time: 0.0150s\n",
      "Epoch: 0011 loss_train: 2.4266 acc_train: 0.1605 loss_val: 2.4580 acc_val: 0.1210 time: 0.0140s\n",
      "Epoch: 0012 loss_train: 2.4183 acc_train: 0.1705 loss_val: 2.4544 acc_val: 0.1274 time: 0.0140s\n",
      "Epoch: 0013 loss_train: 2.4112 acc_train: 0.1734 loss_val: 2.4506 acc_val: 0.1274 time: 0.0170s\n",
      "Epoch: 0014 loss_train: 2.4057 acc_train: 0.1676 loss_val: 2.4463 acc_val: 0.1274 time: 0.0180s\n",
      "Epoch: 0015 loss_train: 2.3960 acc_train: 0.1805 loss_val: 2.4418 acc_val: 0.1338 time: 0.0180s\n",
      "Epoch: 0016 loss_train: 2.3900 acc_train: 0.1633 loss_val: 2.4370 acc_val: 0.1338 time: 0.0150s\n",
      "Epoch: 0017 loss_train: 2.3802 acc_train: 0.1676 loss_val: 2.4321 acc_val: 0.1210 time: 0.0130s\n",
      "Epoch: 0018 loss_train: 2.3760 acc_train: 0.1977 loss_val: 2.4270 acc_val: 0.1210 time: 0.0150s\n",
      "Epoch: 0019 loss_train: 2.3668 acc_train: 0.1834 loss_val: 2.4220 acc_val: 0.1146 time: 0.0150s\n",
      "Epoch: 0020 loss_train: 2.3585 acc_train: 0.2106 loss_val: 2.4169 acc_val: 0.1146 time: 0.0140s\n",
      "Epoch: 0021 loss_train: 2.3513 acc_train: 0.2135 loss_val: 2.4119 acc_val: 0.1146 time: 0.0140s\n",
      "Epoch: 0022 loss_train: 2.3458 acc_train: 0.2278 loss_val: 2.4069 acc_val: 0.1210 time: 0.0150s\n",
      "Epoch: 0023 loss_train: 2.3323 acc_train: 0.2292 loss_val: 2.4019 acc_val: 0.1210 time: 0.0160s\n",
      "Epoch: 0024 loss_train: 2.3218 acc_train: 0.2450 loss_val: 2.3971 acc_val: 0.1338 time: 0.0170s\n",
      "Epoch: 0025 loss_train: 2.3169 acc_train: 0.2192 loss_val: 2.3921 acc_val: 0.1210 time: 0.0170s\n",
      "Epoch: 0026 loss_train: 2.3039 acc_train: 0.2751 loss_val: 2.3870 acc_val: 0.1210 time: 0.0165s\n",
      "Epoch: 0027 loss_train: 2.2920 acc_train: 0.2636 loss_val: 2.3816 acc_val: 0.1210 time: 0.0170s\n",
      "Epoch: 0028 loss_train: 2.2883 acc_train: 0.2894 loss_val: 2.3761 acc_val: 0.1210 time: 0.0160s\n",
      "Epoch: 0029 loss_train: 2.2791 acc_train: 0.3052 loss_val: 2.3705 acc_val: 0.1210 time: 0.0151s\n",
      "Epoch: 0030 loss_train: 2.2757 acc_train: 0.2794 loss_val: 2.3644 acc_val: 0.1210 time: 0.0171s\n",
      "Epoch: 0031 loss_train: 2.2562 acc_train: 0.3324 loss_val: 2.3584 acc_val: 0.1210 time: 0.0180s\n",
      "Epoch: 0032 loss_train: 2.2498 acc_train: 0.2650 loss_val: 2.3525 acc_val: 0.1210 time: 0.0160s\n",
      "Epoch: 0033 loss_train: 2.2379 acc_train: 0.3209 loss_val: 2.3466 acc_val: 0.1083 time: 0.0150s\n",
      "Epoch: 0034 loss_train: 2.2256 acc_train: 0.3266 loss_val: 2.3405 acc_val: 0.0955 time: 0.0170s\n",
      "Epoch: 0035 loss_train: 2.2198 acc_train: 0.3453 loss_val: 2.3341 acc_val: 0.0955 time: 0.0140s\n",
      "Epoch: 0036 loss_train: 2.2009 acc_train: 0.3524 loss_val: 2.3272 acc_val: 0.0955 time: 0.0140s\n",
      "Epoch: 0037 loss_train: 2.1975 acc_train: 0.3582 loss_val: 2.3203 acc_val: 0.1019 time: 0.0150s\n",
      "Epoch: 0038 loss_train: 2.1814 acc_train: 0.3410 loss_val: 2.3138 acc_val: 0.1083 time: 0.0140s\n",
      "Epoch: 0039 loss_train: 2.1721 acc_train: 0.3453 loss_val: 2.3076 acc_val: 0.1083 time: 0.0140s\n",
      "Epoch: 0040 loss_train: 2.1598 acc_train: 0.3510 loss_val: 2.3018 acc_val: 0.1083 time: 0.0140s\n",
      "Epoch: 0041 loss_train: 2.1611 acc_train: 0.3481 loss_val: 2.2962 acc_val: 0.1083 time: 0.0145s\n",
      "Epoch: 0042 loss_train: 2.1400 acc_train: 0.3453 loss_val: 2.2910 acc_val: 0.1146 time: 0.0140s\n",
      "Epoch: 0043 loss_train: 2.1310 acc_train: 0.3539 loss_val: 2.2858 acc_val: 0.1146 time: 0.0140s\n",
      "Epoch: 0044 loss_train: 2.1245 acc_train: 0.3668 loss_val: 2.2805 acc_val: 0.1146 time: 0.0150s\n",
      "Epoch: 0045 loss_train: 2.1136 acc_train: 0.3195 loss_val: 2.2751 acc_val: 0.1146 time: 0.0171s\n",
      "Epoch: 0046 loss_train: 2.1045 acc_train: 0.3539 loss_val: 2.2693 acc_val: 0.1146 time: 0.0130s\n",
      "Epoch: 0047 loss_train: 2.0885 acc_train: 0.3840 loss_val: 2.2638 acc_val: 0.1210 time: 0.0130s\n",
      "Epoch: 0048 loss_train: 2.0790 acc_train: 0.3782 loss_val: 2.2591 acc_val: 0.1274 time: 0.0150s\n",
      "Epoch: 0049 loss_train: 2.0781 acc_train: 0.3539 loss_val: 2.2548 acc_val: 0.1274 time: 0.0150s\n",
      "Epoch: 0050 loss_train: 2.0620 acc_train: 0.3653 loss_val: 2.2508 acc_val: 0.1146 time: 0.0160s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.9621s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_dict = {}\n",
    "train_dict['optimizer'] = optimizer\n",
    "train_dict['features'] = features\n",
    "train_dict['adj'] = adj\n",
    "train_dict['labels'] = labels\n",
    "train_dict['idx_train'] = idx_train\n",
    "train_dict['idx_val'] = idx_dev\n",
    "train_dict['num_epochs'] = num_epochs\n",
    "\n",
    "model = run_train(model, train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 2.3166 accuracy= 0.1429\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test_dict = {}\n",
    "test_dict['features'] = features\n",
    "test_dict['adj'] = adj\n",
    "test_dict['labels'] = labels\n",
    "test_dict['idx_test'] = idx_test\n",
    "\n",
    "test(model, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_name = 'model.pickle'\n",
    "save_pickle(model, result_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
